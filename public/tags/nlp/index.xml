<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Nlp on Aqua Mega</title>
    <link>http://localhost:1313/aquamega/tags/nlp/</link>
    <description>Recent content in Nlp on Aqua Mega</description>
    <image>
      <title>Aqua Mega</title>
      <url>https://github.com/shyu216.png</url>
      <link>https://github.com/shyu216.png</link>
    </image>
    <generator>Hugo -- 0.125.4</generator>
    <language>en</language>
    <lastBuildDate>Tue, 28 May 2024 14:56:05 +1000</lastBuildDate>
    <atom:link href="http://localhost:1313/aquamega/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Part-of-Speech Tagging</title>
      <link>http://localhost:1313/aquamega/nlp/pos/</link>
      <pubDate>Tue, 28 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/pos/</guid>
      <description>Part-of-Speech(词类) Colourless green ideas sleep furiously.
Open Classes (Content Words) Noun Verb Adjective Adverb Closed Classes (Function Words) (Fixed number of words) Preposition Determiner Pronoun Conjunction Interjection Ambiguity: A word can have multiple meanings. A sentence can have multiple interpretations.</description>
    </item>
    <item>
      <title>N-gram Language Model</title>
      <link>http://localhost:1313/aquamega/nlp/n-gram/</link>
      <pubDate>Mon, 27 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/n-gram/</guid>
      <description>Language Model To explain language, based on probability, for generation.
Query completion(搜索引擎) Optical character recognition(OCR) Translation N-gram $$P(w_1, w_2, \cdots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2) \cdots P(w_n|w_1, w_2, \cdots, w_{n-1})$$ Maximum Likelihood Estimation $$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$ Use special tags to denote the beginning and end of a sentence, i.e. &amp;lt;s&amp;gt;, &amp;lt;/s&amp;gt;.
Smoothing Handle unseen words.
Laplacian (add-one) smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$
Add-k smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + kV}$</description>
    </item>
    <item>
      <title>Preprocessing</title>
      <link>http://localhost:1313/aquamega/nlp/preprocessing/</link>
      <pubDate>Mon, 27 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/preprocessing/</guid>
      <description>Preprocessing Word Sentence: words Document: sentences Corpus: documents Word Token: word instance Word Type: verb, noun, etc. Lexicon: word types Steps Remove unwnated formatting, i.e. HTML tags Sentecne segmentation, break text into sentences Tokenization, break sentences into words Normalization, transform words into canonical form, i.e. lower case Stopword removal, delete unwanted words, i.e., &amp;ldquo;the&amp;rdquo;, &amp;ldquo;I&amp;rdquo;, &amp;ldquo;.&amp;rdquo;, etc. Max Match Algorithm Greedily match the longest word in the dictionary. Used in the language withoout space between words, i.</description>
    </item>
    <item>
      <title>Text Classification</title>
      <link>http://localhost:1313/aquamega/nlp/classification/</link>
      <pubDate>Mon, 27 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/classification/</guid>
      <description>Topic classification Sentiment analysis Native language identification Natual language inference Automatic fact-checking Paraphrase Topic Classification Motivation: library science, information retrieval, etc. Classes: topic categories Features: bag-of-words, n-grams, etc. Corpus: reuters news corpus, pubmed abstracts, tweets with hashtags Sentiment Analysis Motivation: opinion mining, business analytics Classes: positive, negative, neutral Features: n-grams, polarity lexicons, etc. Corpus: movie reviews, SEMEVAL twitter polarity dataset Native Language Identification Motivation: forensic linguistics, educational applications Classes: native languages Features: character n-grams, syntactic features (POS), phonological features Corpus: TOEFL/IELTS essays Natural Language Inference (textual entailment 文本蕴含) Motivation: language understanding Classes: entailment, contradiction, neutral Features: word overlap, length difference, etc.</description>
    </item>
    <item>
      <title>Comp90042</title>
      <link>http://localhost:1313/aquamega/course/comp90042/</link>
      <pubDate>Fri, 26 Apr 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90042/</guid>
      <description>Natural Language Processing preprocessing byte pair encoding classification n-gram part-of-speech tagging hidden markov model recurrent neural network LSTM transformer BERT </description>
    </item>
  </channel>
</rss>

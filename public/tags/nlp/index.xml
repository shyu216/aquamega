<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Nlp on Aqua Mega</title>
    <link>http://localhost:1313/aquamega/tags/nlp/</link>
    <description>Recent content in Nlp on Aqua Mega</description>
    <image>
      <title>Aqua Mega</title>
      <url>https://github.com/shyu216.png</url>
      <link>https://github.com/shyu216.png</link>
    </image>
    <generator>Hugo -- 0.125.4</generator>
    <language>en</language>
    <lastBuildDate>Tue, 28 May 2024 03:56:05 +1000</lastBuildDate>
    <atom:link href="http://localhost:1313/aquamega/tags/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Part-of-Speech Tagging</title>
      <link>http://localhost:1313/aquamega/nlp/pos/</link>
      <pubDate>Tue, 28 May 2024 03:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/pos/</guid>
      <description>Part-of-Speech(词类) Colourless green ideas sleep furiously.
Open Classes (Content Words) Noun Verb Adjective Adverb Closed Classes (Function Words) (Fixed number of words) Preposition Determiner Pronoun Conjunction Interjection Ambiguity: A word can have multiple meanings. A sentence can have multiple interpretations.
在自然语言处理（NLP）中，POS代表词性标注（Part-of-Speech Tagging）。词性标注是一种将每个词语与其语法范畴（即词性）相关联的过程。它是NLP中的一项基本任务，用于确定句子中每个单词的词类。
常见的词性包括名词、动词、形容词、副词、介词、代词、冠词、连词等。通过进行词性标注，可以帮助计算机理解句子的结构和含义，为其他NLP任务（如句法分析、语义分析和机器翻译等）提供基础。
以下是一个例子，展示了一句英文句子的词性标注示例：
句子：I love eating ice cream.
词性标注：PRON VERB VERB NOUN NOUN.
在这个例子中，&amp;ldquo;I&amp;quot;被标注为代词（PRON），&amp;ldquo;love&amp;quot;和&amp;quot;eating&amp;quot;被标注为动词（VERB），&amp;ldquo;ice&amp;quot;和&amp;quot;cream&amp;quot;被标注为名词（NOUN）。
词性标注可以通过使用基于规则的方法、基于统计的机器学习方法（如隐马尔可夫模型）或基于深度学习的方法（如循环神经网络和转换器模型）来实现。这个任务在NLP中具有广泛的应用，对于许多文本处理任务都是重要的预处理步骤。
NN（noun）名词：cat（猫）、book（书）、table（桌子） VB（verb）动词：run（跑）、eat（吃）、sleep（睡觉） JJ（adjective）形容词：big（大的）、happy（快乐的）、red（红色的） RB（adverb）副词：quickly（快速地）、well（好地）、very（非常） DT（determiner）限定词：the（定冠词）、this（这个）、some（一些） CD（cardinal number）基数词：one（一）、three（三）、ten（十） IN（preposition）介词：on（在&amp;hellip;上）、with（和&amp;hellip;一起）、at（在&amp;hellip;处） PRP（personal pronoun）人称代词：I（我）、you（你）、he（他） MD（modal）情态动词：can（能够）、should（应该）、will（将会） CC（coordinating conjunction）并列连词：and（和）、but（但是）、or（或者） RP（particle）小品词：up（向上）、down（向下）、out（出去） WH（wh-pronoun）疑问代词：who（谁）、what（什么）、which（哪个） TO（to）不定式标记：to（去）、to（为了）、to（到） Penn Treebank 词性标记集 Penn Treebank 的词性标注集（POS tagset）是一种广泛使用的英文词性标注方法。</description>
    </item>
    <item>
      <title>Lexical Semantics and Distributional Semantics</title>
      <link>http://localhost:1313/aquamega/nlp/semantics/</link>
      <pubDate>Tue, 28 May 2024 00:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/semantics/</guid>
      <description>Lexical Semantics 词汇解释
Synonymy 同义词 Antonymy 反义词 Hypernymy 上义词 Meronymy 下义词 WordNet A lexical database of English.
Word Similarity 路径长度：在WordNet中，两个词之间的路径长度可以用来衡量它们的相似性。 深度信息：在WordNet中，词的深度（在词汇树中的位置）也可以用来衡量词的相似性。 信息内容：词的信息内容（在语料库中的频率）也可以用来衡量词的相似性。 Distributional Semantics 词汇和其上下文的共现信息</description>
    </item>
    <item>
      <title>Neual Networks</title>
      <link>http://localhost:1313/aquamega/nlp/nn/</link>
      <pubDate>Tue, 28 May 2024 00:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/nn/</guid>
      <description>神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。
每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。
神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。
神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。
A Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.
Each neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.</description>
    </item>
    <item>
      <title>Hidden Markov Model</title>
      <link>http://localhost:1313/aquamega/nlp/hmm/</link>
      <pubDate>Mon, 27 May 2024 03:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/hmm/</guid>
      <description> w：word t：tag emission probability: $P(w|t)$ 从tag到word的概率 transition probability: $P(t|t&#39;)$ 从上一个tag到当前tag的概率 Viterbi Algorithm 从最开始算起，每个tag的概率是由前一个tag的概率和从前一个tag到当前tag的概率相乘得到的。
$\hat{t} = \arg\max_{t} \prod_{i=1}^{n} P(w_i|t)P(t|t&#39;)$, $t&#39;$是前一个tag, $\hat{t}$是当前的估计值。
Generative vs Discriminative Tagger HMM是generative model，可以用来生成数据，无监督的学习。 discriminative model例如CRF，需要有监督的数据，可以学到更丰富的特征，大多数deep learning的模型都是discriminative model。 </description>
    </item>
    <item>
      <title>Text Classification</title>
      <link>http://localhost:1313/aquamega/nlp/classification/</link>
      <pubDate>Mon, 27 May 2024 02:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/classification/</guid>
      <description>Topic classification Sentiment analysis Native language identification Natual language inference Automatic fact-checking Paraphrase Topic Classification Motivation: library science, information retrieval, etc. Classes: topic categories Features: bag-of-words, n-grams, etc. Corpus: reuters news corpus, pubmed abstracts, tweets with hashtags Sentiment Analysis Motivation: opinion mining, business analytics Classes: positive, negative, neutral Features: n-grams, polarity lexicons, etc. Corpus: movie reviews, SEMEVAL twitter polarity dataset Native Language Identification Motivation: forensic linguistics, educational applications Classes: native languages Features: character n-grams, syntactic features (POS), phonological features Corpus: TOEFL/IELTS essays Natural Language Inference (textual entailment 文本蕴含) Motivation: language understanding Classes: entailment, contradiction, neutral Features: word overlap, length difference, etc.</description>
    </item>
    <item>
      <title>N-gram Language Model</title>
      <link>http://localhost:1313/aquamega/nlp/n-gram/</link>
      <pubDate>Mon, 27 May 2024 01:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/n-gram/</guid>
      <description>Language Model To explain language, based on probability, for generation.
Query completion(搜索引擎) Optical character recognition(OCR) Translation N-gram $$P(w_1, w_2, \cdots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2) \cdots P(w_n|w_1, w_2, \cdots, w_{n-1})$$ Maximum Likelihood Estimation $$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$ Use special tags to denote the beginning and end of a sentence, i.e. &amp;lt;s&amp;gt;, &amp;lt;/s&amp;gt;.
Smoothing Handle unseen words.
Laplacian (add-one) smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$
Add-k smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + kV}$</description>
    </item>
    <item>
      <title>Preprocessing</title>
      <link>http://localhost:1313/aquamega/nlp/preprocessing/</link>
      <pubDate>Mon, 27 May 2024 00:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/preprocessing/</guid>
      <description>Preprocessing Word Sentence: words Document: sentences Corpus: documents Word Token: word instance Word Type: verb, noun, etc. Lexicon: word types Steps Remove unwnated formatting, i.e. HTML tags Sentecne segmentation, break text into sentences Tokenization, break sentences into words Normalization, transform words into canonical form, i.e. lower case Stopword removal, delete unwanted words, i.e., &amp;ldquo;the&amp;rdquo;, &amp;ldquo;I&amp;rdquo;, &amp;ldquo;.&amp;rdquo;, etc. Max Match Algorithm Greedily match the longest word in the dictionary. Used in the language withoout space between words, i.</description>
    </item>
    <item>
      <title>Comp90042</title>
      <link>http://localhost:1313/aquamega/course/comp90042/</link>
      <pubDate>Fri, 26 Apr 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90042/</guid>
      <description>Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission &amp;amp; Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:</description>
    </item>
  </channel>
</rss>

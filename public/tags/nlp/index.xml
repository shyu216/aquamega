<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>NLP on Daleçš„æ°´ç¡•æ—¥è®°</title>
        <link>http://localhost:1313/aquamega/tags/nlp/</link>
        <description>Recent content in NLP on Daleçš„æ°´ç¡•æ—¥è®°</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Tue, 28 May 2024 03:56:05 +1000</lastBuildDate><atom:link href="http://localhost:1313/aquamega/tags/nlp/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Part-of-Speech Tagging</title>
        <link>http://localhost:1313/aquamega/p/part-of-speech-tagging/</link>
        <pubDate>Tue, 28 May 2024 03:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/part-of-speech-tagging/</guid>
        <description>&lt;h2 id=&#34;part-of-speechè¯ç±»&#34;&gt;Part-of-Speech(è¯ç±»)
&lt;/h2&gt;&lt;p&gt;Colourless green ideas sleep furiously.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Open Classes (Content Words)
&lt;ul&gt;
&lt;li&gt;Noun&lt;/li&gt;
&lt;li&gt;Verb&lt;/li&gt;
&lt;li&gt;Adjective&lt;/li&gt;
&lt;li&gt;Adverb&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Closed Classes (Function Words) (Fixed number of words)
&lt;ul&gt;
&lt;li&gt;Preposition&lt;/li&gt;
&lt;li&gt;Determiner&lt;/li&gt;
&lt;li&gt;Pronoun&lt;/li&gt;
&lt;li&gt;Conjunction&lt;/li&gt;
&lt;li&gt;Interjection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ambiguity: A word can have multiple meanings. A sentence can have multiple interpretations.&lt;/p&gt;
&lt;p&gt;åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä¸­ï¼ŒPOSä»£è¡¨è¯æ€§æ ‡æ³¨ï¼ˆPart-of-Speech Taggingï¼‰ã€‚è¯æ€§æ ‡æ³¨æ˜¯ä¸€ç§å°†æ¯ä¸ªè¯è¯­ä¸å…¶è¯­æ³•èŒƒç•´ï¼ˆå³è¯æ€§ï¼‰ç›¸å…³è”çš„è¿‡ç¨‹ã€‚å®ƒæ˜¯NLPä¸­çš„ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ï¼Œç”¨äºç¡®å®šå¥å­ä¸­æ¯ä¸ªå•è¯çš„è¯ç±»ã€‚&lt;/p&gt;
&lt;p&gt;å¸¸è§çš„è¯æ€§åŒ…æ‹¬åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯ã€ä»‹è¯ã€ä»£è¯ã€å† è¯ã€è¿è¯ç­‰ã€‚é€šè¿‡è¿›è¡Œè¯æ€§æ ‡æ³¨ï¼Œå¯ä»¥å¸®åŠ©è®¡ç®—æœºç†è§£å¥å­çš„ç»“æ„å’Œå«ä¹‰ï¼Œä¸ºå…¶ä»–NLPä»»åŠ¡ï¼ˆå¦‚å¥æ³•åˆ†æã€è¯­ä¹‰åˆ†æå’Œæœºå™¨ç¿»è¯‘ç­‰ï¼‰æä¾›åŸºç¡€ã€‚&lt;/p&gt;
&lt;p&gt;ä»¥ä¸‹æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œå±•ç¤ºäº†ä¸€å¥è‹±æ–‡å¥å­çš„è¯æ€§æ ‡æ³¨ç¤ºä¾‹ï¼š&lt;/p&gt;
&lt;p&gt;å¥å­ï¼šI love eating ice cream.&lt;br&gt;
è¯æ€§æ ‡æ³¨ï¼šPRON VERB VERB NOUN NOUN.&lt;/p&gt;
&lt;p&gt;åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ&amp;ldquo;I&amp;quot;è¢«æ ‡æ³¨ä¸ºä»£è¯ï¼ˆPRONï¼‰ï¼Œ&amp;ldquo;love&amp;quot;å’Œ&amp;quot;eating&amp;quot;è¢«æ ‡æ³¨ä¸ºåŠ¨è¯ï¼ˆVERBï¼‰ï¼Œ&amp;ldquo;ice&amp;quot;å’Œ&amp;quot;cream&amp;quot;è¢«æ ‡æ³¨ä¸ºåè¯ï¼ˆNOUNï¼‰ã€‚&lt;/p&gt;
&lt;p&gt;è¯æ€§æ ‡æ³¨å¯ä»¥é€šè¿‡ä½¿ç”¨åŸºäºè§„åˆ™çš„æ–¹æ³•ã€åŸºäºç»Ÿè®¡çš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼ˆå¦‚éšé©¬å°”å¯å¤«æ¨¡å‹ï¼‰æˆ–åŸºäºæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼ˆå¦‚å¾ªç¯ç¥ç»ç½‘ç»œå’Œè½¬æ¢å™¨æ¨¡å‹ï¼‰æ¥å®ç°ã€‚è¿™ä¸ªä»»åŠ¡åœ¨NLPä¸­å…·æœ‰å¹¿æ³›çš„åº”ç”¨ï¼Œå¯¹äºè®¸å¤šæ–‡æœ¬å¤„ç†ä»»åŠ¡éƒ½æ˜¯é‡è¦çš„é¢„å¤„ç†æ­¥éª¤ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NNï¼ˆnounï¼‰åè¯ï¼šcatï¼ˆçŒ«ï¼‰ã€bookï¼ˆä¹¦ï¼‰ã€tableï¼ˆæ¡Œå­ï¼‰&lt;/li&gt;
&lt;li&gt;VBï¼ˆverbï¼‰åŠ¨è¯ï¼šrunï¼ˆè·‘ï¼‰ã€eatï¼ˆåƒï¼‰ã€sleepï¼ˆç¡è§‰ï¼‰&lt;/li&gt;
&lt;li&gt;JJï¼ˆadjectiveï¼‰å½¢å®¹è¯ï¼šbigï¼ˆå¤§çš„ï¼‰ã€happyï¼ˆå¿«ä¹çš„ï¼‰ã€redï¼ˆçº¢è‰²çš„ï¼‰&lt;/li&gt;
&lt;li&gt;RBï¼ˆadverbï¼‰å‰¯è¯ï¼šquicklyï¼ˆå¿«é€Ÿåœ°ï¼‰ã€wellï¼ˆå¥½åœ°ï¼‰ã€veryï¼ˆéå¸¸ï¼‰&lt;/li&gt;
&lt;li&gt;DTï¼ˆdeterminerï¼‰é™å®šè¯ï¼štheï¼ˆå®šå† è¯ï¼‰ã€thisï¼ˆè¿™ä¸ªï¼‰ã€someï¼ˆä¸€äº›ï¼‰&lt;/li&gt;
&lt;li&gt;CDï¼ˆcardinal numberï¼‰åŸºæ•°è¯ï¼šoneï¼ˆä¸€ï¼‰ã€threeï¼ˆä¸‰ï¼‰ã€tenï¼ˆåï¼‰&lt;/li&gt;
&lt;li&gt;INï¼ˆprepositionï¼‰ä»‹è¯ï¼šonï¼ˆåœ¨&amp;hellip;ä¸Šï¼‰ã€withï¼ˆå’Œ&amp;hellip;ä¸€èµ·ï¼‰ã€atï¼ˆåœ¨&amp;hellip;å¤„ï¼‰&lt;/li&gt;
&lt;li&gt;PRPï¼ˆpersonal pronounï¼‰äººç§°ä»£è¯ï¼šIï¼ˆæˆ‘ï¼‰ã€youï¼ˆä½ ï¼‰ã€heï¼ˆä»–ï¼‰&lt;/li&gt;
&lt;li&gt;MDï¼ˆmodalï¼‰æƒ…æ€åŠ¨è¯ï¼šcanï¼ˆèƒ½å¤Ÿï¼‰ã€shouldï¼ˆåº”è¯¥ï¼‰ã€willï¼ˆå°†ä¼šï¼‰&lt;/li&gt;
&lt;li&gt;CCï¼ˆcoordinating conjunctionï¼‰å¹¶åˆ—è¿è¯ï¼šandï¼ˆå’Œï¼‰ã€butï¼ˆä½†æ˜¯ï¼‰ã€orï¼ˆæˆ–è€…ï¼‰&lt;/li&gt;
&lt;li&gt;RPï¼ˆparticleï¼‰å°å“è¯ï¼šupï¼ˆå‘ä¸Šï¼‰ã€downï¼ˆå‘ä¸‹ï¼‰ã€outï¼ˆå‡ºå»ï¼‰&lt;/li&gt;
&lt;li&gt;WHï¼ˆwh-pronounï¼‰ç–‘é—®ä»£è¯ï¼šwhoï¼ˆè°ï¼‰ã€whatï¼ˆä»€ä¹ˆï¼‰ã€whichï¼ˆå“ªä¸ªï¼‰&lt;/li&gt;
&lt;li&gt;TOï¼ˆtoï¼‰ä¸å®šå¼æ ‡è®°ï¼štoï¼ˆå»ï¼‰ã€toï¼ˆä¸ºäº†ï¼‰ã€toï¼ˆåˆ°ï¼‰&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;penn-treebank-è¯æ€§æ ‡è®°é›†&#34;&gt;Penn Treebank è¯æ€§æ ‡è®°é›†
&lt;/h2&gt;&lt;p&gt;Penn Treebank çš„è¯æ€§æ ‡æ³¨é›†ï¼ˆPOS tagsetï¼‰æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„è‹±æ–‡è¯æ€§æ ‡æ³¨æ–¹æ³•ã€‚&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CC&lt;/code&gt;ï¼šå¹¶åˆ—è¿è¯ (Coordinating conjunction)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CD&lt;/code&gt;ï¼šåŸºæ•°è¯ (Cardinal number)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DT&lt;/code&gt;ï¼šé™å®šè¯ (Determiner)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EX&lt;/code&gt;ï¼šå­˜åœ¨å¥å¼•å¯¼è¯ (Existential there)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;FW&lt;/code&gt;ï¼šå¤–æ¥è¯ (Foreign word)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IN&lt;/code&gt;ï¼šä»‹è¯æˆ–ä»å±è¿è¯ (Preposition or subordinating conjunction)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;JJ&lt;/code&gt;ï¼šå½¢å®¹è¯ (Adjective)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;JJR&lt;/code&gt;ï¼šæ¯”è¾ƒçº§å½¢å®¹è¯ (Adjective, comparative)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;JJS&lt;/code&gt;ï¼šæœ€é«˜çº§å½¢å®¹è¯ (Adjective, superlative)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LS&lt;/code&gt;ï¼šåˆ—è¡¨é¡¹æ ‡è®° (List item marker)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MD&lt;/code&gt;ï¼šæƒ…æ€åŠ¨è¯ (Modal)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NN&lt;/code&gt;ï¼šåè¯, å•æ•°æˆ–é›†æ•° (Noun, singular or mass)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NNS&lt;/code&gt;ï¼šåè¯, å¤æ•° (Noun, plural)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NNP&lt;/code&gt;ï¼šä¸“æœ‰åè¯, å•æ•° (Proper noun, singular)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;NNPS&lt;/code&gt;ï¼šä¸“æœ‰åè¯, å¤æ•° (Proper noun, plural)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PDT&lt;/code&gt;ï¼šå‰é™å®šè¯ (Predeterminer)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;POS&lt;/code&gt;ï¼šæ‰€æœ‰æ ¼ç»“æŸ (Possessive ending)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PRP&lt;/code&gt;ï¼šäººç§°ä»£è¯ (Personal pronoun)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PRP$&lt;/code&gt;ï¼šæ‰€æœ‰æ ¼ä»£è¯ (Possessive pronoun)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RB&lt;/code&gt;ï¼šå‰¯è¯ (Adverb)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RBR&lt;/code&gt;ï¼šå‰¯è¯, æ¯”è¾ƒçº§ (Adverb, comparative)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RBS&lt;/code&gt;ï¼šå‰¯è¯, æœ€é«˜çº§ (Adverb, superlative)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RP&lt;/code&gt;ï¼šå°å“è¯ (Particle)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SYM&lt;/code&gt;ï¼šç¬¦å· (Symbol)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TO&lt;/code&gt;ï¼što&lt;/li&gt;
&lt;li&gt;&lt;code&gt;UH&lt;/code&gt;ï¼šæ„Ÿå¹è¯ (Interjection)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VB&lt;/code&gt;ï¼šåŠ¨è¯, åŸå½¢ (Verb, base form)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VBD&lt;/code&gt;ï¼šåŠ¨è¯, è¿‡å»å¼ (Verb, past tense)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VBG&lt;/code&gt;ï¼šåŠ¨è¯, ç°åœ¨åˆ†è¯æˆ–åŠ¨åè¯ (Verb, gerund or present participle)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VBN&lt;/code&gt;ï¼šåŠ¨è¯, è¿‡å»åˆ†è¯ (Verb, past participle)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VBP&lt;/code&gt;ï¼šåŠ¨è¯, éç¬¬ä¸‰äººç§°å•æ•°ç°åœ¨å¼ (Verb, non-3rd person singular present)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VBZ&lt;/code&gt;ï¼šåŠ¨è¯, ç¬¬ä¸‰äººç§°å•æ•°ç°åœ¨å¼ (Verb, 3rd person singular present)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WDT&lt;/code&gt;ï¼šwh-é™å®šè¯ (Wh-determiner)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WP&lt;/code&gt;ï¼šwh-ä»£è¯ (Wh-pronoun)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WP$&lt;/code&gt;ï¼šwh-æ‰€æœ‰æ ¼ä»£è¯ (Wh-pronoun, possessive)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;WRB&lt;/code&gt;ï¼šwh-å‰¯è¯ (Wh-adverb)&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Lexical Semantics and Distributional Semantics</title>
        <link>http://localhost:1313/aquamega/p/lexical-semantics-and-distributional-semantics/</link>
        <pubDate>Tue, 28 May 2024 00:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/lexical-semantics-and-distributional-semantics/</guid>
        <description>&lt;h2 id=&#34;lexical-semantics&#34;&gt;Lexical Semantics
&lt;/h2&gt;&lt;p&gt;è¯æ±‡è§£é‡Š&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Synonymy åŒä¹‰è¯&lt;/li&gt;
&lt;li&gt;Antonymy åä¹‰è¯&lt;/li&gt;
&lt;li&gt;Hypernymy ä¸Šä¹‰è¯&lt;/li&gt;
&lt;li&gt;Meronymy ä¸‹ä¹‰è¯&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;wordnet&#34;&gt;WordNet
&lt;/h2&gt;&lt;p&gt;A lexical database of English.&lt;/p&gt;
&lt;h2 id=&#34;word-similarity&#34;&gt;Word Similarity
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;è·¯å¾„é•¿åº¦ï¼šåœ¨WordNetä¸­ï¼Œä¸¤ä¸ªè¯ä¹‹é—´çš„è·¯å¾„é•¿åº¦å¯ä»¥ç”¨æ¥è¡¡é‡å®ƒä»¬çš„ç›¸ä¼¼æ€§ã€‚&lt;/li&gt;
&lt;li&gt;æ·±åº¦ä¿¡æ¯ï¼šåœ¨WordNetä¸­ï¼Œè¯çš„æ·±åº¦ï¼ˆåœ¨è¯æ±‡æ ‘ä¸­çš„ä½ç½®ï¼‰ä¹Ÿå¯ä»¥ç”¨æ¥è¡¡é‡è¯çš„ç›¸ä¼¼æ€§ã€‚&lt;/li&gt;
&lt;li&gt;ä¿¡æ¯å†…å®¹ï¼šè¯çš„ä¿¡æ¯å†…å®¹ï¼ˆåœ¨è¯­æ–™åº“ä¸­çš„é¢‘ç‡ï¼‰ä¹Ÿå¯ä»¥ç”¨æ¥è¡¡é‡è¯çš„ç›¸ä¼¼æ€§ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;distributional-semantics&#34;&gt;Distributional Semantics
&lt;/h2&gt;&lt;p&gt;è¯æ±‡å’Œå…¶ä¸Šä¸‹æ–‡çš„å…±ç°ä¿¡æ¯&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Neual Networks</title>
        <link>http://localhost:1313/aquamega/p/neual-networks/</link>
        <pubDate>Tue, 28 May 2024 00:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/neual-networks/</guid>
        <description>&lt;p&gt;ç¥ç»ç½‘ç»œï¼ˆNeural Networkï¼‰æ˜¯ä¸€ç§æ¨¡æ‹Ÿäººè„‘ç¥ç»å…ƒå·¥ä½œæ–¹å¼çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å®ƒç”±å¤§é‡çš„ç¥ç»å…ƒï¼ˆä¹Ÿç§°ä¸ºèŠ‚ç‚¹æˆ–å•å…ƒï¼‰ç»„æˆï¼Œè¿™äº›ç¥ç»å…ƒæŒ‰ç…§ä¸€å®šçš„å±‚æ¬¡ç»“æ„æ’åˆ—ï¼ŒåŒ…æ‹¬è¾“å…¥å±‚ã€éšè—å±‚å’Œè¾“å‡ºå±‚ã€‚&lt;/p&gt;
&lt;p&gt;æ¯ä¸ªç¥ç»å…ƒæ¥æ”¶æ¥è‡ªä¸Šä¸€å±‚ç¥ç»å…ƒçš„è¾“å…¥ï¼Œç„¶åé€šè¿‡ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ Sigmoidã€ReLU ç­‰ï¼‰å¤„ç†è¿™äº›è¾“å…¥ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºï¼Œè¿™ä¸ªè¾“å‡ºå†ä¼ é€’ç»™ä¸‹ä¸€å±‚çš„ç¥ç»å…ƒã€‚&lt;/p&gt;
&lt;p&gt;ç¥ç»ç½‘ç»œçš„è®­ç»ƒé€šå¸¸ä½¿ç”¨åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰ç®—æ³•å’Œæ¢¯åº¦ä¸‹é™ï¼ˆGradient Descentï¼‰ç®—æ³•ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œç¥ç»ç½‘ç»œé€šè¿‡ä¸æ–­è°ƒæ•´ç¥ç»å…ƒä¹‹é—´çš„è¿æ¥æƒé‡ï¼Œæ¥æœ€å°åŒ–é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„å·®å¼‚ã€‚&lt;/p&gt;
&lt;p&gt;ç¥ç»ç½‘ç»œå¯ä»¥å¤„ç†éçº¿æ€§é—®é¢˜ï¼Œé€‚ç”¨äºå„ç§å¤æ‚çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œå¦‚å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚ä½†æ˜¯ï¼Œç¥ç»ç½‘ç»œçš„è®­ç»ƒéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºï¼Œè€Œä¸”æ¨¡å‹çš„è§£é‡Šæ€§è¾ƒå·®ï¼Œè¿™æ˜¯å®ƒçš„ä¸€äº›æŒ‘æˆ˜ã€‚&lt;/p&gt;
&lt;p&gt;A Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.&lt;/p&gt;
&lt;p&gt;Each neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.) to produce an output, which is then passed on to the neurons in the next layer.&lt;/p&gt;
&lt;p&gt;Neural networks are typically trained using the Backpropagation algorithm and the Gradient Descent algorithm. During training, the neural network minimizes the difference between the predicted values and the actual values by continuously adjusting the connection weights between neurons.&lt;/p&gt;
&lt;p&gt;Neural networks can handle non-linear problems and are suitable for various complex machine learning tasks, such as image recognition, speech recognition, natural language processing, etc. However, the training of neural networks requires a large amount of data and computational resources, and the interpretability of the model is relatively poor, which are some of the challenges.&lt;/p&gt;
&lt;h2 id=&#34;å‰é¦ˆç¥ç»ç½‘ç»œ&#34;&gt;å‰é¦ˆç¥ç»ç½‘ç»œ
&lt;/h2&gt;&lt;p&gt;å‰é¦ˆç¥ç»ç½‘ç»œæ˜¯ä¸€ç§åŸºç¡€çš„ç¥ç»ç½‘ç»œï¼Œä¿¡æ¯åªåœ¨ä¸€ä¸ªæ–¹å‘ä¸ŠæµåŠ¨ï¼Œä»è¾“å…¥å±‚åˆ°è¾“å‡ºå±‚ã€‚&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;ä»»åŠ¡ï¼š&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ä¸»é¢˜åˆ†ç±»ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œå¯¹æ–‡æœ¬æˆ–å…¶ä»–æ•°æ®è¿›è¡Œä¸»é¢˜åˆ†ç±»ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è¯­è¨€æ¨¡å‹ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œæ¥ç†è§£å’Œç”Ÿæˆè¯­è¨€ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è¯æ€§æ ‡æ³¨ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¯†åˆ«æ–‡æœ¬ä¸­æ¯ä¸ªè¯çš„è¯æ€§ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;è¯åµŒå…¥ï¼šä¸€ç§è¡¨ç¤ºè¯æ±‡çš„æŠ€æœ¯ï¼Œå°†è¯æ±‡æ˜ å°„åˆ°å‘é‡ç©ºé—´ï¼Œä½¿å¾—è¯­ä¹‰ç›¸è¿‘çš„è¯æ±‡åœ¨å‘é‡ç©ºé—´ä¸­çš„è·ç¦»ä¹Ÿç›¸è¿‘ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;å·ç§¯ç½‘ç»œï¼šä¸€ç§ç‰¹æ®Šçš„ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«é€‚åˆå¤„ç†ç½‘æ ¼å½¢å¼çš„æ•°æ®ï¼Œå¦‚å›¾åƒã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;å¾ªç¯ç¥ç»ç½‘ç»œ&#34;&gt;å¾ªç¯ç¥ç»ç½‘ç»œ
&lt;/h2&gt;&lt;p&gt;å¾ªç¯ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å¯ä»¥å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œå®ƒå¯ä»¥åˆ©ç”¨å‰é¢çš„ä¿¡æ¯æ¥å½±å“åé¢çš„è¾“å‡ºã€‚&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RNNè¯­è¨€æ¨¡å‹ï¼šä½¿ç”¨å¾ªç¯ç¥ç»ç½‘ç»œæ¥ç†è§£å’Œç”Ÿæˆè¯­è¨€ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†æ–‡æœ¬ç­‰åºåˆ—æ•°æ®ã€‚&lt;/li&gt;
&lt;li&gt;LSTMï¼š
&lt;ol&gt;
&lt;li&gt;é—¨çš„åŠŸèƒ½ï¼šLSTMæ˜¯ä¸€ç§ç‰¹æ®Šçš„RNNï¼Œå®ƒæœ‰ä¸‰ä¸ªé—¨ï¼ˆè¾“å…¥é—¨ã€é—å¿˜é—¨å’Œè¾“å‡ºé—¨ï¼‰æ¥æ§åˆ¶ä¿¡æ¯çš„æµåŠ¨ã€‚&lt;/li&gt;
&lt;li&gt;å˜ä½“ï¼šLSTMæœ‰å¤šç§å˜ä½“ï¼Œå¦‚GRUï¼ˆé—¨æ§å¾ªç¯å•å…ƒï¼‰ï¼Œå®ƒç®€åŒ–äº†LSTMçš„ç»“æ„ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;ä»»åŠ¡ï¼š
&lt;ol&gt;
&lt;li&gt;æ–‡æœ¬åˆ†ç±»ï¼šæƒ…æ„Ÿåˆ†æï¼šä½¿ç”¨RNNè¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œå¦‚æƒ…æ„Ÿåˆ†æï¼Œåˆ¤æ–­æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘ã€‚&lt;/li&gt;
&lt;li&gt;è¯æ€§æ ‡æ³¨ï¼šä½¿ç”¨RNNæ¥è¯†åˆ«æ–‡æœ¬ä¸­æ¯ä¸ªè¯çš„è¯æ€§ã€‚&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;éçº¿æ€§æ¿€æ´»å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†å®ƒå¯ä»¥å¸®åŠ©ç¥ç»ç½‘ç»œå­¦ä¹ å’Œé€¼è¿‘å¤æ‚çš„æ¨¡å¼éçº¿æ€§æ¿€æ´»å‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯å°†è¾“å…¥ä¿¡å·è½¬æ¢ä¸ºè¾“å‡ºä¿¡å·ä½†å®ƒä¸ä»…ä»…æ˜¯å¤åˆ¶è¾“å…¥åˆ°è¾“å‡ºå®ƒä¼šä¿®æ”¹æˆ–è€…å˜æ¢è¾“å…¥çš„æ–¹å¼ä½¿å¾—å¯ä»¥é€‚åº”ç½‘ç»œçš„éœ€è¦&#34;&gt;éçº¿æ€§æ¿€æ´»å‡½æ•°æ˜¯ç¥ç»ç½‘ç»œä¸­çš„å…³é”®ç»„æˆéƒ¨åˆ†ï¼Œå®ƒå¯ä»¥å¸®åŠ©ç¥ç»ç½‘ç»œå­¦ä¹ å’Œé€¼è¿‘å¤æ‚çš„æ¨¡å¼ã€‚éçº¿æ€§æ¿€æ´»å‡½æ•°çš„ä¸»è¦ç›®çš„æ˜¯å°†è¾“å…¥ä¿¡å·è½¬æ¢ä¸ºè¾“å‡ºä¿¡å·ï¼Œä½†å®ƒä¸ä»…ä»…æ˜¯å¤åˆ¶è¾“å…¥åˆ°è¾“å‡ºã€‚å®ƒä¼šä¿®æ”¹æˆ–è€…å˜æ¢è¾“å…¥çš„æ–¹å¼ï¼Œä½¿å¾—å¯ä»¥é€‚åº”ç½‘ç»œçš„éœ€è¦ã€‚
&lt;/h2&gt;&lt;p&gt;å¸¸è§çš„éçº¿æ€§æ¿€æ´»å‡½æ•°åŒ…æ‹¬ï¼š&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ReLUï¼ˆRectified Linear Unitï¼‰ï¼šè¿™æ˜¯æœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œå®ƒå°†æ‰€æœ‰è´Ÿå€¼è®¾ä¸º0ï¼Œæ­£å€¼ä¿æŒä¸å˜ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sigmoidï¼šè¿™ä¸ªå‡½æ•°å°†ä»»ä½•æ•°å€¼éƒ½æ˜ å°„åˆ°0å’Œ1ä¹‹é—´ï¼Œå®ƒåœ¨æ—©æœŸçš„ç¥ç»ç½‘ç»œä¸­éå¸¸æµè¡Œï¼Œä½†ç°åœ¨å·²ç»è¾ƒå°‘ä½¿ç”¨ï¼Œå› ä¸ºå®ƒåœ¨è¾“å…¥å€¼å¾ˆå¤§æˆ–å¾ˆå°çš„æ—¶å€™ï¼Œæ¢¯åº¦å‡ ä¹ä¸º0ï¼Œè¿™ä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tanhï¼ˆHyperbolic Tangentï¼‰ï¼šè¿™ä¸ªå‡½æ•°å°†ä»»ä½•æ•°å€¼éƒ½æ˜ å°„åˆ°-1å’Œ1ä¹‹é—´ï¼Œå®ƒæ¯”sigmoidå‡½æ•°çš„è¾“å‡ºèŒƒå›´æ›´å¹¿ï¼Œå› æ­¤åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå®ƒçš„æ€§èƒ½æ¯”sigmoidå‡½æ•°æ›´å¥½ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Softmaxï¼šè¿™ä¸ªå‡½æ•°é€šå¸¸ç”¨åœ¨ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤„ç†å¤šåˆ†ç±»é—®é¢˜æ—¶ã€‚å®ƒå¯ä»¥å°†ä¸€ç»„æ•°å€¼è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;regularization-æ­£åˆ™åŒ–&#34;&gt;Regularization æ­£åˆ™åŒ–
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;L1 Norm&lt;/li&gt;
&lt;li&gt;L2 Norm&lt;/li&gt;
&lt;li&gt;Dropout&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lstm&#34;&gt;LSTM
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Memory cellï¼šè®°å¿†å•å…ƒï¼Œç”¨æ¥å­˜å‚¨ä¿¡æ¯&lt;/li&gt;
&lt;li&gt;Hidden stateï¼šéšè—çŠ¶æ€ï¼Œç”¨æ¥ä¼ é€’ä¿¡æ¯&lt;/li&gt;
&lt;li&gt;Forget gateï¼šé—å¿˜é—¨ï¼Œç”¨æ¥æ§åˆ¶è®°å¿†å•å…ƒä¸­çš„ä¿¡æ¯æ˜¯å¦è¢«é—å¿˜&lt;/li&gt;
&lt;li&gt;Input gateï¼šè¾“å…¥é—¨ï¼Œç”¨æ¥æ§åˆ¶æ–°çš„ä¿¡æ¯æ˜¯å¦è¢«å­˜å‚¨åˆ°è®°å¿†å•å…ƒä¸­&lt;/li&gt;
&lt;li&gt;Output gateï¼šè¾“å‡ºé—¨ï¼Œç”¨æ¥æ§åˆ¶éšè—çŠ¶æ€ä¸­çš„ä¿¡æ¯æ˜¯å¦è¢«ä¼ é€’åˆ°ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Hidden Markov Model</title>
        <link>http://localhost:1313/aquamega/p/hidden-markov-model/</link>
        <pubDate>Mon, 27 May 2024 03:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/hidden-markov-model/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;wï¼šword&lt;/li&gt;
&lt;li&gt;tï¼štag&lt;/li&gt;
&lt;li&gt;emission probability: $P(w|t)$ ä»tagåˆ°wordçš„æ¦‚ç‡&lt;/li&gt;
&lt;li&gt;transition probability: $P(t|t&amp;rsquo;)$ ä»ä¸Šä¸€ä¸ªtagåˆ°å½“å‰tagçš„æ¦‚ç‡&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;viterbi-algorithm&#34;&gt;Viterbi Algorithm
&lt;/h2&gt;&lt;p&gt;ä»æœ€å¼€å§‹ç®—èµ·ï¼Œæ¯ä¸ªtagçš„æ¦‚ç‡æ˜¯ç”±å‰ä¸€ä¸ªtagçš„æ¦‚ç‡å’Œä»å‰ä¸€ä¸ªtagåˆ°å½“å‰tagçš„æ¦‚ç‡ç›¸ä¹˜å¾—åˆ°çš„ã€‚&lt;/p&gt;
&lt;p&gt;$\hat{t} = \arg\max_{t} \prod_{i=1}^{n} P(w_i|t)P(t|t&amp;rsquo;)$, $t&amp;rsquo;$æ˜¯å‰ä¸€ä¸ªtag, $\hat{t}$æ˜¯å½“å‰çš„ä¼°è®¡å€¼ã€‚&lt;/p&gt;
&lt;h2 id=&#34;generative-vs-discriminative-tagger&#34;&gt;Generative vs Discriminative Tagger
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;HMMæ˜¯generative modelï¼Œå¯ä»¥ç”¨æ¥ç”Ÿæˆæ•°æ®ï¼Œæ— ç›‘ç£çš„å­¦ä¹ ã€‚&lt;/li&gt;
&lt;li&gt;discriminative modelä¾‹å¦‚CRFï¼Œéœ€è¦æœ‰ç›‘ç£çš„æ•°æ®ï¼Œå¯ä»¥å­¦åˆ°æ›´ä¸°å¯Œçš„ç‰¹å¾ï¼Œå¤§å¤šæ•°deep learningçš„æ¨¡å‹éƒ½æ˜¯discriminative modelã€‚&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Text Classification</title>
        <link>http://localhost:1313/aquamega/p/text-classification/</link>
        <pubDate>Mon, 27 May 2024 02:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/text-classification/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;Topic classification&lt;/li&gt;
&lt;li&gt;Sentiment analysis&lt;/li&gt;
&lt;li&gt;Native language identification&lt;/li&gt;
&lt;li&gt;Natual language inference&lt;/li&gt;
&lt;li&gt;Automatic fact-checking&lt;/li&gt;
&lt;li&gt;Paraphrase&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;topic-classification&#34;&gt;Topic Classification
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Motivation: library science, information retrieval, etc.&lt;/li&gt;
&lt;li&gt;Classes: topic categories&lt;/li&gt;
&lt;li&gt;Features: bag-of-words, n-grams, etc.&lt;/li&gt;
&lt;li&gt;Corpus: reuters news corpus, pubmed abstracts, tweets with hashtags&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sentiment-analysis&#34;&gt;Sentiment Analysis
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Motivation: opinion mining, business analytics&lt;/li&gt;
&lt;li&gt;Classes: positive, negative, neutral&lt;/li&gt;
&lt;li&gt;Features: n-grams, polarity lexicons, etc.&lt;/li&gt;
&lt;li&gt;Corpus: movie reviews, SEMEVAL twitter polarity dataset&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;native-language-identification&#34;&gt;Native Language Identification
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Motivation: forensic linguistics, educational applications&lt;/li&gt;
&lt;li&gt;Classes: native languages&lt;/li&gt;
&lt;li&gt;Features: character n-grams, syntactic features (POS), phonological features&lt;/li&gt;
&lt;li&gt;Corpus: TOEFL/IELTS essays&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;natural-language-inference-textual-entailment-æ–‡æœ¬è•´å«&#34;&gt;Natural Language Inference (textual entailment æ–‡æœ¬è•´å«)
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Motivation: language understanding&lt;/li&gt;
&lt;li&gt;Classes: entailment, contradiction, neutral&lt;/li&gt;
&lt;li&gt;Features: word overlap, length difference, etc.&lt;/li&gt;
&lt;li&gt;Corpus: SNLI, MultiNLI&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;build-a-text-classifier&#34;&gt;Build a Text Classifier
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Identify the task&lt;/li&gt;
&lt;li&gt;Collect and preprocess data&lt;/li&gt;
&lt;li&gt;Annotation&lt;/li&gt;
&lt;li&gt;Select features&lt;/li&gt;
&lt;li&gt;Select an algorithm&lt;/li&gt;
&lt;li&gt;Train and tune the model&lt;/li&gt;
&lt;li&gt;Evaluate the model&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;naive-bayes-classifier&#34;&gt;Naive Bayes Classifier
&lt;/h2&gt;&lt;p&gt;assume independence between features&lt;/p&gt;
&lt;p&gt;$$P(c|d) = \frac{P(d|c)P(c)}{P(d)}$$&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast, simple&lt;/li&gt;
&lt;li&gt;Robust, low-variance&lt;/li&gt;
&lt;li&gt;Optimal if independence holds&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rarely holds&lt;/li&gt;
&lt;li&gt;Low accuracy&lt;/li&gt;
&lt;li&gt;Smoothing needed for unseen&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;logistic-regression&#34;&gt;Logistic Regression
&lt;/h2&gt;&lt;p&gt;$$P(c|d) = \frac{1}{1 + e^{-\sum_{i=1}^n w_i x_i}}$$&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better performance&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;li&gt;Scaling needed&lt;/li&gt;
&lt;li&gt;Regularization needed for overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;support-vector-machine&#34;&gt;Support Vector Machine
&lt;/h2&gt;&lt;p&gt;To find the hyperplane that maximizes the margin&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast and accurate&lt;/li&gt;
&lt;li&gt;Non-linear kernel&lt;/li&gt;
&lt;li&gt;Work well in big data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multi-class classification&lt;/li&gt;
&lt;li&gt;Scaling needed&lt;/li&gt;
&lt;li&gt;Imbalanced data&lt;/li&gt;
&lt;li&gt;Interpretability&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;k-nearest-neighbors&#34;&gt;K-Nearest Neighbors
&lt;/h2&gt;&lt;p&gt;Euclidean distance, cosine similarity&lt;/p&gt;
&lt;p&gt;Probs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple&lt;/li&gt;
&lt;li&gt;No training needed&lt;/li&gt;
&lt;li&gt;Multi-class classification&lt;/li&gt;
&lt;li&gt;Optimal with infinite data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set k&lt;/li&gt;
&lt;li&gt;Imbalanced classes&lt;/li&gt;
&lt;li&gt;Slow&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;decision-tree&#34;&gt;Decision Tree
&lt;/h2&gt;&lt;p&gt;Greedy maxmize information gain&lt;/p&gt;
&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast&lt;/li&gt;
&lt;li&gt;Non-linear&lt;/li&gt;
&lt;li&gt;Good for small data&lt;/li&gt;
&lt;li&gt;Feature scaling irrelevant&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not that interpretable&lt;/li&gt;
&lt;li&gt;Redundant features&lt;/li&gt;
&lt;li&gt;Not competitive for big data&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;random-forest&#34;&gt;Random Forest
&lt;/h2&gt;&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better than decision tree&lt;/li&gt;
&lt;li&gt;Parallelizable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Same as decision tree&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;neural-network&#34;&gt;Neural Network
&lt;/h2&gt;&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Powerful&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hyperparameters&lt;/li&gt;
&lt;li&gt;Training time&lt;/li&gt;
&lt;li&gt;Overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;evaluation&#34;&gt;Evaluation
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Accuracy&lt;/p&gt;
&lt;p&gt;$$\frac{TP + TN}{TP + TN + FP + FN}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Precision&lt;/p&gt;
&lt;p&gt;$$\frac{TP}{TP + FP}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Recall&lt;/p&gt;
&lt;p&gt;$$\frac{TP}{TP + FN}$$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;F1-score&lt;/p&gt;
&lt;p&gt;$$\frac{2 \times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}$$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Text Classification</title>
        <link>http://localhost:1313/aquamega/p/text-classification/</link>
        <pubDate>Mon, 27 May 2024 02:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/text-classification/</guid>
        <description>&lt;p&gt;½œç”¨&amp;quot;ï¼ˆSide Effectï¼‰æ˜¯æŒ‡å‡½æ•°æˆ–è¡¨è¾¾å¼åœ¨è®¡ç®—ç»“æœä»¥å¤–ï¼Œå¯¹å¤–éƒ¨ä¸–ç•Œäº§ç”Ÿçš„å½±å“ã€‚è¿™äº›å½±å“å¯èƒ½åŒ…æ‹¬æ›´æ”¹å…¨å±€å˜é‡çš„å€¼ï¼Œä¿®æ”¹è¾“å…¥å‚æ•°ï¼Œæ‰§è¡Œ I/O æ“ä½œï¼Œç­‰ç­‰ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ Python ä¸­ï¼Œè®¸å¤šå‡½æ•°å’Œæ–¹æ³•éƒ½æœ‰å‰¯ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œåˆ—è¡¨çš„ &lt;code&gt;append&lt;/code&gt; æ–¹æ³•å°±æœ‰å‰¯ä½œç”¨ï¼Œå®ƒä¼šæ”¹å˜åˆ—è¡¨çš„å†…å®¹ï¼š&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;add_element&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;my_list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;my_list&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;element&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;numbers&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;add_element&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;numbers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;numbers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# è¾“å‡ºï¼š[1, 2, 3, 4]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œ&lt;code&gt;add_element&lt;/code&gt; å‡½æ•°æœ‰ä¸€ä¸ªå‰¯ä½œç”¨ï¼Œå³ä¿®æ”¹äº†è¾“å…¥å‚æ•° &lt;code&gt;my_list&lt;/code&gt; çš„å†…å®¹ã€‚&lt;/p&gt;
&lt;p&gt;åœ¨ç¼–ç¨‹æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸åº”å°½é‡é¿å…å‰¯ä½œç”¨ï¼Œå› ä¸ºå®ƒä»¬ä¼šä½¿ä»£ç æ›´éš¾ç†è§£å’Œæµ‹è¯•ã€‚æ— å‰¯ä½œç”¨çš„å‡½æ•°ï¼ˆä¹Ÿç§°ä¸ºçº¯å‡½æ•°ï¼‰åªä¾èµ–äºè¾“å…¥å‚æ•°ï¼Œå¹¶ä¸”åªé€šè¿‡è¿”å›å€¼ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’ï¼Œè¿™ä½¿å¾—å®ƒä»¬çš„è¡Œä¸ºæ›´åŠ å¯é¢„æµ‹å’Œå¯é‡ç”¨ã€‚ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½&lt;/p&gt;
</description>
        </item>
        <item>
        <title>N-gram Language Model</title>
        <link>http://localhost:1313/aquamega/p/n-gram-language-model/</link>
        <pubDate>Mon, 27 May 2024 01:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/n-gram-language-model/</guid>
        <description>&lt;h2 id=&#34;language-model&#34;&gt;Language Model
&lt;/h2&gt;&lt;p&gt;To explain language, based on probability, for generation.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Query completion(æœç´¢å¼•æ“)&lt;/li&gt;
&lt;li&gt;Optical character recognition(OCR)&lt;/li&gt;
&lt;li&gt;Translation&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;n-gram&#34;&gt;N-gram
&lt;/h3&gt;&lt;p&gt;$$P(w_1, w_2, \cdots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2) \cdots P(w_n|w_1, w_2, \cdots, w_{n-1})$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Maximum Likelihood Estimation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$&lt;/p&gt;
&lt;p&gt;Use special tags to denote the beginning and end of a sentence, i.e. &lt;code&gt;&amp;lt;s&amp;gt;&lt;/code&gt;, &lt;code&gt;&amp;lt;/s&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;smoothing&#34;&gt;Smoothing
&lt;/h3&gt;&lt;p&gt;Handle unseen words.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Laplacian (add-one) smoothing&lt;/p&gt;
&lt;p&gt;$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add-k smoothing&lt;/p&gt;
&lt;p&gt;$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + kV}$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Lidstone smoothing&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generalized Add-k, æ‰€æœ‰éè´Ÿå®æ•°&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Absolute discounting&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Borrow a bit of probability mass from seen words to unseen wordsï¼Œ æ¯ä¸ªå€Ÿä¸€ç‚¹ï¼Œå‡æ‘Šç»™æ²¡è§è¿‡çš„è¯&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Katz Backoff&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use lower-order n-gram&lt;/li&gt;
&lt;li&gt;Issueï¼šä½é˜¶n-gramå¯èƒ½æœ‰å¾ˆé«˜æ¦‚ç‡ï¼Œä½†æ­¤n-gramç»„åˆæ˜¾ç„¶ä¸ä¼šå­˜åœ¨&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Knser-Ney&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Versatility of lower-order n-gram, co-occur with many words&lt;/li&gt;
&lt;li&gt;è®©æ›´é€šç”¨çš„ä½é˜¶n-gramæœ‰æ›´é«˜çš„æ¦‚ç‡&lt;/li&gt;
&lt;li&gt;contiuation probability $P_{cont}(w_i) = \frac{|{w_{i-1}:C(w_{i-1}, w_i) &amp;gt; 0}|}{\sum_{w_i}C(w_{i-1}, w_i)}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Interpolation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Combine lower-order n-gram&lt;/li&gt;
&lt;li&gt;ç»™ä¸åŒé˜¶çš„n-gramåŠ æƒï¼Œç›¸åŠ &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Preprocessing</title>
        <link>http://localhost:1313/aquamega/p/preprocessing/</link>
        <pubDate>Mon, 27 May 2024 00:56:05 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/preprocessing/</guid>
        <description>&lt;h2 id=&#34;preprocessing&#34;&gt;Preprocessing
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Word&lt;/li&gt;
&lt;li&gt;Sentence: words&lt;/li&gt;
&lt;li&gt;Document: sentences&lt;/li&gt;
&lt;li&gt;Corpus: documents&lt;/li&gt;
&lt;li&gt;Word Token: word instance&lt;/li&gt;
&lt;li&gt;Word Type: verb, noun, etc.&lt;/li&gt;
&lt;li&gt;Lexicon: word types&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;steps&#34;&gt;Steps
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Remove unwnated formatting, i.e. HTML tags&lt;/li&gt;
&lt;li&gt;Sentecne segmentation, break text into sentences&lt;/li&gt;
&lt;li&gt;Tokenization, break sentences into words&lt;/li&gt;
&lt;li&gt;Normalization, transform words into canonical form, i.e. lower case&lt;/li&gt;
&lt;li&gt;Stopword removal, delete unwanted words, i.e., &amp;ldquo;the&amp;rdquo;, &amp;ldquo;I&amp;rdquo;, &amp;ldquo;.&amp;rdquo;, etc.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;max-match-algorithm&#34;&gt;Max Match Algorithm
&lt;/h3&gt;&lt;p&gt;Greedily match the longest word in the dictionary. Used in the language withoout space between words, i.e. Chinese.&lt;/p&gt;
&lt;h3 id=&#34;byte-pair-encoding&#34;&gt;Byte Pair Encoding
&lt;/h3&gt;&lt;p&gt;Iteratively merge the most frequent pair of bytes. Used in ChatGPT.&lt;/p&gt;
&lt;p&gt;Adv:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data-informed tokenization&lt;/li&gt;
&lt;li&gt;Works for different languages&lt;/li&gt;
&lt;li&gt;Deals better with unknown words&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disadv:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Rarer words will be split into subwords, even individual letter&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;word-normalization&#34;&gt;Word Normalization
&lt;/h3&gt;&lt;p&gt;Goal: reduce vocabulary size, map different forms of a word to the same form&lt;/p&gt;
&lt;p&gt;å½¢æ€å¤„ç†ï¼ˆMorphological processingï¼‰é€šå¸¸åŒ…æ‹¬è¯å¹²æå–ï¼ˆstemmingï¼‰å’Œè¯å½¢è¿˜åŸï¼ˆlemmatizationï¼‰&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lowercasing&lt;/li&gt;
&lt;li&gt;Remove morphology, i.e. &amp;ldquo;running&amp;rdquo; -&amp;gt; &amp;ldquo;run&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Correct spelling, i.e. &amp;ldquo;colour&amp;rdquo; -&amp;gt; &amp;ldquo;color&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Expand abbreviations, i.e. &amp;ldquo;can&amp;rsquo;t&amp;rdquo; -&amp;gt; &amp;ldquo;cannot&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inflectional-morphologyå±ˆæŠ˜å½¢æ€&#34;&gt;Inflectional Morphology(å±ˆæŠ˜å½¢æ€)
&lt;/h3&gt;&lt;p&gt;It creates grammatical variants of a word, i.e. &amp;ldquo;run&amp;rdquo; -&amp;gt; &amp;ldquo;running&amp;rdquo;, &amp;ldquo;ran&amp;rdquo;, &amp;ldquo;runs&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Nouns, verbs, adjectives, adverbs&amp;hellip;&lt;/p&gt;
&lt;h3 id=&#34;lemmatizationè¯å½¢è¿˜åŸ&#34;&gt;Lemmatization(è¯å½¢è¿˜åŸ)
&lt;/h3&gt;&lt;p&gt;To remove inflection.&lt;/p&gt;
&lt;p&gt;Lemma(è¯å…ƒ): the uninflated form&lt;/p&gt;
&lt;h3 id=&#34;derivational-morphologyæ´¾ç”Ÿå½¢æ€&#34;&gt;Derivational Morphology(æ´¾ç”Ÿå½¢æ€)
&lt;/h3&gt;&lt;p&gt;It creates new words.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change the lexical category(), i.e. &amp;ldquo;happy&amp;rdquo; -&amp;gt; &amp;ldquo;happiness&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Change the meaning, i.e. &amp;ldquo;happy&amp;rdquo; -&amp;gt; &amp;ldquo;unhappy&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stemmizationè¯å¹²æå–&#34;&gt;Stemmization(è¯å¹²æå–)
&lt;/h3&gt;&lt;p&gt;To remove derivation.&lt;/p&gt;
&lt;p&gt;Stem(è¯å¹²): the root form&lt;/p&gt;
&lt;h3 id=&#34;the-porter-stemmer&#34;&gt;The Porter Stemmer
&lt;/h3&gt;&lt;p&gt;most widely used in English&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C: consonant(è¾…éŸ³)&lt;/li&gt;
&lt;li&gt;V: vowel(å…ƒéŸ³)&lt;/li&gt;
&lt;li&gt;m: measure $&lt;a class=&#34;link&#34; href=&#34;VC&#34; &gt;C&lt;/a&gt;^m[V]$&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;stopword-removal&#34;&gt;Stopword Removal
&lt;/h3&gt;&lt;p&gt;Typically in bag-of-words model&lt;/p&gt;
&lt;p&gt;All closed-class or function words, any high frequency words&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>

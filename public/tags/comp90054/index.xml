<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Comp90054 on Aqua Mega</title>
    <link>http://localhost:1313/aquamega/tags/comp90054/</link>
    <description>Recent content in Comp90054 on Aqua Mega</description>
    <image>
      <title>Aqua Mega</title>
      <url>https://github.com/shyu216.png</url>
      <link>https://github.com/shyu216.png</link>
    </image>
    <generator>Hugo -- 0.125.4</generator>
    <language>en</language>
    <lastBuildDate>Thu, 30 May 2024 11:13:45 +1000</lastBuildDate>
    <atom:link href="http://localhost:1313/aquamega/tags/comp90054/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Game Theory</title>
      <link>http://localhost:1313/aquamega/algorithm/game/</link>
      <pubDate>Thu, 30 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/game/</guid>
      <description>pure Strategy: single action
mixed Strategy: probability distribution over actions
weakly dominate: $\leq$
strongly dominate: $&lt;$
a weakly(strictly) dominant strategy: always better than any other strategy
nash equilibrium: 每个agent都选了最优策略，其他agent不会改变策略
indifference: 通过调整我的策略概率，改变对手的收益（payoff）期望，使无论对手如何选择，他的满意度（utility）都一样
Utility Function U_i(a): what can agent i get from action a
Normal Form Game 一轮，不知道对手的策略，只知道对手的utility function
Extensive Form Game 广义形式博弈 轮流决策，所以知道对手的策略
Subgame Perfect Equilibrium 子博弈完美均衡 当前玩家在他的回合的最优策略，对手在他的回合的最优策略。。。
Backward Induction 反向归纳 输入：广义形式博弈 G = (N, Agt, S, s_0, A, T, r) 输出：每个状态 s ∈ S 的子博弈均衡 函数 BackwardInduction(s ∈ S): 如果 A(s) = ∅，则返回 r(s) best_child ← (-∞, .</description>
    </item>
    <item>
      <title>Greedy Relaxed Planning</title>
      <link>http://localhost:1313/aquamega/algorithm/relax/</link>
      <pubDate>Wed, 29 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/relax/</guid>
      <description>h*: the optimal heuristic
h pre&amp;amp;del: the heuristic ignoring preconditions and delete effects, adimissible and consistent, &amp;ldquo;subset sum&amp;rdquo; problem, NP-hard
h goal_count: the number of goals not yet achieved, neither admissible nor consistent
h+/h del: admitssible and consistent, 相当于MST, 每个state只访问一遍，也NP-hard
h add: easily not admissible
h max: easily too small
h ff: use h add and h max
Removing preconditions and delete effects is efficiently constructive but not computable。</description>
    </item>
    <item>
      <title>UCB and Greedy BFS</title>
      <link>http://localhost:1313/aquamega/algorithm/uniform/</link>
      <pubDate>Wed, 29 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/uniform/</guid>
      <description>Uniform Cost Search Priority Queue，最先被探索离起始节点最近（即路径成本最低）的节点。
Breath First Search属于Uniform Cost Search的特例，即每个节点的路径成本都是1。
UCS只看到了路径成本，没有考虑启发式。
def uniformCostSearch(problem): candidate_pq = util.PriorityQueue() init_state = problem.getStartState() init_path = [] candidate_pq.push((init_state,init_path),0) viewed = [] while not candidate_pq.isEmpty(): state,path = candidate_pq.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, child_cost in problem.getSuccessors(state): child_path = path + [child_action] candidate_pq.push((child_state,child_path),problem.getCostOfActions(child_path)) return None Greedy Best First Search BFS只看到了启发式，没有考虑路径成本。
If h=0，BFS是什么根据它的Priority Queue的实现。
def bestFirstSearch(problem, heuristic=nullHeuristic): candidate_pq = util.PriorityQueue() init_state = problem.</description>
    </item>
    <item>
      <title>A* and Hill Climbing</title>
      <link>http://localhost:1313/aquamega/algorithm/astar/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/astar/</guid>
      <description>A*算法（带有重复检测和重新打开） 设 open 为新的优先级队列，按照 g(state(σ)) + h(state(σ)) 升序排列 open.insert(make-root-node(init())) 设 closed 为空集 设 best-g 为空集 /* 将状态映射到数字 */ 当 open 不为空时： σ := open.pop-min() 如果 state(σ) 不在 closed 中或 g(σ) &amp;lt; best-g(state(σ))： /* 如果 g 更好则重新打开；注意所有具有相同状态但 g 较差的 σ′ 都在 open 中位于 σ 后面，并且在轮到它们时将被跳过 */ closed := closed ∪{state(σ)} best-g(state(σ)) := g(σ) 如果 is-goal(state(σ))：返回 extract-solution(σ) 对于每个 (a,s′) ∈succ(state(σ))： σ′ := make-node(σ,a,s′) 如果 h(state(σ′)) &amp;lt; ∞：open.insert(σ′) 返回 unsolvable def aStarSearch(problem, heuristic=nullHeuristic): &amp;#34;&amp;#34;&amp;#34;Search the node that has the lowest combined cost and heuristic first.</description>
    </item>
    <item>
      <title>BFS and DFS</title>
      <link>http://localhost:1313/aquamega/algorithm/bfs/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/bfs/</guid>
      <description>广度优先搜索（BFS） 队列，先进先出，后进后出。
Optimal when costs are uniform
def breadthFirstSearch(problem): candidate_queue = util.Queue() init_state = problem.getStartState() init_path = [] candidate_queue.push((init_state,init_path)) viewed = [] while not candidate_queue.isEmpty(): state,path = candidate_queue.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_queue.push((child_state,child_path)) return None 深度优先搜索（DFS） 栈，先进后出，后进先出。
搜索空间可能无限大（无限深）。
def depthFirstSearch(problem): candidate_stack = util.Stack() init_state = problem.getStartState() init_path = [] candidate_stack.</description>
    </item>
    <item>
      <title>MDPs</title>
      <link>http://localhost:1313/aquamega/algorithm/mdps/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/mdps/</guid>
      <description>Markov Decision Processes（MDPs）马尔可夫决策过程 MDP是完全可观察的，概率状态模型：
状态空间 $S$ 初始状态 $s_0 \in S$ 一组目标状态 $G \subseteq S$ 在每个状态 $s \in S$ 中可应用的动作 $A(s) \subseteq A$ 对于 $s \in S$ 和 $a \in A(s)$，有转移概率 $P_a(s&#39;|s)$ 动作成本 $c(a,s) &gt; 0$
其中：
解决方案是将状态映射到动作的函数（策略）
最优解最小化预期的前往目标的成本
Partially Observable MDPs (POMDPs) 部分可观察的马尔可夫决策过程 POMDP是部分可观察的，概率状态模型：
状态 $s \in S$ 在每个状态 $s \in S$ 中可应用的动作 $A(s) \subseteq A$ 对于 $s \in S$ 和 $a \in A(s)$，有转移概率 $P_a(s&#39;|s)$ 初始信念状态 $b_0$ 最终信念状态 $b_f$ 由概率 $P_a(o|s)$，$o \in Obs$ 给出的传感器模型</description>
    </item>
    <item>
      <title>Comp90054</title>
      <link>http://localhost:1313/aquamega/course/comp90054/</link>
      <pubDate>Fri, 26 Apr 2024 14:55:43 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90054/</guid>
      <description>AI Planning for Autonomy classical planning (blind/heuristic): https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf PDDL relaxation reinforcement learning: https://gibberblot.github.io/rl-notes/index.html Vocabs acyclic 无环
systematics 系统的 / local 局部
heuristic 启发式
monotonic 单调
priority queue = min heap
conformant 符合的
policy 策略，map from state to action, denoted by $\pi$
negation 否定
precondition 前提条件
propositional 命题
predicate 谓词，return true/false
schema 模式，define somthing
misplace 放错
dominate 支配
corollary 推论
point-wise 逐点
pessimistic 悲观
benchmark 基准
novelty 新颖性
prune 修剪
velocity 速度</description>
    </item>
  </channel>
</rss>

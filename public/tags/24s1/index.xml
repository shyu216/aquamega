<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>24s1 on Dale的水硕日记</title>
        <link>http://localhost:1313/aquamega/tags/24s1/</link>
        <description>Recent content in 24s1 on Dale的水硕日记</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 09 Jun 2024 00:00:00 +1000</lastBuildDate><atom:link href="http://localhost:1313/aquamega/tags/24s1/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>COMP90042</title>
        <link>http://localhost:1313/aquamega/p/comp90042/</link>
        <pubDate>Sun, 09 Jun 2024 00:00:00 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/comp90042/</guid>
        <description>&lt;ol&gt;
&lt;li&gt;Preprocessing
&lt;ol&gt;
&lt;li&gt;Sentence segmentation&lt;/li&gt;
&lt;li&gt;Tokenization, subword tokenization&lt;/li&gt;
&lt;li&gt;Word normalization
&lt;ol&gt;
&lt;li&gt;Inflectional vs derivational morphology&lt;/li&gt;
&lt;li&gt;Lemmatization vs stemming&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Stopword removal&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;N-gram Language Model
&lt;ol&gt;
&lt;li&gt;Derivation&lt;/li&gt;
&lt;li&gt;Smoothing techniques
&lt;ol&gt;
&lt;li&gt;Add-k&lt;/li&gt;
&lt;li&gt;Absolute discounting&lt;/li&gt;
&lt;li&gt;Katz backoff&lt;/li&gt;
&lt;li&gt;Kneser-Ney smoothing&lt;/li&gt;
&lt;li&gt;Interpolation&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Text Classification
&lt;ol&gt;
&lt;li&gt;Build a classifier&lt;/li&gt;
&lt;li&gt;Task
&lt;ol&gt;
&lt;li&gt;Topic classification&lt;/li&gt;
&lt;li&gt;Sentiment analysis&lt;/li&gt;
&lt;li&gt;Native language identification&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Algorithms
&lt;ol&gt;
&lt;li&gt;Naive Bayes, logistic regression, SVM&lt;/li&gt;
&lt;li&gt;kNN, neural networks&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Bias vs variance：欠拟合under和过拟合over的取舍&lt;/li&gt;
&lt;li&gt;Evaluation
&lt;ol&gt;
&lt;li&gt;Precision, recall, F1&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Part of Speech Tagging
&lt;ol&gt;
&lt;li&gt;English POS
&lt;ol&gt;
&lt;li&gt;Closed vs open classes&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Tagsets
&lt;ol&gt;
&lt;li&gt;Penn Treebank tagset&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Automatic taggers
&lt;ol&gt;
&lt;li&gt;Rule-based&lt;/li&gt;
&lt;li&gt;Statistical
&lt;ol&gt;
&lt;li&gt;Unigram, classifier-based, HMM&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Hidden Markov Model
&lt;ol&gt;
&lt;li&gt;Probabilistic formulation:
&lt;ol&gt;
&lt;li&gt;Emission &amp;amp; Transition&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Training&lt;/li&gt;
&lt;li&gt;Viterbi algorithm&lt;/li&gt;
&lt;li&gt;Generative vs discriminative models&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Feedforward Neural Network
&lt;ol&gt;
&lt;li&gt;Formulation&lt;/li&gt;
&lt;li&gt;Tasks:
&lt;ol&gt;
&lt;li&gt;Topic classifcation&lt;/li&gt;
&lt;li&gt;Language models&lt;/li&gt;
&lt;li&gt;POS tagging&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Word embeddings&lt;/li&gt;
&lt;li&gt;Convolutional networks&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Recurrent Neural Network
&lt;ol&gt;
&lt;li&gt;Formulation&lt;/li&gt;
&lt;li&gt;RNN: language models&lt;/li&gt;
&lt;li&gt;LSTM:
&lt;ol&gt;
&lt;li&gt;Functions of gates&lt;/li&gt;
&lt;li&gt;Variants&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Tasks:
&lt;ol&gt;
&lt;li&gt;Text classification: sentiment analysis&lt;/li&gt;
&lt;li&gt;POS tagging&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Lexical Semantics
&lt;ol&gt;
&lt;li&gt;Definition of word sense, gloss&lt;/li&gt;
&lt;li&gt;Lexical Relationship:
&lt;ol&gt;
&lt;li&gt;Synonymy, antonymy, hypernymy, meronymy&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Structure of wordnet&lt;/li&gt;
&lt;li&gt;Word similarity
&lt;ol&gt;
&lt;li&gt;Path lenght&lt;/li&gt;
&lt;li&gt;Depth information&lt;/li&gt;
&lt;li&gt;Information content&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Word sense unambiguation
&lt;ol&gt;
&lt;li&gt;supervised, unsupervised&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Distributional Semantics
&lt;ol&gt;
&lt;li&gt;Matrices:
&lt;ol&gt;
&lt;li&gt;VSM, TF-IDF, word-word co-occurrence&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Association measures: PMI, PPMI&lt;/li&gt;
&lt;li&gt;Count-based method: SVM&lt;/li&gt;
&lt;li&gt;Neural method: skip-gram, CBOW&lt;/li&gt;
&lt;li&gt;Evaluation:
&lt;ol&gt;
&lt;li&gt;Word similarity, analogy&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Contextual Representation
&lt;ol&gt;
&lt;li&gt;Formulation with RNN&lt;/li&gt;
&lt;li&gt;ELMo&lt;/li&gt;
&lt;li&gt;BERT
&lt;ol&gt;
&lt;li&gt;Objective&lt;/li&gt;
&lt;li&gt;Fine-tuning for downstream tasks&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Transformers
&lt;ol&gt;
&lt;li&gt;Multi-head attention&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Second half:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Attention
&lt;ol&gt;
&lt;li&gt;Sequential model with attention&lt;/li&gt;
&lt;li&gt;Attention variants:
&lt;ol&gt;
&lt;li&gt;Concat&lt;/li&gt;
&lt;li&gt;Dot product&lt;/li&gt;
&lt;li&gt;Scaled dot product&lt;/li&gt;
&lt;li&gt;Location-based&lt;/li&gt;
&lt;li&gt;Cosine similarity&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Global vs local attention&lt;/li&gt;
&lt;li&gt;Self-attention&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Machine Translation
&lt;ol&gt;
&lt;li&gt;Statistical MT&lt;/li&gt;
&lt;li&gt;Neural MT with teacher forcing&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Transformer
&lt;ol&gt;
&lt;li&gt;Mutli-head self-attention&lt;/li&gt;
&lt;li&gt;Position encoding&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Pretrained Language Models
&lt;ol&gt;
&lt;li&gt;Encoder architecture
&lt;ol&gt;
&lt;li&gt;BERT&lt;/li&gt;
&lt;li&gt;bidirectional context&lt;/li&gt;
&lt;li&gt;good for classification&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Encoder-decoder architecture&lt;/li&gt;
&lt;li&gt;Decoder architecture
&lt;ol&gt;
&lt;li&gt;GPT&lt;/li&gt;
&lt;li&gt;unidirectional context&lt;/li&gt;
&lt;li&gt;good for generation&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Large Language Models
&lt;ol&gt;
&lt;li&gt;In-context learning&lt;/li&gt;
&lt;li&gt;Step-by-step reasoning&lt;/li&gt;
&lt;li&gt;Human-feedback reinforcement&lt;/li&gt;
&lt;li&gt;Instruction following&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Named Entity Recognition
&lt;ol&gt;
&lt;li&gt;Predict entities in a text&lt;/li&gt;
&lt;li&gt;Traditional ML methods&lt;/li&gt;
&lt;li&gt;Bi-LSTM with another RNN/CRF&lt;/li&gt;
&lt;li&gt;Multi-aspect NER&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Coreference Resolution 共指消解
&lt;ol&gt;
&lt;li&gt;Coreference, anaphora&lt;/li&gt;
&lt;li&gt;B-Cubed metric&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Question Answering and Reading Comprehension
&lt;ol&gt;
&lt;li&gt;Knowledge-based QA&lt;/li&gt;
&lt;li&gt;Visual QA&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Spoken Language Understanding
&lt;ol&gt;
&lt;li&gt;Attention RNNs&lt;/li&gt;
&lt;li&gt;Joint BERT&lt;/li&gt;
&lt;li&gt;Tri-level model&lt;/li&gt;
&lt;li&gt;Explainable NLU&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Vision Language Pretrained Model
&lt;ol&gt;
&lt;li&gt;V-L Interaction Model
&lt;ol&gt;
&lt;li&gt;Self-attention, co-attention, VSE&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Constrastive Language-Image Pretraining&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Ethics
&lt;ol&gt;
&lt;li&gt;Sotial bias 刻板印象&lt;/li&gt;
&lt;li&gt;Incivility 仇恨言论&lt;/li&gt;
&lt;li&gt;Privacy Violation 歧视&lt;/li&gt;
&lt;li&gt;Misinformation 谣言&lt;/li&gt;
&lt;li&gt;Technological divide 发展不平衡&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;vocab&#34;&gt;Vocab
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;stemming:词干提取，去掉后缀，通常不是个词&lt;/li&gt;
&lt;li&gt;lemmatisation:词形还原&lt;/li&gt;
&lt;li&gt;morphology &lt;strong&gt;詞法學&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;lexicon 字典&lt;/li&gt;
&lt;li&gt;corpus 语料&lt;/li&gt;
&lt;li&gt;tokenization 分词&lt;/li&gt;
&lt;li&gt;entailment vs. contradiction 蕴涵与矛盾&lt;/li&gt;
&lt;li&gt;neutral 中性&lt;/li&gt;
&lt;li&gt;vanish 消失&lt;/li&gt;
&lt;li&gt;distill 提炼&lt;/li&gt;
&lt;li&gt;lexical 词汇的，与词汇或词语有关&lt;/li&gt;
&lt;li&gt;semantic 语义的，与词语的含义或解释有关&lt;/li&gt;
&lt;li&gt;syntactic 句法的，与句子结构有关&lt;/li&gt;
&lt;li&gt;sentiment 情感，对某事物的态度&lt;/li&gt;
&lt;li&gt;polysemy 一词多义&lt;/li&gt;
&lt;li&gt;gloss 词义, from dictionary，词典中对词语含义的解释&lt;/li&gt;
&lt;li&gt;synonymy 同义词&lt;/li&gt;
&lt;li&gt;antonymy 反义词&lt;/li&gt;
&lt;li&gt;hypernymy 上义词, is-a，是一个更一般的词&lt;/li&gt;
&lt;li&gt;hyponymy 一个更具体的词&lt;/li&gt;
&lt;li&gt;meronymy 下义词, is-part-of&lt;/li&gt;
&lt;li&gt;spectrum 范围&lt;/li&gt;
&lt;li&gt;conference 共指，在文本中指代同一实体&lt;/li&gt;
&lt;li&gt;anaphora 指代，一种语言现象，代替先前提到的词&lt;/li&gt;
&lt;li&gt;utterence 话语&lt;/li&gt;
&lt;li&gt;utter 说，发出声音&lt;/li&gt;
&lt;li&gt;tedious 冗长的工作，乏味，枯燥&lt;/li&gt;
&lt;li&gt;versality 可以在多种不同的情况和环境中使用，具有很高的适应性和灵活性&lt;/li&gt;
&lt;li&gt;continuation 相连&lt;/li&gt;
&lt;li&gt;patch-based 基于块的&lt;/li&gt;
&lt;li&gt;interpretability 可解释性，白盒&lt;/li&gt;
&lt;li&gt;explainability 可解释性，黑盒&lt;/li&gt;
&lt;li&gt;monolingual 只使用一种语言的情况或者系统&lt;/li&gt;
&lt;li&gt;sinusoidal 正弦的&lt;/li&gt;
&lt;li&gt;proximal 接近的&lt;/li&gt;
&lt;li&gt;miscellaneous 混杂的&lt;/li&gt;
&lt;li&gt;antecedent 先行词, Trump&lt;/li&gt;
&lt;li&gt;anaphor 指代词, he&lt;/li&gt;
&lt;li&gt;pronominal 代词的&lt;/li&gt;
&lt;li&gt;adjective 形容词&lt;/li&gt;
&lt;li&gt;factoid 事实&lt;/li&gt;
&lt;li&gt;intent 意图&lt;/li&gt;
&lt;li&gt;clause 子句&lt;/li&gt;
&lt;li&gt;toxicity 有毒&lt;/li&gt;
&lt;li&gt;insult 侮辱&lt;/li&gt;
&lt;li&gt;humiliation 羞辱&lt;/li&gt;
&lt;li&gt;altered 改变&lt;/li&gt;
&lt;li&gt;conjunction 连接词&lt;/li&gt;
&lt;li&gt;conform 符合&lt;/li&gt;
&lt;li&gt;genre 风格&lt;/li&gt;
&lt;li&gt;spurious 伪造的&lt;/li&gt;
&lt;li&gt;occurrence 出现&lt;/li&gt;
&lt;li&gt;maginalisation 边缘化&lt;/li&gt;
&lt;li&gt;nuance 细微差别&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;information-bottleneck&#34;&gt;Information Bottleneck
&lt;/h2&gt;&lt;p&gt;信息瓶颈问题是指在机器学习模型中，模型在尝试压缩输入数据的同时，还要尽可能保留与目标输出相关的信息。这是一个权衡问题，因为模型需要找到一个平衡点，既能有效地压缩数据，又能保留足够的信息来进行准确的预测。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>COMP90054</title>
        <link>http://localhost:1313/aquamega/p/comp90054/</link>
        <pubDate>Sun, 09 Jun 2024 00:00:00 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/comp90054/</guid>
        <description>&lt;ul&gt;
&lt;li&gt;classical planning (blind/heuristic): &lt;a class=&#34;link&#34; href=&#34;https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PDDL&lt;/li&gt;
&lt;li&gt;relaxation&lt;/li&gt;
&lt;li&gt;reinforcement learning: &lt;a class=&#34;link&#34; href=&#34;https://gibberblot.github.io/rl-notes/index.html&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://gibberblot.github.io/rl-notes/index.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;vocabs&#34;&gt;Vocabs
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;acyclic 无环&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;systematics 系统的 / local 局部&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;heuristic 启发式&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;monotonic 单调&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;priority queue = min heap&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;conformant 符合的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;policy 策略，map from state to action, denoted by $\pi$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;negation 否定&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;precondition 前提条件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;propositional 命题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;predicate 谓词，return true/false&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;schema 模式，define somthing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;misplace 放错&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;dominate 支配&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;corollary 推论&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;point-wise 逐点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;pessimistic 悲观&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;benchmark 基准&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;novelty 新颖性&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;prune 修剪&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;velocity 速度&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;stochastic 随机&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;tabular 表格&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;dilemma 困境&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;equilibria 平衡&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;, set minus operation&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;payoff 收益&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;utility 效用&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nondeterministic Polynomial: 不确定多项式，没有已知的多项式时间算法可以在所有情况下找到一个解&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;non-deterministic: countless outcomes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;mixed strategy equilibria: 混合策略均衡&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;nash equilibrium: 纳什均衡，每个玩家都在最佳响应下(pure strategy)，没有人会改变策略&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;安全目标感知可接受一致性&#34;&gt;安全/目标感知/可接受/一致性
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$h$ remaining cost to reach the goal, $*$ optimal&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;假设$\Pi$是一个计划任务，具有状态空间$\Theta_{\Pi} = (S, L, c, T, I, G)$，并且$h$是$\Pi$的一个启发式。如果启发式满足以下条件，那么它被称为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安全（Safe）：如果对于所有$h(s) = \infty$的状态$s \in S$，都有$h^*(s) = \infty$，则启发式被称为安全。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;目标感知（Goal-aware）：如果对于所有目标状态$s \in S_G$，都有$h(s) = 0$，则启发式被称为目标感知。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可接受（Admissible）：如果对于所有状态$s \in S$，都有$h(s) \leq h^*(s)$，则启发式被称为可接受。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一致（Consistent）：如果对于所有$s \xrightarrow{a} s&amp;rsquo;$的转移，都有$h(s) \leq h(s&amp;rsquo;) + c(a)$，则启发式被称为一致。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;命题：假设$\Pi$是一个计划任务，具有状态空间$\Theta_{\Pi} = (S, L, c, T, I, S_G)$，并且$h$是$\Pi$的一个启发式。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果$h$是一致的和目标感知的，则$h$是可接受的。&lt;/li&gt;
&lt;li&gt;如果$h$是可接受的，则$h$是目标感知的。&lt;/li&gt;
&lt;li&gt;如果$h$是可接受的，则$h$是安全的。&lt;/li&gt;
&lt;li&gt;没有其他这种形式的蕴含关系成立。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不可接受：最优的节点如果被高估，就会优先扩展其他节点，而错过最优。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一致性：保证最优路径依次访问。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;strips-问题是一个四元组p--langle-foig-rangle&#34;&gt;STRIPS: 问题是一个四元组$P = \langle F,O,I,G \rangle$：
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$F$ fact, 原子, 变量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$O$ 或 $A$ operator, action, 操作符, 动作&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$I \subseteq F$代表初始情况&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;$G \subseteq F$代表目标情况&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;操作符$o \in O$由以下表示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;添加列表$Add(o) \subseteq F$&lt;/li&gt;
&lt;li&gt;删除列表$Del(o) \subseteq F$&lt;/li&gt;
&lt;li&gt;前提条件列表$Pre(o) \subseteq F$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;relaxiation&#34;&gt;Relaxiation:
&lt;/h2&gt;&lt;p&gt;Goal：Helps compute heuristic function。&lt;/p&gt;
&lt;p&gt;设$h^* : P \rightarrow R^+_0 \cup {\infty}$是一个函数。$h^&lt;em&gt;$的松弛是一个三元组$R= (P&amp;rsquo;,r,h&amp;rsquo;^&lt;/em&gt;)$，其中$P&amp;rsquo;$是任意集合，$r : P \rightarrow P&amp;rsquo;$和$h&amp;rsquo;^* : P&amp;rsquo; \rightarrow R^+_0 \cup {\infty}$是函数，对于所有的$\Pi \in P$，松弛启发式$h_R(\Pi) := h&amp;rsquo;^&lt;em&gt;(r(\Pi))$满足$h_R(\Pi) \leq h^&lt;/em&gt;(\Pi)$。松弛是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题$P$：寻路。&lt;/li&gt;
&lt;li&gt;更简单的问题$P&amp;rsquo;$：鸟类的寻路。&lt;/li&gt;
&lt;li&gt;$P&amp;rsquo;$的完美启发式$h&amp;rsquo;^*$：直线距离。&lt;/li&gt;
&lt;li&gt;转换$r$：假装你是一只鸟。&lt;/li&gt;
&lt;li&gt;原生的，如果$P&amp;rsquo; \subseteq P$且$h&amp;rsquo;^* = h^*$；&lt;/li&gt;
&lt;li&gt;可有效构造的，如果存在一个多项式时间算法，给定$\Pi \in P$，可以计算$r(\Pi)$；&lt;/li&gt;
&lt;li&gt;可有效计算的，如果存在一个多项式时间算法，给定$\Pi&amp;rsquo; \in P&amp;rsquo;$，可以计算$h&amp;rsquo;^*(\Pi&amp;rsquo;)$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;提醒：你有一个问题$P$，你希望估计其完美启发式$h^&lt;em&gt;$。你定义了一个更简单的问题$P&amp;rsquo;$，其完美启发式$h&amp;rsquo;^&lt;/em&gt;$可以用来（可接受地！）估计$h^&lt;em&gt;$。你定义了一个从$P$到$P&amp;rsquo;$的转换$r$。给定$\Pi \in P$，你通过$h&amp;rsquo;^&lt;/em&gt;(r(\Pi))$来估计$h^*(\Pi)$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;notation in this course&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$\Pi_s$：将初始状态替换为$s$的$\Pi$，即，将$\Pi = (F,A,c,I,G)$更改为$(F,A,c,s,G)$。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;c: clause, preconditions+effects&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;有关quality-value的一些概念&#34;&gt;有关Quality Value的一些概念：
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Value：在强化学习中，value通常指的是一个状态的价值，也就是从这个状态开始，遵循某个策略能够获得的预期回报。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q-value：Q-value是对于状态-动作对(state-action pair)的价值的一种评估。也就是说，如果在某个状态下执行某个动作，然后遵循某个策略，能够获得的预期回报。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q-value table：Q-value table是一种数据结构，用于存储每个状态-动作对的Q-value。在表格型强化学习算法（如Q-learning）中，Q-value table是主要的数据结构。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q-value function：Q-value function是一个函数，它接受一个状态和一个动作作为输入，返回这个状态-动作对的Q-value。在函数逼近方法（如深度Q网络）中，Q-value function通常由神经网络来表示。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Q-table：Q-table和Q-value table是同一个概念，只是名称不同。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>COMP90077</title>
        <link>http://localhost:1313/aquamega/p/comp90077/</link>
        <pubDate>Sun, 09 Jun 2024 00:00:00 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/comp90077/</guid>
        <description>&lt;p&gt;Learn some advanced algorithms and data structures.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Treap&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Admortized Analysis: Prepaid/Potential&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quake Heap&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Splay Tree&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Perfect Hashing/Cuckoo Hashing&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Range Tree&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Min Cut/Max Flow&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Karger&amp;rsquo;s algorithm: 找最小割, 找多次取最优, 随机地找两个节点合并, 直到只剩下两个节点&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ford-Fulkerson algorithm: 最早的最大流算法, 重复地找增广路径, 直到找不到&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Edmonds-Karp algorithm: 用BFS找增广路径, complexity更低&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hall&amp;rsquo;s theorem: 一个二分图存在完美匹配当且仅当对于每一个子集, 子集的大小大于等于子集的邻居的大小&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;readings&#34;&gt;Readings
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Karger&amp;rsquo;s Randomised Contraction algorithm: Chapter 13.2 of [KT]&lt;/li&gt;
&lt;li&gt;Flow networks, max flow, min cut and basic Ford-Fulkerson: Chapter 7.1 - 7.2 of [KT] and also Chapter 10.1 - 10.4 of [JE]&lt;/li&gt;
&lt;li&gt;Flow network applications (Bipartite Matching and Disjoint Paths): Chapter 7.5-7.6 of [KT]&lt;/li&gt;
&lt;li&gt;Capacity-Scaling: Chapter 7.3 of [KT]Edmonds-Karps algorithms: Chapter 10.6 of [JE]&lt;/li&gt;
&lt;li&gt;Circulation with demands: Chapter 7.7 of [KT]&lt;/li&gt;
&lt;li&gt;Linear Programming: &lt;a class=&#34;link&#34; href=&#34;https://jeffe.cs.illinois.edu/teaching/algorithms/notes/H-lp.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://jeffe.cs.illinois.edu/teaching/algorithms/notes/H-lp.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Approximation Algorithms: Chapter 1 of &lt;a class=&#34;link&#34; href=&#34;https://www.designofapproxalgs.com/book.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.designofapproxalgs.com/book.pdf&lt;/a&gt; and see also Approximation Algorithms by Vazirani&lt;/li&gt;
&lt;li&gt;[JE] = &lt;a class=&#34;link&#34; href=&#34;https://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[KT] = Algorithm Design by Kleinberg and Tardos&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;vocabs&#34;&gt;Vocabs
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;conservation node: 保守节点, a node that has the same flow in and out&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;residual graph: 残余图, a graph that represents the remaining capacity of each edge&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;augmenting path: 增广路径, a path from source to sink in the residual graph&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;feasible flow: 可行流, a flow that satisfies the capacity constraints and conservation constraints&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;perfect matching: 完美匹配, a matching that covers all the nodes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;bipartite graph: 二分图, a graph that can be divided into two sets such that all edges are between the two sets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;disjoint paths: 不相交路径, paths that do not share any nodes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;vertex cover: 顶点覆盖, a set of vertices that covers all the edges&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;maximal matching: 最大匹配, a matching that cannot be extended by adding another edge&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;cardinality 基数，集合中元素的数量&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>GEOM90008</title>
        <link>http://localhost:1313/aquamega/p/geom90008/</link>
        <pubDate>Sun, 09 Jun 2024 00:00:00 +1000</pubDate>
        
        <guid>http://localhost:1313/aquamega/p/geom90008/</guid>
        <description>&lt;p&gt;Course notebook: &lt;a class=&#34;link&#34; href=&#34;https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;postgreSQL&lt;/li&gt;
&lt;li&gt;postGIS&lt;/li&gt;
&lt;li&gt;QGIS&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;vocabs&#34;&gt;Vocabs
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;scope creep: 任务蔓延&lt;/li&gt;
&lt;li&gt;schema: 数据库的结构&lt;/li&gt;
&lt;li&gt;query: 查询&lt;/li&gt;
&lt;li&gt;ACID: Atomicity 原子性, Consistency 一致性, Isolation 隔离性, Durability 持久性&lt;/li&gt;
&lt;li&gt;waterfall: 瀑布模型, 一次性完成所有工作&lt;/li&gt;
&lt;li&gt;agile: 敏捷开发, 分阶段完成工作&lt;/li&gt;
&lt;li&gt;ERD/ERM: Entity Relationship Diagram/Model, 实体关系图/模型&lt;/li&gt;
&lt;li&gt;entity, attribute, relationship: 实体, 属性, 关系&lt;/li&gt;
&lt;li&gt;location: space and time&lt;/li&gt;
&lt;li&gt;position: reference to a coordinate system&lt;/li&gt;
&lt;li&gt;地理學第一定律: 所有事物都與其他事物相關, 但是近處的事物比遠處的事物更相關&lt;/li&gt;
&lt;li&gt;longitude: 经度&lt;/li&gt;
&lt;li&gt;latitude: 纬度&lt;/li&gt;
&lt;li&gt;altitude: 海拔&lt;/li&gt;
&lt;li&gt;wgs84: 地球坐标系, 经典的经纬度&lt;/li&gt;
&lt;li&gt;polynomial: 多项式&lt;/li&gt;
&lt;li&gt;ployline: 折线&lt;/li&gt;
&lt;li&gt;polygon: 多边形&lt;/li&gt;
&lt;li&gt;vertex: 顶点&lt;/li&gt;
&lt;li&gt;vector: 向量&lt;/li&gt;
&lt;li&gt;raster: 栅格&lt;/li&gt;
&lt;li&gt;sphere: 球体&lt;/li&gt;
&lt;li&gt;spheroid: 椭球体&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;coordinate-system&#34;&gt;Coordinate system
&lt;/h2&gt;&lt;p&gt;因为板块漂移, 会导致地理坐标系的变化, 所以经纬度总是在更新&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>

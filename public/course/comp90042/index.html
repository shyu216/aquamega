<!doctype html><html lang=en dir=auto><head><script src="/aquamega/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=aquamega/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Comp90042 | Aqua Mega</title>
<meta name=keywords content="comp90042,nlp,24s1"><meta name=description content="Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission & Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:"><meta name=author content="Dale"><link rel=canonical href=http://localhost:1313/aquamega/course/comp90042/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/aquamega/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/aquamega/course/comp90042/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Comp90042"><meta property="og:description" content="Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission & Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/aquamega/course/comp90042/"><meta property="og:image" content="https://github.com/shyu216.png"><meta property="article:section" content="course"><meta property="article:published_time" content="2024-04-26T14:56:05+10:00"><meta property="article:modified_time" content="2024-04-26T14:56:05+10:00"><meta property="og:site_name" content="Dale的水硕记录"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/shyu216.png"><meta name=twitter:title content="Comp90042"><meta name=twitter:description content="Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission & Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Courses","item":"http://localhost:1313/aquamega/course/"},{"@type":"ListItem","position":2,"name":"Comp90042","item":"http://localhost:1313/aquamega/course/comp90042/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Comp90042","name":"Comp90042","description":"Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission \u0026amp; Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:","keywords":["comp90042","nlp","24s1"],"articleBody":"Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission \u0026 Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:\nAttention Sequential model with attention Attention variants: Concat Dot product Scaled dot product Location-based Cosine similarity Global vs local attention Self-attention Machine Translation Statistical MT Neural MT with teacher forcing Transformer Mutli-head self-attention Position encoding Pretrained Language Models Encoder architecture BERT bidirectional context good for classification Encoder-decoder architecture Decoder architecture GPT unidirectional context good for generation Large Language Models In-context learning Step-by-step reasoning Human-feedback reinforcement Instruction following Named Entity Recognition Predict entities in a text Traditional ML methods Bi-LSTM with another RNN/CRF Multi-aspect NER Coreference Resolution 共指消解 Coreference, anaphora B-Cubed metric Question Answering and Reading Comprehension Knowledge-based QA Visual QA Spoken Language Understanding Attention RNNs Joint BERT Tri-level model Explainable NLU Vision Language Pretrained Model V-L Interaction Model Self-attention, co-attention, VSE Constrastive Language-Image Pretraining Ethics Sotial bias 刻板印象 Incivility 仇恨言论 Privacy Violation 歧视 Misinformation 谣言 Technological divide 发展不平衡 Vocab stemming:词干提取，去掉后缀，通常不是个词 lemmatisation:词形还原 morphology 詞法學 lexicon 字典 corpus 语料 tokenization 分词 entailment vs. contradiction 蕴涵与矛盾 neutral 中性 vanish 消失 distill 提炼 lexical 词汇的，与词汇或词语有关 semantic 语义的，与词语的含义或解释有关 syntactic 句法的，与句子结构有关 sentiment 情感，对某事物的态度 polysemy 一词多义 gloss 词义, from dictionary，词典中对词语含义的解释 synonymy 同义词 antonymy 反义词 hypernymy 上义词, is-a，是一个更一般的词 hyponymy 一个更具体的词 meronymy 下义词, is-part-of spectrum 范围 conference 共指，在文本中指代同一实体 anaphora 指代，一种语言现象，代替先前提到的词 utterence 话语 utter 说，发出声音 tedious 冗长的工作，乏味，枯燥 versality 可以在多种不同的情况和环境中使用，具有很高的适应性和灵活性 continuation 相连 patch-based 基于块的 interpretability 可解释性，白盒 explainability 可解释性，黑盒 monolingual 只使用一种语言的情况或者系统 sinusoidal 正弦的 proximal 接近的 miscellaneous 混杂的 antecedent 先行词, Trump anaphor 指代词, he pronominal 代词的 adjective 形容词 factoid 事实 Information Bottleneck 信息瓶颈问题是指在机器学习模型中，模型在尝试压缩输入数据的同时，还要尽可能保留与目标输出相关的信息。这是一个权衡问题，因为模型需要找到一个平衡点，既能有效地压缩数据，又能保留足够的信息来进行准确的预测。\n","wordCount":"431","inLanguage":"en","image":"https://github.com/shyu216.png","datePublished":"2024-04-26T14:56:05+10:00","dateModified":"2024-04-26T14:56:05+10:00","author":{"@type":"Person","name":"Dale"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/aquamega/course/comp90042/"},"publisher":{"@type":"Organization","name":"Aqua Mega","logo":{"@type":"ImageObject","url":"http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/aquamega/ accesskey=h title="Dale的水硕记录 (Alt + H)"><img src=https://github.com/shyu216.png alt aria-label=logo height=35>Dale的水硕记录</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/aquamega/posts/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/aquamega/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/aquamega/tags/ title=tags><span>tags</span></a></li><li><a href=https://github.com/shyu216 title=github><span>github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/aquamega/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/aquamega/course/>Courses</a></div><h1 class="post-title entry-hint-parent">Comp90042</h1><div class=post-meta><span title='2024-04-26 14:56:05 +1000 AEST'>April 26, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;431 words&nbsp;·&nbsp;Dale&nbsp;|&nbsp;<a href=https://github.com/shyu216/aquamega/content/course/comp90042.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h1 id=natural-language-processing>Natural Language Processing<a hidden class=anchor aria-hidden=true href=#natural-language-processing>#</a></h1><ol><li>Preprocessing<ol><li>Sentence segmentation</li><li>Tokenization, subword tokenization</li><li>Word normalization<ol><li>Inflectional vs derivational morphology</li><li>Lemmatization vs stemming</li></ol></li><li>Stopword removal</li></ol></li><li>N-gram Language Model<ol><li>Derivation</li><li>Smoothing techniques<ol><li>Add-k</li><li>Absolute discounting</li><li>Katz backoff</li><li>Kneser-Ney smoothing</li><li>Interpolation</li></ol></li></ol></li><li>Text Classification<ol><li>Build a classifier</li><li>Task<ol><li>Topic classification</li><li>Sentiment analysis</li><li>Native language identification</li></ol></li><li>Algorithms<ol><li>Naive Bayes, logistic regression, SVM</li><li>kNN, neural networks</li></ol></li><li>Bias vs variance：欠拟合under和过拟合over的取舍</li><li>Evaluation<ol><li>Precision, recall, F1</li></ol></li></ol></li><li>Part of Speech Tagging<ol><li>English POS<ol><li>Closed vs open classes</li></ol></li><li>Tagsets<ol><li>Penn Treebank tagset</li></ol></li><li>Automatic taggers<ol><li>Rule-based</li><li>Statistical<ol><li>Unigram, classifier-based, HMM</li></ol></li></ol></li></ol></li><li>Hidden Markov Model<ol><li>Probabilistic formulation:<ol><li>Emission & Transition</li></ol></li><li>Training</li><li>Viterbi algorithm</li><li>Generative vs discriminative models</li></ol></li><li>Feedforward Neural Network<ol><li>Formulation</li><li>Tasks:<ol><li>Topic classifcation</li><li>Language models</li><li>POS tagging</li></ol></li><li>Word embeddings</li><li>Convolutional networks</li></ol></li><li>Recurrent Neural Network<ol><li>Formulation</li><li>RNN: language models</li><li>LSTM:<ol><li>Functions of gates</li><li>Variants</li></ol></li><li>Tasks:<ol><li>Text classification: sentiment analysis</li><li>POS tagging</li></ol></li></ol></li><li>Lexical Semantics<ol><li>Definition of word sense, gloss</li><li>Lexical Relationship:<ol><li>Synonymy, antonymy, hypernymy, meronymy</li></ol></li><li>Structure of wordnet</li><li>Word similarity<ol><li>Path lenght</li><li>Depth information</li><li>Information content</li></ol></li><li>Word sense unambiguation<ol><li>supervised, unsupervised</li></ol></li></ol></li><li>Distributional Semantics<ol><li>Matrices:<ol><li>VSM, TF-IDF, word-word co-occurrence</li></ol></li><li>Association measures: PMI, PPMI</li><li>Count-based method: SVM</li><li>Neural method: skip-gram, CBOW</li><li>Evaluation:<ol><li>Word similarity, analogy</li></ol></li></ol></li><li>Contextual Representation<ol><li>Formulation with RNN</li><li>ELMo</li><li>BERT<ol><li>Objective</li><li>Fine-tuning for downstream tasks</li></ol></li><li>Transformers<ol><li>Multi-head attention</li></ol></li></ol></li></ol><p>Second half:</p><ol><li>Attention<ol><li>Sequential model with attention</li><li>Attention variants:<ol><li>Concat</li><li>Dot product</li><li>Scaled dot product</li><li>Location-based</li><li>Cosine similarity</li></ol></li><li>Global vs local attention</li><li>Self-attention</li></ol></li><li>Machine Translation<ol><li>Statistical MT</li><li>Neural MT with teacher forcing</li></ol></li><li>Transformer<ol><li>Mutli-head self-attention</li><li>Position encoding</li></ol></li><li>Pretrained Language Models<ol><li>Encoder architecture<ol><li>BERT</li><li>bidirectional context</li><li>good for classification</li></ol></li><li>Encoder-decoder architecture</li><li>Decoder architecture<ol><li>GPT</li><li>unidirectional context</li><li>good for generation</li></ol></li></ol></li><li>Large Language Models<ol><li>In-context learning</li><li>Step-by-step reasoning</li><li>Human-feedback reinforcement</li><li>Instruction following</li></ol></li><li>Named Entity Recognition<ol><li>Predict entities in a text</li><li>Traditional ML methods</li><li>Bi-LSTM with another RNN/CRF</li><li>Multi-aspect NER</li></ol></li><li>Coreference Resolution 共指消解<ol><li>Coreference, anaphora</li><li>B-Cubed metric</li></ol></li><li>Question Answering and Reading Comprehension<ol><li>Knowledge-based QA</li><li>Visual QA</li></ol></li><li>Spoken Language Understanding<ol><li>Attention RNNs</li><li>Joint BERT</li><li>Tri-level model</li><li>Explainable NLU</li></ol></li><li>Vision Language Pretrained Model<ol><li>V-L Interaction Model<ol><li>Self-attention, co-attention, VSE</li></ol></li><li>Constrastive Language-Image Pretraining</li></ol></li><li>Ethics<ol><li>Sotial bias 刻板印象</li><li>Incivility 仇恨言论</li><li>Privacy Violation 歧视</li><li>Misinformation 谣言</li><li>Technological divide 发展不平衡</li></ol></li></ol><h2 id=vocab>Vocab<a hidden class=anchor aria-hidden=true href=#vocab>#</a></h2><ul><li>stemming:词干提取，去掉后缀，通常不是个词</li><li>lemmatisation:词形还原</li><li>morphology <strong>詞法學</strong></li><li>lexicon 字典</li><li>corpus 语料</li><li>tokenization 分词</li><li>entailment vs. contradiction 蕴涵与矛盾</li><li>neutral 中性</li><li>vanish 消失</li><li>distill 提炼</li><li>lexical 词汇的，与词汇或词语有关</li><li>semantic 语义的，与词语的含义或解释有关</li><li>syntactic 句法的，与句子结构有关</li><li>sentiment 情感，对某事物的态度</li><li>polysemy 一词多义</li><li>gloss 词义, from dictionary，词典中对词语含义的解释</li><li>synonymy 同义词</li><li>antonymy 反义词</li><li>hypernymy 上义词, is-a，是一个更一般的词</li><li>hyponymy 一个更具体的词</li><li>meronymy 下义词, is-part-of</li><li>spectrum 范围</li><li>conference 共指，在文本中指代同一实体</li><li>anaphora 指代，一种语言现象，代替先前提到的词</li><li>utterence 话语</li><li>utter 说，发出声音</li><li>tedious 冗长的工作，乏味，枯燥</li><li>versality 可以在多种不同的情况和环境中使用，具有很高的适应性和灵活性</li><li>continuation 相连</li><li>patch-based 基于块的</li><li>interpretability 可解释性，白盒</li><li>explainability 可解释性，黑盒</li><li>monolingual 只使用一种语言的情况或者系统</li><li>sinusoidal 正弦的</li><li>proximal 接近的</li><li>miscellaneous 混杂的</li><li>antecedent 先行词, Trump</li><li>anaphor 指代词, he</li><li>pronominal 代词的</li><li>adjective 形容词</li><li>factoid 事实</li><li></li></ul><h1 id=information-bottleneck>Information Bottleneck<a hidden class=anchor aria-hidden=true href=#information-bottleneck>#</a></h1><p>信息瓶颈问题是指在机器学习模型中，模型在尝试压缩输入数据的同时，还要尽可能保留与目标输出相关的信息。这是一个权衡问题，因为模型需要找到一个平衡点，既能有效地压缩数据，又能保留足够的信息来进行准确的预测。</p></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/aquamega/tags/comp90042/>Comp90042</a></li><li><a href=http://localhost:1313/aquamega/tags/nlp/>Nlp</a></li><li><a href=http://localhost:1313/aquamega/tags/24s1/>24s1</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/aquamega/>Aqua Mega</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
[{"content":" pure Strategy: single action\nmixed Strategy: probability distribution over actions\nweakly dominate: $\\leq$\nstrongly dominate: $\u003c$\na weakly(strictly) dominant strategy: always better than any other strategy\nnash equilibrium: 每个agent都选了最优策略，其他agent不会改变策略\nindifference: 通过调整我的策略概率，改变对手的收益（payoff）期望，使无论对手如何选择，他的满意度（utility）都一样\nUtility Function U_i(a): what can agent i get from action a\nNormal Form Game 一轮，不知道对手的策略，只知道对手的utility function\nExtensive Form Game 广义形式博弈 轮流决策，所以知道对手的策略\nSubgame Perfect Equilibrium 子博弈完美均衡 当前玩家在他的回合的最优策略，对手在他的回合的最优策略。。。\nBackward Induction 反向归纳 输入：广义形式博弈 G = (N, Agt, S, s_0, A, T, r) 输出：每个状态 s ∈ S 的子博弈均衡 函数 BackwardInduction(s ∈ S): 如果 A(s) = ∅，则返回 r(s) best_child ← (-∞, ..., -∞) 对于每个 a ∈ A(s)： s\u0026#39; ← T(s,a) child_reward ← BackwardInduction(s\u0026#39;) 如果 child_reward(P(s)) \u0026gt; best_child(P(s))，则 best_child ← child_reward 返回 best_child 返回 BackwardInduction(s_0) Multi-agent Q-learning 输入：随机博弈 G = (S, s_0, A^1, ..., A^n, r^1, ..., r^n, Agt, P, γ) 输出：Q函数 Q^j，其中 j 是 self agent 初始化 Q^j 任意，例如，对于所有的状态 s 和联合动作 a，Q^j(s,a)=0 重复： s ← episode e 的第一个状态 重复（对于 episode e 的每一步）： 选择在 s 中应用的动作 a^j 例如，使用 Q^j 和一个多臂老虎机算法，如 ε-greedy 在状态 s 中执行动作 a^j 观察奖励 r^j 和新状态 s\u0026#39; Q^j(s,a) ← Q^j(s,a) + α * [r^j + γ * max_a\u0026#39; Q^j(s\u0026#39;,a\u0026#39;) - Q^j(s,a)] s ← s\u0026#39; 直到 episode e 结束（一个终止状态） 直到 Q 收敛 ","permalink":"http://localhost:1313/aquamega/algorithm/game/","summary":"pure Strategy: single action\nmixed Strategy: probability distribution over actions\nweakly dominate: $\\leq$\nstrongly dominate: $\u003c$\na weakly(strictly) dominant strategy: always better than any other strategy\nnash equilibrium: 每个agent都选了最优策略，其他agent不会改变策略\nindifference: 通过调整我的策略概率，改变对手的收益（payoff）期望，使无论对手如何选择，他的满意度（utility）都一样\nUtility Function U_i(a): what can agent i get from action a\nNormal Form Game 一轮，不知道对手的策略，只知道对手的utility function\nExtensive Form Game 广义形式博弈 轮流决策，所以知道对手的策略\nSubgame Perfect Equilibrium 子博弈完美均衡 当前玩家在他的回合的最优策略，对手在他的回合的最优策略。。。\nBackward Induction 反向归纳 输入：广义形式博弈 G = (N, Agt, S, s_0, A, T, r) 输出：每个状态 s ∈ S 的子博弈均衡 函数 BackwardInduction(s ∈ S): 如果 A(s) = ∅，则返回 r(s) best_child ← (-∞, .","title":"Game Theory"},{"content":" h*: the optimal heuristic\nh pre\u0026amp;del: the heuristic ignoring preconditions and delete effects, adimissible and consistent, \u0026ldquo;subset sum\u0026rdquo; problem, NP-hard\nh goal_count: the number of goals not yet achieved, neither admissible nor consistent\nh+/h del: admitssible and consistent, 相当于MST, 每个state只访问一遍，也NP-hard\nh add: easily not admissible\nh max: easily too small\nh ff: use h add and h max\nRemoving preconditions and delete effects is efficiently constructive but not computable。\nh max和h add的table是不一样的，一个是最大值，一个是累加值。\nDelete Relaxation 忽略搜索中所有的delete效果，在发现goal之前减少重复的状态。\nState Dominance: 如果一个状态支配另一个状态，那么我们可以忽略支配状态。被包含了。 def greedyRelaxedPlan(problem): candidate_pq = util.PriorityQueue() init_state = problem.getStartState() init_path = [] candidate_pq.push((init_state,init_path),0) viewed = [] while not candidate_pq.isEmpty(): state,path = candidate_pq.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_pq.push((child_state,child_path),0) return None Neither admissible nor consistent. 因为不保证optimal，只保证能找到解决方案。\nOptimal的都NP-hard。\nAdditive and Max Heuristics Additive: 相加子目标的启发式，明显不是admissible。 $h^{add}(s, g) = \\begin{cases} 0 \u0026 \\text{if } g \\subseteq s \\\\ \\min_{a \\in A, g \\in add_a} (c(a) + h^{add}(s, pre_a)) \u0026 \\text{if } |g| = 1 \\\\ \\sum_{g' \\in g} h^{add}(s, \\{g'\\}) \u0026 \\text{if } |g| \u003e 1 \\end{cases}$\nMax: 选择子目标中最大的启发式。最难解决的子节点。 $h^{max}(s, g) = \\begin{cases} 0 \u0026 \\text{if } g \\subseteq s \\\\ \\max_{a \\in A, g \\in add_a} (c(a) + h^{max}(s, pre_a)) \u0026 \\text{if } |g| = 1 \\\\ \\max_{g' \\in g} h^{max}(s, \\{g'\\}) \u0026 \\text{if } |g| \u003e 1 \\end{cases}$\n都goal-aware，因为h+ ∞时，h也是∞。\nBest Supporter Heuristic $bs_{s}^{max}(p) = argmin_{a\\in A,p \\in add_a}c(a)+h^{max}(s,pre_a)$\n$bs_{s}^{add}(p) = argmin_{a\\in A,p \\in add_a}c(a)+h^{add}(s,pre_a)$\n把$h_{add}$和$h_{max}$结合起来，选择最好的支持者。\nargmin: 在一系列动作里，选最小的h。\nBellman-Ford for hmax and hadd Bellman-Ford variant computing hadd for state s\n反复更新表Tadd，直到表中的值不再改变。在每次迭代中，对于每个目标状态g，都会计算一个新的值fi(g)，这个值是当前状态s到状态g的最短路径的估计值。\ndef bellmanFordHadd(problem): states = problem.getStates() actions = problem.getActions() P = problem.getTransitionProbabilities() r = problem.getRewards() gamma = problem.getDiscount() theta = 0.01 Tadd = {s: 0 for s in states} while True: delta = 0 for s in states: v = Tadd[s] Tadd[s] = min([sum([r[s][a][s_prime] + gamma * Tadd[s_prime] for s_prime in states]) for a in actions]) delta = max(delta, abs(v - Tadd[s])) if delta \u0026lt; theta: break return Tadd Iterated Width Novelty：只考虑$w(s)$个状态变量atoms的变化情况。\n搜索直到目标状态的状态变量的数量。\n一个state的novelty：第一次出现的atom组合中的atom数量。the size of the smallest subset of atoms in s，that is true for the first time in search。\n","permalink":"http://localhost:1313/aquamega/algorithm/relax/","summary":"h*: the optimal heuristic\nh pre\u0026amp;del: the heuristic ignoring preconditions and delete effects, adimissible and consistent, \u0026ldquo;subset sum\u0026rdquo; problem, NP-hard\nh goal_count: the number of goals not yet achieved, neither admissible nor consistent\nh+/h del: admitssible and consistent, 相当于MST, 每个state只访问一遍，也NP-hard\nh add: easily not admissible\nh max: easily too small\nh ff: use h add and h max\nRemoving preconditions and delete effects is efficiently constructive but not computable。","title":"Greedy Relaxed Planning"},{"content":"Uniform Cost Search Priority Queue，最先被探索离起始节点最近（即路径成本最低）的节点。\nBreath First Search属于Uniform Cost Search的特例，即每个节点的路径成本都是1。\nUCS只看到了路径成本，没有考虑启发式。\ndef uniformCostSearch(problem): candidate_pq = util.PriorityQueue() init_state = problem.getStartState() init_path = [] candidate_pq.push((init_state,init_path),0) viewed = [] while not candidate_pq.isEmpty(): state,path = candidate_pq.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, child_cost in problem.getSuccessors(state): child_path = path + [child_action] candidate_pq.push((child_state,child_path),problem.getCostOfActions(child_path)) return None Greedy Best First Search BFS只看到了启发式，没有考虑路径成本。\nIf h=0，BFS是什么根据它的Priority Queue的实现。\ndef bestFirstSearch(problem, heuristic=nullHeuristic): candidate_pq = util.PriorityQueue() init_state = problem.getStartState() init_path = [] candidate_pq.push((init_state,init_path),heuristic(init_state, problem)) viewed = [] while not candidate_pq.isEmpty(): state,path = candidate_pq.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_pq.push((child_state,child_path),heuristic(child_state, problem)) return None ","permalink":"http://localhost:1313/aquamega/algorithm/uniform/","summary":"Uniform Cost Search Priority Queue，最先被探索离起始节点最近（即路径成本最低）的节点。\nBreath First Search属于Uniform Cost Search的特例，即每个节点的路径成本都是1。\nUCS只看到了路径成本，没有考虑启发式。\ndef uniformCostSearch(problem): candidate_pq = util.PriorityQueue() init_state = problem.getStartState() init_path = [] candidate_pq.push((init_state,init_path),0) viewed = [] while not candidate_pq.isEmpty(): state,path = candidate_pq.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, child_cost in problem.getSuccessors(state): child_path = path + [child_action] candidate_pq.push((child_state,child_path),problem.getCostOfActions(child_path)) return None Greedy Best First Search BFS只看到了启发式，没有考虑路径成本。\nIf h=0，BFS是什么根据它的Priority Queue的实现。\ndef bestFirstSearch(problem, heuristic=nullHeuristic): candidate_pq = util.PriorityQueue() init_state = problem.","title":"UCB and Greedy BFS"},{"content":"Part-of-Speech(词类) Colourless green ideas sleep furiously.\nOpen Classes (Content Words) Noun Verb Adjective Adverb Closed Classes (Function Words) (Fixed number of words) Preposition Determiner Pronoun Conjunction Interjection Ambiguity: A word can have multiple meanings. A sentence can have multiple interpretations.\n在自然语言处理（NLP）中，POS代表词性标注（Part-of-Speech Tagging）。词性标注是一种将每个词语与其语法范畴（即词性）相关联的过程。它是NLP中的一项基本任务，用于确定句子中每个单词的词类。\n常见的词性包括名词、动词、形容词、副词、介词、代词、冠词、连词等。通过进行词性标注，可以帮助计算机理解句子的结构和含义，为其他NLP任务（如句法分析、语义分析和机器翻译等）提供基础。\n以下是一个例子，展示了一句英文句子的词性标注示例：\n句子：I love eating ice cream.\n词性标注：PRON VERB VERB NOUN NOUN.\n在这个例子中，\u0026ldquo;I\u0026quot;被标注为代词（PRON），\u0026ldquo;love\u0026quot;和\u0026quot;eating\u0026quot;被标注为动词（VERB），\u0026ldquo;ice\u0026quot;和\u0026quot;cream\u0026quot;被标注为名词（NOUN）。\n词性标注可以通过使用基于规则的方法、基于统计的机器学习方法（如隐马尔可夫模型）或基于深度学习的方法（如循环神经网络和转换器模型）来实现。这个任务在NLP中具有广泛的应用，对于许多文本处理任务都是重要的预处理步骤。\nNN（noun）名词：cat（猫）、book（书）、table（桌子） VB（verb）动词：run（跑）、eat（吃）、sleep（睡觉） JJ（adjective）形容词：big（大的）、happy（快乐的）、red（红色的） RB（adverb）副词：quickly（快速地）、well（好地）、very（非常） DT（determiner）限定词：the（定冠词）、this（这个）、some（一些） CD（cardinal number）基数词：one（一）、three（三）、ten（十） IN（preposition）介词：on（在\u0026hellip;上）、with（和\u0026hellip;一起）、at（在\u0026hellip;处） PRP（personal pronoun）人称代词：I（我）、you（你）、he（他） MD（modal）情态动词：can（能够）、should（应该）、will（将会） CC（coordinating conjunction）并列连词：and（和）、but（但是）、or（或者） RP（particle）小品词：up（向上）、down（向下）、out（出去） WH（wh-pronoun）疑问代词：who（谁）、what（什么）、which（哪个） TO（to）不定式标记：to（去）、to（为了）、to（到） Penn Treebank 词性标记集 Penn Treebank 的词性标注集（POS tagset）是一种广泛使用的英文词性标注方法。\nCC：并列连词 (Coordinating conjunction) CD：基数词 (Cardinal number) DT：限定词 (Determiner) EX：存在句引导词 (Existential there) FW：外来词 (Foreign word) IN：介词或从属连词 (Preposition or subordinating conjunction) JJ：形容词 (Adjective) JJR：比较级形容词 (Adjective, comparative) JJS：最高级形容词 (Adjective, superlative) LS：列表项标记 (List item marker) MD：情态动词 (Modal) NN：名词, 单数或集数 (Noun, singular or mass) NNS：名词, 复数 (Noun, plural) NNP：专有名词, 单数 (Proper noun, singular) NNPS：专有名词, 复数 (Proper noun, plural) PDT：前限定词 (Predeterminer) POS：所有格结束 (Possessive ending) PRP：人称代词 (Personal pronoun) PRP$：所有格代词 (Possessive pronoun) RB：副词 (Adverb) RBR：副词, 比较级 (Adverb, comparative) RBS：副词, 最高级 (Adverb, superlative) RP：小品词 (Particle) SYM：符号 (Symbol) TO：to UH：感叹词 (Interjection) VB：动词, 原形 (Verb, base form) VBD：动词, 过去式 (Verb, past tense) VBG：动词, 现在分词或动名词 (Verb, gerund or present participle) VBN：动词, 过去分词 (Verb, past participle) VBP：动词, 非第三人称单数现在式 (Verb, non-3rd person singular present) VBZ：动词, 第三人称单数现在式 (Verb, 3rd person singular present) WDT：wh-限定词 (Wh-determiner) WP：wh-代词 (Wh-pronoun) WP$：wh-所有格代词 (Wh-pronoun, possessive) WRB：wh-副词 (Wh-adverb) ","permalink":"http://localhost:1313/aquamega/nlp/pos/","summary":"Part-of-Speech(词类) Colourless green ideas sleep furiously.\nOpen Classes (Content Words) Noun Verb Adjective Adverb Closed Classes (Function Words) (Fixed number of words) Preposition Determiner Pronoun Conjunction Interjection Ambiguity: A word can have multiple meanings. A sentence can have multiple interpretations.\n在自然语言处理（NLP）中，POS代表词性标注（Part-of-Speech Tagging）。词性标注是一种将每个词语与其语法范畴（即词性）相关联的过程。它是NLP中的一项基本任务，用于确定句子中每个单词的词类。\n常见的词性包括名词、动词、形容词、副词、介词、代词、冠词、连词等。通过进行词性标注，可以帮助计算机理解句子的结构和含义，为其他NLP任务（如句法分析、语义分析和机器翻译等）提供基础。\n以下是一个例子，展示了一句英文句子的词性标注示例：\n句子：I love eating ice cream.\n词性标注：PRON VERB VERB NOUN NOUN.\n在这个例子中，\u0026ldquo;I\u0026quot;被标注为代词（PRON），\u0026ldquo;love\u0026quot;和\u0026quot;eating\u0026quot;被标注为动词（VERB），\u0026ldquo;ice\u0026quot;和\u0026quot;cream\u0026quot;被标注为名词（NOUN）。\n词性标注可以通过使用基于规则的方法、基于统计的机器学习方法（如隐马尔可夫模型）或基于深度学习的方法（如循环神经网络和转换器模型）来实现。这个任务在NLP中具有广泛的应用，对于许多文本处理任务都是重要的预处理步骤。\nNN（noun）名词：cat（猫）、book（书）、table（桌子） VB（verb）动词：run（跑）、eat（吃）、sleep（睡觉） JJ（adjective）形容词：big（大的）、happy（快乐的）、red（红色的） RB（adverb）副词：quickly（快速地）、well（好地）、very（非常） DT（determiner）限定词：the（定冠词）、this（这个）、some（一些） CD（cardinal number）基数词：one（一）、three（三）、ten（十） IN（preposition）介词：on（在\u0026hellip;上）、with（和\u0026hellip;一起）、at（在\u0026hellip;处） PRP（personal pronoun）人称代词：I（我）、you（你）、he（他） MD（modal）情态动词：can（能够）、should（应该）、will（将会） CC（coordinating conjunction）并列连词：and（和）、but（但是）、or（或者） RP（particle）小品词：up（向上）、down（向下）、out（出去） WH（wh-pronoun）疑问代词：who（谁）、what（什么）、which（哪个） TO（to）不定式标记：to（去）、to（为了）、to（到） Penn Treebank 词性标记集 Penn Treebank 的词性标注集（POS tagset）是一种广泛使用的英文词性标注方法。","title":"Part-of-Speech Tagging"},{"content":"Lexical Semantics 词汇解释\nSynonymy 同义词 Antonymy 反义词 Hypernymy 上义词 Meronymy 下义词 WordNet A lexical database of English.\nWord Similarity 路径长度：在WordNet中，两个词之间的路径长度可以用来衡量它们的相似性。 深度信息：在WordNet中，词的深度（在词汇树中的位置）也可以用来衡量词的相似性。 信息内容：词的信息内容（在语料库中的频率）也可以用来衡量词的相似性。 Distributional Semantics 词汇和其上下文的共现信息\n","permalink":"http://localhost:1313/aquamega/nlp/semantics/","summary":"Lexical Semantics 词汇解释\nSynonymy 同义词 Antonymy 反义词 Hypernymy 上义词 Meronymy 下义词 WordNet A lexical database of English.\nWord Similarity 路径长度：在WordNet中，两个词之间的路径长度可以用来衡量它们的相似性。 深度信息：在WordNet中，词的深度（在词汇树中的位置）也可以用来衡量词的相似性。 信息内容：词的信息内容（在语料库中的频率）也可以用来衡量词的相似性。 Distributional Semantics 词汇和其上下文的共现信息","title":"Lexical Semantics and Distributional Semantics"},{"content":"神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。\n每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。\n神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。\n神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。\nA Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.\nEach neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.) to produce an output, which is then passed on to the neurons in the next layer.\nNeural networks are typically trained using the Backpropagation algorithm and the Gradient Descent algorithm. During training, the neural network minimizes the difference between the predicted values and the actual values by continuously adjusting the connection weights between neurons.\nNeural networks can handle non-linear problems and are suitable for various complex machine learning tasks, such as image recognition, speech recognition, natural language processing, etc. However, the training of neural networks requires a large amount of data and computational resources, and the interpretability of the model is relatively poor, which are some of the challenges.\n前馈神经网络 前馈神经网络是一种基础的神经网络，信息只在一个方向上流动，从输入层到输出层。\n任务：\n主题分类：使用神经网络对文本或其他数据进行主题分类。\n语言模型：使用神经网络来理解和生成语言。\n词性标注：使用神经网络来识别文本中每个词的词性。\n词嵌入：一种表示词汇的技术，将词汇映射到向量空间，使得语义相近的词汇在向量空间中的距离也相近。\n卷积网络：一种特殊的神经网络，特别适合处理网格形式的数据，如图像。\n循环神经网络 循环神经网络是一种可以处理序列数据的神经网络，它可以利用前面的信息来影响后面的输出。\nRNN语言模型：使用循环神经网络来理解和生成语言，特别适合处理文本等序列数据。 LSTM： 门的功能：LSTM是一种特殊的RNN，它有三个门（输入门、遗忘门和输出门）来控制信息的流动。 变体：LSTM有多种变体，如GRU（门控循环单元），它简化了LSTM的结构。 任务： 文本分类：情感分析：使用RNN进行文本分类，如情感分析，判断文本的情感倾向。 词性标注：使用RNN来识别文本中每个词的词性。 非线性激活函数是神经网络中的关键组成部分，它可以帮助神经网络学习和逼近复杂的模式。非线性激活函数的主要目的是将输入信号转换为输出信号，但它不仅仅是复制输入到输出。它会修改或者变换输入的方式，使得可以适应网络的需要。 常见的非线性激活函数包括：\nReLU（Rectified Linear Unit）：这是最常用的激活函数，它将所有负值设为0，正值保持不变。\nSigmoid：这个函数将任何数值都映射到0和1之间，它在早期的神经网络中非常流行，但现在已经较少使用，因为它在输入值很大或很小的时候，梯度几乎为0，这会导致梯度消失问题。\nTanh（Hyperbolic Tangent）：这个函数将任何数值都映射到-1和1之间，它比sigmoid函数的输出范围更广，因此在某些情况下，它的性能比sigmoid函数更好。\nSoftmax：这个函数通常用在神经网络的输出层，特别是在处理多分类问题时。它可以将一组数值转换为概率分布。\nRegularization 正则化 L1 Norm L2 Norm Dropout LSTM Memory cell：记忆单元，用来存储信息 Hidden state：隐藏状态，用来传递信息 Forget gate：遗忘门，用来控制记忆单元中的信息是否被遗忘 Input gate：输入门，用来控制新的信息是否被存储到记忆单元中 Output gate：输出门，用来控制隐藏状态中的信息是否被传递到下一个时间步 ","permalink":"http://localhost:1313/aquamega/nlp/nn/","summary":"神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。\n每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。\n神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。\n神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。\nA Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.\nEach neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.","title":"Neual Networks"},{"content":" w：word t：tag emission probability: $P(w|t)$ 从tag到word的概率 transition probability: $P(t|t')$ 从上一个tag到当前tag的概率 Viterbi Algorithm 从最开始算起，每个tag的概率是由前一个tag的概率和从前一个tag到当前tag的概率相乘得到的。\n$\\hat{t} = \\arg\\max_{t} \\prod_{i=1}^{n} P(w_i|t)P(t|t')$, $t'$是前一个tag, $\\hat{t}$是当前的估计值。\nGenerative vs Discriminative Tagger HMM是generative model，可以用来生成数据，无监督的学习。 discriminative model例如CRF，需要有监督的数据，可以学到更丰富的特征，大多数deep learning的模型都是discriminative model。 ","permalink":"http://localhost:1313/aquamega/nlp/hmm/","summary":" w：word t：tag emission probability: $P(w|t)$ 从tag到word的概率 transition probability: $P(t|t')$ 从上一个tag到当前tag的概率 Viterbi Algorithm 从最开始算起，每个tag的概率是由前一个tag的概率和从前一个tag到当前tag的概率相乘得到的。\n$\\hat{t} = \\arg\\max_{t} \\prod_{i=1}^{n} P(w_i|t)P(t|t')$, $t'$是前一个tag, $\\hat{t}$是当前的估计值。\nGenerative vs Discriminative Tagger HMM是generative model，可以用来生成数据，无监督的学习。 discriminative model例如CRF，需要有监督的数据，可以学到更丰富的特征，大多数deep learning的模型都是discriminative model。 ","title":"Hidden Markov Model"},{"content":" Topic classification Sentiment analysis Native language identification Natual language inference Automatic fact-checking Paraphrase Topic Classification Motivation: library science, information retrieval, etc. Classes: topic categories Features: bag-of-words, n-grams, etc. Corpus: reuters news corpus, pubmed abstracts, tweets with hashtags Sentiment Analysis Motivation: opinion mining, business analytics Classes: positive, negative, neutral Features: n-grams, polarity lexicons, etc. Corpus: movie reviews, SEMEVAL twitter polarity dataset Native Language Identification Motivation: forensic linguistics, educational applications Classes: native languages Features: character n-grams, syntactic features (POS), phonological features Corpus: TOEFL/IELTS essays Natural Language Inference (textual entailment 文本蕴含) Motivation: language understanding Classes: entailment, contradiction, neutral Features: word overlap, length difference, etc. Corpus: SNLI, MultiNLI Build a Text Classifier Identify the task Collect and preprocess data Annotation Select features Select an algorithm Train and tune the model Evaluate the model Naive Bayes Classifier assume independence between features\n$$P(c|d) = \\frac{P(d|c)P(c)}{P(d)}$$ Pros:\nFast, simple Robust, low-variance Optimal if independence holds Cons:\nRarely holds Low accuracy Smoothing needed for unseen Logistic Regression $$P(c|d) = \\frac{1}{1 + e^{-\\sum_{i=1}^n w_i x_i}}$$ Pros:\nBetter performance Cons:\nSlow Scaling needed Regularization needed for overfitting Support Vector Machine To find the hyperplane that maximizes the margin\nPros:\nFast and accurate Non-linear kernel Work well in big data Cons:\nMulti-class classification Scaling needed Imbalanced data Interpretability K-Nearest Neighbors Euclidean distance, cosine similarity\nProbs:\nSimple No training needed Multi-class classification Optimal with infinite data Cons:\nSet k Imbalanced classes Slow Decision Tree Greedy maxmize information gain\nPros:\nFast Non-linear Good for small data Feature scaling irrelevant Cons:\nNot that interpretable Redundant features Not competitive for big data Random Forest Pros:\nBetter than decision tree Parallelizable Cons:\nSame as decision tree Neural Network Pros:\nPowerful Cons:\nHyperparameters Training time Overfitting Evaluation Accuracy\n$$\\frac{TP + TN}{TP + TN + FP + FN}$$ Precision\n$$\\frac{TP}{TP + FP}$$ Recall\n$$\\frac{TP}{TP + FN}$$ F1-score\n$$\\frac{2 \\times \\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$ ","permalink":"http://localhost:1313/aquamega/nlp/classification/","summary":"Topic classification Sentiment analysis Native language identification Natual language inference Automatic fact-checking Paraphrase Topic Classification Motivation: library science, information retrieval, etc. Classes: topic categories Features: bag-of-words, n-grams, etc. Corpus: reuters news corpus, pubmed abstracts, tweets with hashtags Sentiment Analysis Motivation: opinion mining, business analytics Classes: positive, negative, neutral Features: n-grams, polarity lexicons, etc. Corpus: movie reviews, SEMEVAL twitter polarity dataset Native Language Identification Motivation: forensic linguistics, educational applications Classes: native languages Features: character n-grams, syntactic features (POS), phonological features Corpus: TOEFL/IELTS essays Natural Language Inference (textual entailment 文本蕴含) Motivation: language understanding Classes: entailment, contradiction, neutral Features: word overlap, length difference, etc.","title":"Text Classification"},{"content":"Language Model To explain language, based on probability, for generation.\nQuery completion(搜索引擎) Optical character recognition(OCR) Translation N-gram $$P(w_1, w_2, \\cdots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2) \\cdots P(w_n|w_1, w_2, \\cdots, w_{n-1})$$ Maximum Likelihood Estimation $$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$ Use special tags to denote the beginning and end of a sentence, i.e. \u0026lt;s\u0026gt;, \u0026lt;/s\u0026gt;.\nSmoothing Handle unseen words.\nLaplacian (add-one) smoothing\n$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$\nAdd-k smoothing\n$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + kV}$\nLidstone smoothing\nGeneralized Add-k, 所有非负实数 Absolute discounting\nBorrow a bit of probability mass from seen words to unseen words， 每个借一点，均摊给没见过的词 Katz Backoff\nUse lower-order n-gram Issue：低阶n-gram可能有很高概率，但此n-gram组合显然不会存在 Knser-Ney\nVersatility of lower-order n-gram, co-occur with many words 让更通用的低阶n-gram有更高的概率 Interpolation\nCombine lower-order n-gram 给不同阶的n-gram加权，相加 ","permalink":"http://localhost:1313/aquamega/nlp/n-gram/","summary":"Language Model To explain language, based on probability, for generation.\nQuery completion(搜索引擎) Optical character recognition(OCR) Translation N-gram $$P(w_1, w_2, \\cdots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2) \\cdots P(w_n|w_1, w_2, \\cdots, w_{n-1})$$ Maximum Likelihood Estimation $$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$ Use special tags to denote the beginning and end of a sentence, i.e. \u0026lt;s\u0026gt;, \u0026lt;/s\u0026gt;.\nSmoothing Handle unseen words.\nLaplacian (add-one) smoothing\n$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$\nAdd-k smoothing\n$P(w_i|w_{i-1}) = \\frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + kV}$","title":"N-gram Language Model"},{"content":"Preprocessing Word Sentence: words Document: sentences Corpus: documents Word Token: word instance Word Type: verb, noun, etc. Lexicon: word types Steps Remove unwnated formatting, i.e. HTML tags Sentecne segmentation, break text into sentences Tokenization, break sentences into words Normalization, transform words into canonical form, i.e. lower case Stopword removal, delete unwanted words, i.e., \u0026ldquo;the\u0026rdquo;, \u0026ldquo;I\u0026rdquo;, \u0026ldquo;.\u0026rdquo;, etc. Max Match Algorithm Greedily match the longest word in the dictionary. Used in the language withoout space between words, i.e. Chinese.\nByte Pair Encoding Iteratively merge the most frequent pair of bytes. Used in ChatGPT.\nAdv:\nData-informed tokenization Works for different languages Deals better with unknown words Disadv:\nRarer words will be split into subwords, even individual letter Word Normalization Goal: reduce vocabulary size, map different forms of a word to the same form\n形态处理（Morphological processing）通常包括词干提取（stemming）和词形还原（lemmatization）\nLowercasing Remove morphology, i.e. \u0026ldquo;running\u0026rdquo; -\u0026gt; \u0026ldquo;run\u0026rdquo; Correct spelling, i.e. \u0026ldquo;colour\u0026rdquo; -\u0026gt; \u0026ldquo;color\u0026rdquo; Expand abbreviations, i.e. \u0026ldquo;can\u0026rsquo;t\u0026rdquo; -\u0026gt; \u0026ldquo;cannot\u0026rdquo; Inflectional Morphology(屈折形态) It creates grammatical variants of a word, i.e. \u0026ldquo;run\u0026rdquo; -\u0026gt; \u0026ldquo;running\u0026rdquo;, \u0026ldquo;ran\u0026rdquo;, \u0026ldquo;runs\u0026rdquo;\nNouns, verbs, adjectives, adverbs\u0026hellip;\nLemmatization(词形还原) To remove inflection.\nLemma(词元): the uninflated form\nDerivational Morphology(派生形态) It creates new words.\nChange the lexical category(), i.e. \u0026ldquo;happy\u0026rdquo; -\u0026gt; \u0026ldquo;happiness\u0026rdquo; Change the meaning, i.e. \u0026ldquo;happy\u0026rdquo; -\u0026gt; \u0026ldquo;unhappy\u0026rdquo; Stemmization(词干提取) To remove derivation.\nStem(词干): the root form\nThe Porter Stemmer most widely used in English\nC: consonant(辅音) V: vowel(元音) m: measure $[C](VC)^m[V]$ Stopword Removal Typically in bag-of-words model\nAll closed-class or function words, any high frequency words\n","permalink":"http://localhost:1313/aquamega/nlp/preprocessing/","summary":"Preprocessing Word Sentence: words Document: sentences Corpus: documents Word Token: word instance Word Type: verb, noun, etc. Lexicon: word types Steps Remove unwnated formatting, i.e. HTML tags Sentecne segmentation, break text into sentences Tokenization, break sentences into words Normalization, transform words into canonical form, i.e. lower case Stopword removal, delete unwanted words, i.e., \u0026ldquo;the\u0026rdquo;, \u0026ldquo;I\u0026rdquo;, \u0026ldquo;.\u0026rdquo;, etc. Max Match Algorithm Greedily match the longest word in the dictionary. Used in the language withoout space between words, i.","title":"Preprocessing"},{"content":"A*算法（带有重复检测和重新打开） 设 open 为新的优先级队列，按照 g(state(σ)) + h(state(σ)) 升序排列 open.insert(make-root-node(init())) 设 closed 为空集 设 best-g 为空集 /* 将状态映射到数字 */ 当 open 不为空时： σ := open.pop-min() 如果 state(σ) 不在 closed 中或 g(σ) \u0026lt; best-g(state(σ))： /* 如果 g 更好则重新打开；注意所有具有相同状态但 g 较差的 σ′ 都在 open 中位于 σ 后面，并且在轮到它们时将被跳过 */ closed := closed ∪{state(σ)} best-g(state(σ)) := g(σ) 如果 is-goal(state(σ))：返回 extract-solution(σ) 对于每个 (a,s′) ∈succ(state(σ))： σ′ := make-node(σ,a,s′) 如果 h(state(σ′)) \u0026lt; ∞：open.insert(σ′) 返回 unsolvable def aStarSearch(problem, heuristic=nullHeuristic): \u0026#34;\u0026#34;\u0026#34;Search the node that has the lowest combined cost and heuristic first.\u0026#34;\u0026#34;\u0026#34; myPQ = util.PriorityQueue() startState = problem.getStartState() startNode = (startState, 0, []) myPQ.push(startNode, heuristic(startState, problem)) best_g = dict() while not myPQ.isEmpty(): node = myPQ.pop() state, cost, path = node if (not state in best_g) or (cost \u0026lt; best_g[state]): best_g[state] = cost if problem.isGoalState(state): return path for succ in problem.getSuccessors(state): succState, succAction, succCost = succ new_cost = cost + succCost newNode = (succState, new_cost, path + [succAction]) myPQ.push(newNode, heuristic(succState, problem) + new_cost) return None # Goal not found If h=0, A* is equivalent to Uniform Cost Search.\nIf h admissible, A* is optimal. 因为我们的h比最优h小，在达到最优h之前，我们已经尝试过这个状态。\nWeighted A*算法 给heuristic函数加权，以调整搜索的速度。\nw越大，搜索越快，但可能会错过最优解，w越小，搜索越慢，但更有可能找到最优解。\nw=0时，等价于Uniform Cost Search。\nw to ∞时，等价于Greedy Best First Search。\ndef weightedAStarSearch(problem, heuristic=nullHeuristic, weight=1): \u0026#34;\u0026#34;\u0026#34;Search the node that has the lowest combined cost and heuristic first.\u0026#34;\u0026#34;\u0026#34; myPQ = util.PriorityQueue() startState = problem.getStartState() startNode = (startState, 0, []) myPQ.push(startNode, weight * heuristic(startState, problem)) best_g = dict() while not myPQ.isEmpty(): node = myPQ.pop() state, cost, path = node if (not state in best_g) or (cost \u0026lt; best_g[state]): best_g[state] = cost if problem.isGoalState(state): return path for succ in problem.getSuccessors(state): succState, succAction, succCost = succ new_cost = cost + succCost newNode = (succState, new_cost, path + [succAction]) myPQ.push(newNode, weight * heuristic(succState, problem) + new_cost) return None # Goal not found 爬山算法 local最优。\nσ := make-root-node(init()) 永远执行以下操作： 如果 is-goal(state(σ))： 返回 extract-solution(σ) Σ′ := {make-node(σ,a,s′) |(a,s′) ∈succ(state(σ)) } σ := 选择 Σ′ 中 h 值最小的元素 /* （随机打破平局） */ ","permalink":"http://localhost:1313/aquamega/algorithm/astar/","summary":"A*算法（带有重复检测和重新打开） 设 open 为新的优先级队列，按照 g(state(σ)) + h(state(σ)) 升序排列 open.insert(make-root-node(init())) 设 closed 为空集 设 best-g 为空集 /* 将状态映射到数字 */ 当 open 不为空时： σ := open.pop-min() 如果 state(σ) 不在 closed 中或 g(σ) \u0026lt; best-g(state(σ))： /* 如果 g 更好则重新打开；注意所有具有相同状态但 g 较差的 σ′ 都在 open 中位于 σ 后面，并且在轮到它们时将被跳过 */ closed := closed ∪{state(σ)} best-g(state(σ)) := g(σ) 如果 is-goal(state(σ))：返回 extract-solution(σ) 对于每个 (a,s′) ∈succ(state(σ))： σ′ := make-node(σ,a,s′) 如果 h(state(σ′)) \u0026lt; ∞：open.insert(σ′) 返回 unsolvable def aStarSearch(problem, heuristic=nullHeuristic): \u0026#34;\u0026#34;\u0026#34;Search the node that has the lowest combined cost and heuristic first.","title":"A* and Hill Climbing"},{"content":"广度优先搜索（BFS） 队列，先进先出，后进后出。\nOptimal when costs are uniform\ndef breadthFirstSearch(problem): candidate_queue = util.Queue() init_state = problem.getStartState() init_path = [] candidate_queue.push((init_state,init_path)) viewed = [] while not candidate_queue.isEmpty(): state,path = candidate_queue.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_queue.push((child_state,child_path)) return None 深度优先搜索（DFS） 栈，先进后出，后进先出。\n搜索空间可能无限大（无限深）。\ndef depthFirstSearch(problem): candidate_stack = util.Stack() init_state = problem.getStartState() init_path = [] candidate_stack.push((init_state,init_path)) viewed = [] while not candidate_stack.isEmpty(): state,path = candidate_stack.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_stack.push((child_state,child_path)) return None ","permalink":"http://localhost:1313/aquamega/algorithm/bfs/","summary":"广度优先搜索（BFS） 队列，先进先出，后进后出。\nOptimal when costs are uniform\ndef breadthFirstSearch(problem): candidate_queue = util.Queue() init_state = problem.getStartState() init_path = [] candidate_queue.push((init_state,init_path)) viewed = [] while not candidate_queue.isEmpty(): state,path = candidate_queue.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_queue.push((child_state,child_path)) return None 深度优先搜索（DFS） 栈，先进后出，后进先出。\n搜索空间可能无限大（无限深）。\ndef depthFirstSearch(problem): candidate_stack = util.Stack() init_state = problem.getStartState() init_path = [] candidate_stack.","title":"BFS and DFS"},{"content":" Insertion: O(1) expected, O(n) worst case Search: O(n) expected, O(n) worst case Deletion: O(n) expected, O(n) worst case class DynamicArray: def __init__(self): self.n = 0 # Number of elements self.capacity = 1 # Initial capacity self.A = self._make_array(self.capacity) def __len__(self): return self.n def __getitem__(self, k): if not 0 \u0026lt;= k \u0026lt; self.n: raise IndexError(\u0026#39;invalid index\u0026#39;) return self.A[k] def insert(self, element): \u0026#34;\u0026#34;\u0026#34; element = (id,key) \u0026#34;\u0026#34;\u0026#34; if self.n == self.capacity: self._resize(2 * self.capacity) self.A[self.n] = element self.n += 1 def delete(self, keydel): for i in range(self.n): if self.A[i][1] == keydel: self.A[i], self.A[self.n - 1] = self.A[self.n - 1], self.A[i] self.n -= 1 if self.n \u0026gt; 0 and self.n \u0026lt;= self.capacity // 4: self._resize(self.capacity // 2) break def search(self, keysch): for element in self.A[:self.n]: if element[1] == keysch: return element return None def _resize(self, new_cap): B = self._make_array(new_cap) for k in range(self.n): B[k] = self.A[k] self.A = B self.capacity = new_cap def _make_array(self, new_cap): return [None] * new_cap ","permalink":"http://localhost:1313/aquamega/datatype/array/","summary":"Insertion: O(1) expected, O(n) worst case Search: O(n) expected, O(n) worst case Deletion: O(n) expected, O(n) worst case class DynamicArray: def __init__(self): self.n = 0 # Number of elements self.capacity = 1 # Initial capacity self.A = self._make_array(self.capacity) def __len__(self): return self.n def __getitem__(self, k): if not 0 \u0026lt;= k \u0026lt; self.n: raise IndexError(\u0026#39;invalid index\u0026#39;) return self.A[k] def insert(self, element): \u0026#34;\u0026#34;\u0026#34; element = (id,key) \u0026#34;\u0026#34;\u0026#34; if self.n == self.capacity: self.","title":"Dynamic Array"},{"content":"Markov Decision Processes（MDPs）马尔可夫决策过程 MDP是完全可观察的，概率状态模型：\n状态空间 $S$ 初始状态 $s_0 \\in S$ 一组目标状态 $G \\subseteq S$ 在每个状态 $s \\in S$ 中可应用的动作 $A(s) \\subseteq A$ 对于 $s \\in S$ 和 $a \\in A(s)$，有转移概率 $P_a(s'|s)$ 动作成本 $c(a,s) \u003e 0$\n其中：\n解决方案是将状态映射到动作的函数（策略）\n最优解最小化预期的前往目标的成本\nPartially Observable MDPs (POMDPs) 部分可观察的马尔可夫决策过程 POMDP是部分可观察的，概率状态模型：\n状态 $s \\in S$ 在每个状态 $s \\in S$ 中可应用的动作 $A(s) \\subseteq A$ 对于 $s \\in S$ 和 $a \\in A(s)$，有转移概率 $P_a(s'|s)$ 初始信念状态 $b_0$ 最终信念状态 $b_f$ 由概率 $P_a(o|s)$，$o \\in Obs$ 给出的传感器模型\n其中：\n信念状态是关于 $S$ 的概率分布 解决方案是将信念状态映射到动作的策略 最优策略最小化从 $b_0$ 到 $G$ 的预期成本\nsee also: https://gibberblot.github.io/rl-notes/single-agent/MDPs.html\nValue Iteration 一种动态规划算法，用于计算MDP的最优策略。\ndef value_iteration(states, actions, P, r, gamma, theta): V = {s: 0 for s in states} # Initialize value function while True: delta = 0 for s in states: v = V[s] V[s] = max(sum(P[s][a][s_prime] * (r[s][a][s_prime] + gamma * V[s_prime]) for s_prime in states) for a in actions[s]) delta = max(delta, abs(v - V[s])) if delta \u0026lt; theta: # Check for convergence break return V Bellman Optimality Equation $ V^*(s) = \\max_{a \\in A(s)} \\sum_{s'} P_a(s'|s) \\left[ R_a(s'|s) + \\gamma V^*(s') \\right] $\n所有可能的下一个状态的概率 动作的奖励 下一个状态的价值 x 折扣，前一个iteration存储的价值 Q-Value 对于每个状态 $s \\in S$，其一个可能动作 $a \\in A(s)$ 的质量是：\n$ Q(s,a) = \\sum_{s'} P_a(s'|s) \\left[ R_a(s'|s) + \\gamma V^*(s') \\right] $\n其中 $\\gamma$ 是折扣，越接近1，越重视长期奖励，越接近0，越重视短期奖励。\n0.95, 0.9025, 0.857375, 0.81450625... 0.9, 0.81, 0.729, 0.6561... 0.8, 0.64, 0.512, 0.4096... 0.7, 0.49, 0.343, 0.2401... Policy $\\pi(s) = arg max Q(s,a)$\nMulti-Armed Bandit 平行地尝试多个动作，平衡exploitation和exploration。\nminimising regret，没有选择最佳动作的损失\n输入: 多臂老虎机问题 $M = \\{X_{i,k}, A, T\\}$ 输出: Q函数 $Q$ 初始化 $Q$ 为任意值; 例如，对所有的臂 $a$，$Q(a) \\leftarrow 0$ 初始化 $N$ 为任意值; 例如，对所有的臂 $a$，$N(a) \\leftarrow 0$ $k \\leftarrow 1$ while $k \\leq T$ do\n$\\quad$ $a \\leftarrow$ 在第 $k$ 轮选择一个臂\n$\\quad$ 在第 $k$ 轮执行臂 $a$ 并观察奖励 $X_{a,k}$\n$\\quad$ $N(a) \\leftarrow N(a) + 1$\n$\\quad$ $Q(a) \\leftarrow Q(a) + \\frac{1}{N(a)} [X_{a,k} - Q(a)]$\n$\\quad$ $k \\leftarrow k + 1$\nend while\n$\\epsilon$-greedy，以 $1-\\epsilon$ 的概率选择最佳动作，以 $\\epsilon$ 的概率选择随机动作\n$\\epsilon$-greedy with decay，随着时间的推移，减少 $\\epsilon$ 的值\nUCB\n$\\text{argmax}_{a}\\left(Q(a) + \\sqrt{\\frac{2 \\ln t}{N(a)}}\\right)$\nQ-Learning Input: MDP $M = \\langle S, s_0, A, P_a(s' | s), r(s, a, s') \\rangle$ Output: Q-function $Q$ Initialise $Q$ arbitrarily; e.g., $Q(s, a) \\leftarrow 0$ for all $s$ and $a$ repeat\n$\\quad$ $s \\leftarrow$ the first state in episode $e$\n$\\quad$ repeat (for each step in episode $e$)\n$\\quad\\quad$ Select action $a$ to apply in $s$; e.g. using $Q$ and a multi-armed bandit algorithm such as $\\epsilon$-greedy\n$\\quad\\quad$ Execute action $a$ in state $s$\n$\\quad\\quad$ Observe reward $r$ and new state $s'$\n$\\quad\\quad$ $\\delta \\leftarrow r + \\gamma \\cdot \\max_{a'} Q(s', a') - Q(s, a)$\n$\\quad\\quad$ $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\delta$\n$\\quad\\quad$ $s \\leftarrow s'$\n$\\quad$ until $s$ is the last state of episode $e$ (a terminal state)\nuntil $Q$ converges\n$max_{a'} Q(s', a')$ 也可以写成 $V(s')$，即下一个状态的价值 SARSA Input: MDP $M = \\langle S, s_0, A, P_a(s' | s), r(s, a, s') \\rangle$ Output: Q-function $Q$ Initialise $Q$ arbitrarily; e.g., $Q(s, a) \\leftarrow 0$ for all $s$ and $a$ repeat\n$\\quad$ $s \\leftarrow$ the first state in episode $e$\n$\\quad$ Choose $a$ from $s$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)\n$\\quad$ repeat (for each step in episode $e$)\n$\\quad\\quad$ Take action $a$, observe $r$, $s'$\n$\\quad\\quad$ Choose $a'$ from $s'$ using policy derived from $Q$ (e.g., $\\epsilon$-greedy)\n$\\quad\\quad$ $\\delta \\leftarrow r + \\gamma \\cdot Q(s', a') - Q(s, a)$\n$\\quad\\quad$ $Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\delta$\n$\\quad\\quad$ $s \\leftarrow s'$, $a \\leftarrow a'$\n$\\quad$ until $s$ is the last state of episode $e$ (a terminal state)\nuntil $Q$ converges\nQ-Learning是off-policy，因为on当前策略下的Q值，对当前策略更乐观 SARSA是on-policy，所以off了当前策略下的Q值，更保守 n-step reinforcement learning 记账，在n-step后再一起更新Q值。\nMCTS Selection：选择一个节点，直到找到一个未扩展的节点 Expansion：扩展一个未扩展的节点 Simulation：模拟一个随机游戏，直到结束 Backpropagation：更新所有访问的节点的值 offline：完成所有模拟后再选择最佳动作\nonline：每次模拟后选择最佳动作，继续对新的节点进行模拟。在下次选择时，同时也利用了之前的模拟结果，MCTS是online的。\n用平均值更新：新Q = 旧Q + 学习率 * 误差，实际上就是平均值\nUCT 用UCB来select。\n$\\text{argmax}_{a \\in A(s)} Q(s,a) + 2 C_p \\sqrt{\\frac{2 \\ln N(s)}{N(s,a)}}$ where $C_p$ 自己选，看是更偏向exploration还是exploitation\nLinear Q-functionn Approximation # features = # states * # actions\n$Q(s,a) = f^T w = \\sum_{i=1}^{n} f_i(s,a) w_i$\nUpdate $w \\leftarrow w + \\alpha \\delta f(s,a)$ where $\\delta = r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)$ if Q-learning $\\delta = r + \\gamma Q(s',a') - Q(s,a)$ if SARSA\nShaped Reward $Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\underbrace{F(s,s')}_{\\text{additional reward}} + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\nPotential-based Reward Shaping $F(s,s') = \\gamma \\Phi(s') - \\Phi(s)$\nFor example, in Gridworld, $\\Phi(s) = 1 - \\frac{|x(g) - x(s)| + |y(g) - y(s)|}{width + height - 2}$\nPolicy Iteration 魔改bellman方程，将所有动作可能性替换成当前策略下的动作。\n","permalink":"http://localhost:1313/aquamega/algorithm/mdps/","summary":"Markov Decision Processes（MDPs）马尔可夫决策过程 MDP是完全可观察的，概率状态模型：\n状态空间 $S$ 初始状态 $s_0 \\in S$ 一组目标状态 $G \\subseteq S$ 在每个状态 $s \\in S$ 中可应用的动作 $A(s) \\subseteq A$ 对于 $s \\in S$ 和 $a \\in A(s)$，有转移概率 $P_a(s'|s)$ 动作成本 $c(a,s) \u003e 0$\n其中：\n解决方案是将状态映射到动作的函数（策略）\n最优解最小化预期的前往目标的成本\nPartially Observable MDPs (POMDPs) 部分可观察的马尔可夫决策过程 POMDP是部分可观察的，概率状态模型：\n状态 $s \\in S$ 在每个状态 $s \\in S$ 中可应用的动作 $A(s) \\subseteq A$ 对于 $s \\in S$ 和 $a \\in A(s)$，有转移概率 $P_a(s'|s)$ 初始信念状态 $b_0$ 最终信念状态 $b_f$ 由概率 $P_a(o|s)$，$o \\in Obs$ 给出的传感器模型","title":"MDPs"},{"content":" Tree + Heap = Treap Insertion: O(log n) expected, O(n) worst case Search: O(log n) expected, O(n) worst case Deletion: O(log n) expected, O(n) worst case import random class Node: def __init__(self, id, key, priority): self.id = id self.key = key self.priority = priority self.left = None self.right = None class Treap: def __init__(self): self.root = None def _left_rotate(self, node): right_child = node.right node.right = right_child.left right_child.left = node return right_child def _right_rotate(self, node): left_child = node.left node.left = left_child.right left_child.right = node return left_child def insert(self, element): node = Node(element[0], element[1], random.random()) if not self.root: self.root = node return self.root = self._insert_node(self.root, node) def _insert_node(self, root, node): if not root: return node if node.key \u0026lt; root.key: root.left = self._insert_node(root.left, node) if root.left.priority \u0026gt; root.priority: root = self._right_rotate(root) else: root.right = self._insert_node(root.right, node) if root.right.priority \u0026gt; root.priority: root = self._left_rotate(root) return root def delete(self, keydel): self.root = self._delete_node(self.root, keydel) def _delete_node(self, root, keydel): if not root: return None if keydel \u0026lt; root.key: root.left = self._delete_node(root.left, keydel) elif keydel \u0026gt; root.key: root.right = self._delete_node(root.right, keydel) else: if not root.left and not root.right: root = None elif not root.left: root = root.right elif not root.right: root = root.left else: if root.left.priority \u0026lt; root.right.priority: root = self._right_rotate(root) root.right = self._delete_node(root.right, keydel) else: root = self._left_rotate(root) root.left = self._delete_node(root.left, keydel) return root def search(self, key): return self._search_node(self.root, key) def _search_node(self, root, key): if not root: return None if root.key == key: return (root.id, root.key) elif key \u0026lt; root.key: return self._search_node(root.left, key) else: return self._search_node(root.right, key) ","permalink":"http://localhost:1313/aquamega/datatype/treap/","summary":"Tree + Heap = Treap Insertion: O(log n) expected, O(n) worst case Search: O(log n) expected, O(n) worst case Deletion: O(log n) expected, O(n) worst case import random class Node: def __init__(self, id, key, priority): self.id = id self.key = key self.priority = priority self.left = None self.right = None class Treap: def __init__(self): self.root = None def _left_rotate(self, node): right_child = node.right node.right = right_child.left right_child.left = node return right_child def _right_rotate(self, node): left_child = node.","title":"Treap"},{"content":"Spatial Data Management Course notebook: https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/.\nGoal: Learn Postgre(pgAdmin 4) and GIS(QGIS). Help staff with herbarium data.\nVocabs scope creep: 任务蔓延 schema: 数据库的结构 query: 查询 ACID: Atomicity 原子性, Consistency 一致性, Isolation 隔离性, Durability 持久性 waterfall: 瀑布模型, 一次性完成所有工作 agile: 敏捷开发, 分阶段完成工作 ERD/ERM: Entity Relationship Diagram/Model, 实体关系图/模型 entity, attribute, relationship: 实体, 属性, 关系 location: space and time position: reference to a coordinate system 地理學第一定律: 所有事物都與其他事物相關, 但是近處的事物比遠處的事物更相關 longitude: 经度 latitude: 纬度 altitude: 海拔 wgs84: 地球坐标系, 经典的经纬度 polynomial: 多项式 ployline: 折线 polygon: 多边形 vertex: 顶点 vector: 向量 raster: 栅格 sphere: 球体 spheroid: 椭球体 Ideas 因为板块漂移, 会导致地理坐标系的变化 ","permalink":"http://localhost:1313/aquamega/course/geom90008/","summary":"Spatial Data Management Course notebook: https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/.\nGoal: Learn Postgre(pgAdmin 4) and GIS(QGIS). Help staff with herbarium data.\nVocabs scope creep: 任务蔓延 schema: 数据库的结构 query: 查询 ACID: Atomicity 原子性, Consistency 一致性, Isolation 隔离性, Durability 持久性 waterfall: 瀑布模型, 一次性完成所有工作 agile: 敏捷开发, 分阶段完成工作 ERD/ERM: Entity Relationship Diagram/Model, 实体关系图/模型 entity, attribute, relationship: 实体, 属性, 关系 location: space and time position: reference to a coordinate system 地理學第一定律: 所有事物都與其他事物相關, 但是近處的事物比遠處的事物更相關 longitude: 经度 latitude: 纬度 altitude: 海拔 wgs84: 地球坐标系, 经典的经纬度 polynomial: 多项式 ployline: 折线 polygon: 多边形 vertex: 顶点 vector: 向量 raster: 栅格 sphere: 球体 spheroid: 椭球体 Ideas 因为板块漂移, 会导致地理坐标系的变化 ","title":"Geom90008"},{"content":"Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission \u0026amp; Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:\nAttention Sequential model with attention Attention variants: Concat Dot product Scaled dot product Location-based Cosine similarity Global vs local attention Self-attention Machine Translation Statistical MT Neural MT with teacher forcing Transformer Mutli-head self-attention Position encoding Pretrained Language Models Encoder architecture BERT bidirectional context good for classification Encoder-decoder architecture Decoder architecture GPT unidirectional context good for generation Large Language Models In-context learning Step-by-step reasoning Human-feedback reinforcement Instruction following Named Entity Recognition Predict entities in a text Traditional ML methods Bi-LSTM with another RNN/CRF Multi-aspect NER Coreference Resolution 共指消解 Coreference, anaphora B-Cubed metric Question Answering and Reading Comprehension Knowledge-based QA Visual QA Spoken Language Understanding Attention RNNs Joint BERT Tri-level model Explainable NLU Vision Language Pretrained Model V-L Interaction Model Self-attention, co-attention, VSE Constrastive Language-Image Pretraining Ethics Sotial bias 刻板印象 Incivility 仇恨言论 Privacy Violation 歧视 Misinformation 谣言 Technological divide 发展不平衡 Vocab stemming:词干提取，去掉后缀，通常不是个词 lemmatisation:词形还原 morphology 詞法學 lexicon 字典 corpus 语料 tokenization 分词 entailment vs. contradiction 蕴涵与矛盾 neutral 中性 vanish 消失 distill 提炼 lexical 词汇的，与词汇或词语有关 semantic 语义的，与词语的含义或解释有关 syntactic 句法的，与句子结构有关 sentiment 情感，对某事物的态度 polysemy 一词多义 gloss 词义, from dictionary，词典中对词语含义的解释 synonymy 同义词 antonymy 反义词 hypernymy 上义词, is-a，是一个更一般的词 hyponymy 一个更具体的词 meronymy 下义词, is-part-of spectrum 范围 conference 共指，在文本中指代同一实体 anaphora 指代，一种语言现象，代替先前提到的词 utterence 话语 utter 说，发出声音 tedious 冗长的工作，乏味，枯燥 versality 可以在多种不同的情况和环境中使用，具有很高的适应性和灵活性 continuation 相连 patch-based 基于块的 interpretability 可解释性，白盒 explainability 可解释性，黑盒 monolingual 只使用一种语言的情况或者系统 sinusoidal 正弦的 proximal 接近的 miscellaneous 混杂的 antecedent 先行词, Trump anaphor 指代词, he pronominal 代词的 adjective 形容词 factoid 事实 Information Bottleneck 信息瓶颈问题是指在机器学习模型中，模型在尝试压缩输入数据的同时，还要尽可能保留与目标输出相关的信息。这是一个权衡问题，因为模型需要找到一个平衡点，既能有效地压缩数据，又能保留足够的信息来进行准确的预测。\n","permalink":"http://localhost:1313/aquamega/course/comp90042/","summary":"Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission \u0026amp; Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:","title":"Comp90042"},{"content":"Advanced Algorithms and Data Structures Learn some advanced algorithms and data structures.\nTreap\nAdmortized Analysis: Prepaid/Potential\nQuake Heap\nSplay Tree\nPerfect Hashing/Cuckoo Hashing\nRange Tree\nMin Cut/Max Flow\nKarger\u0026rsquo;s algorithm: 找最小割, 找多次取最优, 随机地找两个节点合并, 直到只剩下两个节点\nFord-Fulkerson algorithm: 最早的最大流算法, 重复地找增广路径, 直到找不到\nEdmonds-Karp algorithm: 用BFS找增广路径, complexity更低\nHall\u0026rsquo;s theorem: 一个二分图存在完美匹配当且仅当对于每一个子集, 子集的大小大于等于子集的邻居的大小\nVocabs conservation node: 保守节点, a node that has the same flow in and out\nresidual graph: 残余图, a graph that represents the remaining capacity of each edge\naugmenting path: 增广路径, a path from source to sink in the residual graph\nfeasible flow: 可行流, a flow that satisfies the capacity constraints and conservation constraints\nperfect matching: 完美匹配, a matching that covers all the nodes\nbipartite graph: 二分图, a graph that can be divided into two sets such that all edges are between the two sets\ndisjoint paths: 不相交路径, paths that do not share any nodes\nvertex cover: 顶点覆盖, a set of vertices that covers all the edges\nmaximal matching: 最大匹配, a matching that cannot be extended by adding another edge\n","permalink":"http://localhost:1313/aquamega/course/comp90077/","summary":"Advanced Algorithms and Data Structures Learn some advanced algorithms and data structures.\nTreap\nAdmortized Analysis: Prepaid/Potential\nQuake Heap\nSplay Tree\nPerfect Hashing/Cuckoo Hashing\nRange Tree\nMin Cut/Max Flow\nKarger\u0026rsquo;s algorithm: 找最小割, 找多次取最优, 随机地找两个节点合并, 直到只剩下两个节点\nFord-Fulkerson algorithm: 最早的最大流算法, 重复地找增广路径, 直到找不到\nEdmonds-Karp algorithm: 用BFS找增广路径, complexity更低\nHall\u0026rsquo;s theorem: 一个二分图存在完美匹配当且仅当对于每一个子集, 子集的大小大于等于子集的邻居的大小\nVocabs conservation node: 保守节点, a node that has the same flow in and out\nresidual graph: 残余图, a graph that represents the remaining capacity of each edge","title":"Comp90077"},{"content":"AI Planning for Autonomy classical planning (blind/heuristic): https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf PDDL relaxation reinforcement learning: https://gibberblot.github.io/rl-notes/index.html Vocabs acyclic 无环\nsystematics 系统的 / local 局部\nheuristic 启发式\nmonotonic 单调\npriority queue = min heap\nconformant 符合的\npolicy 策略，map from state to action, denoted by $\\pi$\nnegation 否定\nprecondition 前提条件\npropositional 命题\npredicate 谓词，return true/false\nschema 模式，define somthing\nmisplace 放错\ndominate 支配\ncorollary 推论\npoint-wise 逐点\npessimistic 悲观\nbenchmark 基准\nnovelty 新颖性\nprune 修剪\nvelocity 速度\nstochastic 随机\ntabular 表格\ndilemma 困境\nequilibria 平衡\n, set minus operation\npayoff 收益\nutility 效用\nNondeterministic Polynomial: 不确定多项式，没有已知的多项式时间算法可以在所有情况下找到一个解\nnon-deterministic: countless outcomes mixed strategy equilibria: 混合策略均衡\nnash equilibrium: 纳什均衡，每个玩家都在最佳响应下(pure strategy)，没有人会改变策略\n安全/目标感知/可接受/一致性 $h$ remaining cost to reach the goal, $*$ optimal\n假设$\\Pi$是一个计划任务，具有状态空间$\\Theta_{\\Pi} = (S, L, c, T, I, G)$，并且$h$是$\\Pi$的一个启发式。如果启发式满足以下条件，那么它被称为：\n安全（Safe）：如果对于所有$h(s) = \\infty$的状态$s \\in S$，都有$h^*(s) = \\infty$，则启发式被称为安全。\n目标感知（Goal-aware）：如果对于所有目标状态$s \\in S_G$，都有$h(s) = 0$，则启发式被称为目标感知。\n可接受（Admissible）：如果对于所有状态$s \\in S$，都有$h(s) \\leq h^*(s)$，则启发式被称为可接受。\n一致（Consistent）：如果对于所有$s \\xrightarrow{a} s'$的转移，都有$h(s) \\leq h(s') + c(a)$，则启发式被称为一致。\n命题：假设$\\Pi$是一个计划任务，具有状态空间$\\Theta_{\\Pi} = (S, L, c, T, I, S_G)$，并且$h$是$\\Pi$的一个启发式。\n如果$h$是一致的和目标感知的，则$h$是可接受的。 如果$h$是可接受的，则$h$是目标感知的。 如果$h$是可接受的，则$h$是安全的。 没有其他这种形式的蕴含关系成立。 不可接受：最优的节点如果被高估，就会优先扩展其他节点，而错过最优。\n一致性：保证最优路径依次访问。\nSTRIPS: 问题是一个四元组$P = \\langle F,O,I,G \\rangle$： $F$ fact, 原子, 变量\n$O$ 或 $A$ operator, action, 操作符, 动作\n$I \\subseteq F$代表初始情况\n$G \\subseteq F$代表目标情况\n操作符$o \\in O$由以下表示：\n添加列表$Add(o) \\subseteq F$ 删除列表$Del(o) \\subseteq F$ 前提条件列表$Pre(o) \\subseteq F$ Relaxiation: Goal：Helps compute heuristic function。\n设$h^* : P \\rightarrow R^+_0 \\cup \\{\\infty\\}$是一个函数。$h^*$的松弛是一个三元组$R= (P',r,h'^*)$，其中$P'$是任意集合，$r : P \\rightarrow P'$和$h'^* : P' \\rightarrow R^+_0 \\cup \\{\\infty\\}$是函数，对于所有的$\\Pi \\in P$，松弛启发式$h_R(\\Pi) := h'^*(r(\\Pi))$满足$h_R(\\Pi) \\leq h^*(\\Pi)$。松弛是：\n问题$P$：寻路。 更简单的问题$P'$：鸟类的寻路。 $P'$的完美启发式$h'^*$：直线距离。 转换$r$：假装你是一只鸟。 原生的，如果$P' \\subseteq P$且$h'^* = h^*$； 可有效构造的，如果存在一个多项式时间算法，给定$\\Pi \\in P$，可以计算$r(\\Pi)$； 可有效计算的，如果存在一个多项式时间算法，给定$\\Pi' \\in P'$，可以计算$h'^*(\\Pi')$。 提醒：你有一个问题$P$，你希望估计其完美启发式$h^*$。你定义了一个更简单的问题$P'$，其完美启发式$h'^*$可以用来（可接受地！）估计$h^*$。你定义了一个从$P$到$P'$的转换$r$。给定$\\Pi \\in P$，你通过$h'^*(r(\\Pi))$来估计$h^*(\\Pi)$。\nnotation in this course:\n$\\Pi_s$：将初始状态替换为$s$的$\\Pi$，即，将$\\Pi = (F,A,c,I,G)$更改为$(F,A,c,s,G)$。\nc: clause, preconditions+effects 有关Quality Value的一些概念： Value：在强化学习中，value通常指的是一个状态的价值，也就是从这个状态开始，遵循某个策略能够获得的预期回报。\nQ-value：Q-value是对于状态-动作对(state-action pair)的价值的一种评估。也就是说，如果在某个状态下执行某个动作，然后遵循某个策略，能够获得的预期回报。\nQ-value table：Q-value table是一种数据结构，用于存储每个状态-动作对的Q-value。在表格型强化学习算法（如Q-learning）中，Q-value table是主要的数据结构。\nQ-value function：Q-value function是一个函数，它接受一个状态和一个动作作为输入，返回这个状态-动作对的Q-value。在函数逼近方法（如深度Q网络）中，Q-value function通常由神经网络来表示。\nQ-table：Q-table和Q-value table是同一个概念，只是名称不同。\n","permalink":"http://localhost:1313/aquamega/course/comp90054/","summary":"AI Planning for Autonomy classical planning (blind/heuristic): https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf PDDL relaxation reinforcement learning: https://gibberblot.github.io/rl-notes/index.html Vocabs acyclic 无环\nsystematics 系统的 / local 局部\nheuristic 启发式\nmonotonic 单调\npriority queue = min heap\nconformant 符合的\npolicy 策略，map from state to action, denoted by $\\pi$\nnegation 否定\nprecondition 前提条件\npropositional 命题\npredicate 谓词，return true/false\nschema 模式，define somthing\nmisplace 放错\ndominate 支配\ncorollary 推论\npoint-wise 逐点\npessimistic 悲观\nbenchmark 基准\nnovelty 新颖性\nprune 修剪\nvelocity 速度","title":"Comp90054"},{"content":"Hello World!\n","permalink":"http://localhost:1313/aquamega/posts/welcome/","summary":"Hello World!","title":"Welcome"}]
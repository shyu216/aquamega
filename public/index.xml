<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Aqua Mega</title>
    <link>http://localhost:1313/aquamega/</link>
    <description>Recent content on Aqua Mega</description>
    <image>
      <title>Aqua Mega</title>
      <url>https://github.com/shyu216.png</url>
      <link>https://github.com/shyu216.png</link>
    </image>
    <generator>Hugo -- 0.125.4</generator>
    <language>en</language>
    <lastBuildDate>Wed, 29 May 2024 11:13:45 +1000</lastBuildDate>
    <atom:link href="http://localhost:1313/aquamega/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>UCB and Greedy BFS</title>
      <link>http://localhost:1313/aquamega/algorithm/uniform/</link>
      <pubDate>Wed, 29 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/uniform/</guid>
      <description>Uniform Cost Search Priority Queue，最先被探索离起始节点最近（即路径成本最低）的节点。
Breath First Search属于Uniform Cost Search的特例，即每个节点的路径成本都是1。
UCS只看到了路径成本，没有考虑启发式。
def uniformCostSearch(problem): candidate_pq = util.PriorityQueue() init_state = problem.getStartState() init_path = [] candidate_pq.push((init_state,init_path),0) viewed = [] while not candidate_pq.isEmpty(): state,path = candidate_pq.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, child_cost in problem.getSuccessors(state): child_path = path + [child_action] candidate_pq.push((child_state,child_path),problem.getCostOfActions(child_path)) return None Greedy Best First Search BFS只看到了启发式，没有考虑路径成本。
If h=0，BFS是什么根据它的Priority Queue的实现。
def bestFirstSearch(problem, heuristic=nullHeuristic): candidate_pq = util.PriorityQueue() init_state = problem.</description>
    </item>
    <item>
      <title>Neual Networks</title>
      <link>http://localhost:1313/aquamega/nlp/nn/</link>
      <pubDate>Tue, 28 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/nn/</guid>
      <description>非线性激活函数是神经网络中的关键组成部分，它可以帮助神经网络学习和逼近复杂的模式。非线性激活函数的主要目的是将输入信号转换为输出信号，但它不仅仅是复制输入到输出。它会修改或者变换输入的方式，使得可以适应网络的需要。 常见的非线性激活函数包括：
ReLU（Rectified Linear Unit）：这是最常用的激活函数，它将所有负值设为0，正值保持不变。
Sigmoid：这个函数将任何数值都映射到0和1之间，它在早期的神经网络中非常流行，但现在已经较少使用，因为它在输入值很大或很小的时候，梯度几乎为0，这会导致梯度消失问题。
Tanh（Hyperbolic Tangent）：这个函数将任何数值都映射到-1和1之间，它比sigmoid函数的输出范围更广，因此在某些情况下，它的性能比sigmoid函数更好。
Softmax：这个函数通常用在神经网络的输出层，特别是在处理多分类问题时。它可以将一组数值转换为概率分布。
Regularization 正则化 L1 Norm L2 Norm Dropout </description>
    </item>
    <item>
      <title>Part-of-Speech Tagging</title>
      <link>http://localhost:1313/aquamega/nlp/pos/</link>
      <pubDate>Tue, 28 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/pos/</guid>
      <description>Part-of-Speech(词类) Colourless green ideas sleep furiously.
Open Classes (Content Words) Noun Verb Adjective Adverb Closed Classes (Function Words) (Fixed number of words) Preposition Determiner Pronoun Conjunction Interjection Ambiguity: A word can have multiple meanings. A sentence can have multiple interpretations.</description>
    </item>
    <item>
      <title>N-gram Language Model</title>
      <link>http://localhost:1313/aquamega/nlp/n-gram/</link>
      <pubDate>Mon, 27 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/n-gram/</guid>
      <description>Language Model To explain language, based on probability, for generation.
Query completion(搜索引擎) Optical character recognition(OCR) Translation N-gram $$P(w_1, w_2, \cdots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2) \cdots P(w_n|w_1, w_2, \cdots, w_{n-1})$$ Maximum Likelihood Estimation $$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$ Use special tags to denote the beginning and end of a sentence, i.e. &amp;lt;s&amp;gt;, &amp;lt;/s&amp;gt;.
Smoothing Handle unseen words.
Laplacian (add-one) smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$
Add-k smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + kV}$</description>
    </item>
    <item>
      <title>Preprocessing</title>
      <link>http://localhost:1313/aquamega/nlp/preprocessing/</link>
      <pubDate>Mon, 27 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/preprocessing/</guid>
      <description>Preprocessing Word Sentence: words Document: sentences Corpus: documents Word Token: word instance Word Type: verb, noun, etc. Lexicon: word types Steps Remove unwnated formatting, i.e. HTML tags Sentecne segmentation, break text into sentences Tokenization, break sentences into words Normalization, transform words into canonical form, i.e. lower case Stopword removal, delete unwanted words, i.e., &amp;ldquo;the&amp;rdquo;, &amp;ldquo;I&amp;rdquo;, &amp;ldquo;.&amp;rdquo;, etc. Max Match Algorithm Greedily match the longest word in the dictionary. Used in the language withoout space between words, i.</description>
    </item>
    <item>
      <title>Text Classification</title>
      <link>http://localhost:1313/aquamega/nlp/classification/</link>
      <pubDate>Mon, 27 May 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/classification/</guid>
      <description>Topic classification Sentiment analysis Native language identification Natual language inference Automatic fact-checking Paraphrase Topic Classification Motivation: library science, information retrieval, etc. Classes: topic categories Features: bag-of-words, n-grams, etc. Corpus: reuters news corpus, pubmed abstracts, tweets with hashtags Sentiment Analysis Motivation: opinion mining, business analytics Classes: positive, negative, neutral Features: n-grams, polarity lexicons, etc. Corpus: movie reviews, SEMEVAL twitter polarity dataset Native Language Identification Motivation: forensic linguistics, educational applications Classes: native languages Features: character n-grams, syntactic features (POS), phonological features Corpus: TOEFL/IELTS essays Natural Language Inference (textual entailment 文本蕴含) Motivation: language understanding Classes: entailment, contradiction, neutral Features: word overlap, length difference, etc.</description>
    </item>
    <item>
      <title>A*</title>
      <link>http://localhost:1313/aquamega/algorithm/astar/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/astar/</guid>
      <description>A*算法（带有重复检测和重新打开） 设 open 为新的优先级队列，按照 g(state(σ)) + h(state(σ)) 升序排列 open.insert(make-root-node(init())) 设 closed 为空集 设 best-g 为空集 /* 将状态映射到数字 */ 当 open 不为空时： σ := open.pop-min() 如果 state(σ) 不在 closed 中或 g(σ) &amp;lt; best-g(state(σ))： /* 如果 g 更好则重新打开；注意所有具有相同状态但 g 较差的 σ′ 都在 open 中位于 σ 后面，并且在轮到它们时将被跳过 */ closed := closed ∪{state(σ)} best-g(state(σ)) := g(σ) 如果 is-goal(state(σ))：返回 extract-solution(σ) 对于每个 (a,s′) ∈succ(state(σ))： σ′ := make-node(σ,a,s′) 如果 h(state(σ′)) &amp;lt; ∞：open.insert(σ′) 返回 unsolvable def aStarSearch(problem, heuristic=nullHeuristic): &amp;#34;&amp;#34;&amp;#34;Search the node that has the lowest combined cost and heuristic first.</description>
    </item>
    <item>
      <title>BFS and DFS</title>
      <link>http://localhost:1313/aquamega/algorithm/bfs/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/bfs/</guid>
      <description>广度优先搜索（BFS） 队列，先进先出，后进后出。
Optimal when costs are uniform
def breadthFirstSearch(problem): candidate_queue = util.Queue() init_state = problem.getStartState() init_path = [] candidate_queue.push((init_state,init_path)) viewed = [] while not candidate_queue.isEmpty(): state,path = candidate_queue.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_queue.push((child_state,child_path)) return None 深度优先搜索（DFS） 栈，先进后出，后进先出。
搜索空间可能无限大（无限深）。
def depthFirstSearch(problem): candidate_stack = util.Stack() init_state = problem.getStartState() init_path = [] candidate_stack.</description>
    </item>
    <item>
      <title>Dynamic Array</title>
      <link>http://localhost:1313/aquamega/datatype/array/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/datatype/array/</guid>
      <description>Insertion: O(1) expected, O(n) worst case Search: O(n) expected, O(n) worst case Deletion: O(n) expected, O(n) worst case class DynamicArray: def __init__(self): self.n = 0 # Number of elements self.capacity = 1 # Initial capacity self.A = self._make_array(self.capacity) def __len__(self): return self.n def __getitem__(self, k): if not 0 &amp;lt;= k &amp;lt; self.n: raise IndexError(&amp;#39;invalid index&amp;#39;) return self.A[k] def insert(self, element): &amp;#34;&amp;#34;&amp;#34; element = (id,key) &amp;#34;&amp;#34;&amp;#34; if self.n == self.capacity: self.</description>
    </item>
    <item>
      <title>MDPs</title>
      <link>http://localhost:1313/aquamega/algorithm/mdps/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/mdps/</guid>
      <description>Markov Decision Processes（MDPs）马尔可夫决策过程 MDP是完全可观察的，概率状态模型：
状态空间 $S$ 初始状态 $s_0 \in S$ 一组目标状态 $G \subseteq S$ 在每个状态 $s \in S$ 中可应用的动作 $A(s) \subseteq A$ 对于 $s \in S$ 和 $a \in A(s)$，有转移概率 $P_a(s&#39;|s)$ 动作成本 $c(a,s) &gt; 0$
其中：
解决方案是将状态映射到动作的函数（策略）
最优解最小化预期的前往目标的成本
Partially Observable MDPs (POMDPs) 部分可观察的马尔可夫决策过程 POMDP是部分可观察的，概率状态模型：
状态 $s \in S$ 在每个状态 $s \in S$ 中可应用的动作 $A(s) \subseteq A$ 对于 $s \in S$ 和 $a \in A(s)$，有转移概率 $P_a(s&#39;|s)$ 初始信念状态 $b_0$ 最终信念状态 $b_f$ 由概率 $P_a(o|s)$，$o \in Obs$ 给出的传感器模型</description>
    </item>
    <item>
      <title>Treap</title>
      <link>http://localhost:1313/aquamega/datatype/treap/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/datatype/treap/</guid>
      <description>Tree + Heap = Treap Insertion: O(log n) expected, O(n) worst case Search: O(log n) expected, O(n) worst case Deletion: O(log n) expected, O(n) worst case import random class Node: def __init__(self, id, key, priority): self.id = id self.key = key self.priority = priority self.left = None self.right = None class Treap: def __init__(self): self.root = None def _left_rotate(self, node): right_child = node.right node.right = right_child.left right_child.left = node return right_child def _right_rotate(self, node): left_child = node.</description>
    </item>
    <item>
      <title>Geom90008</title>
      <link>http://localhost:1313/aquamega/course/geom90008/</link>
      <pubDate>Fri, 26 Apr 2024 14:56:11 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/geom90008/</guid>
      <description>Spatial Data Management Course notebook: https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/.
Goal: Learn Postgre(pgAdmin 4) and GIS(QGIS). Help staff with herbarium data.
Vocabs scope creep: 任务蔓延 schema: 数据库的结构 query: 查询 ACID: Atomicity 原子性, Consistency 一致性, Isolation 隔离性, Durability 持久性 waterfall: 瀑布模型, 一次性完成所有工作 agile: 敏捷开发, 分阶段完成工作 ERD/ERM: Entity Relationship Diagram/Model, 实体关系图/模型 entity, attribute, relationship: 实体, 属性, 关系 location: space and time position: reference to a coordinate system 地理學第一定律: 所有事物都與其他事物相關, 但是近處的事物比遠處的事物更相關 longitude: 经度 latitude: 纬度 altitude: 海拔 wgs84: 地球坐标系, 经典的经纬度 polynomial: 多项式 ployline: 折线 polygon: 多边形 vertex: 顶点 vector: 向量 raster: 栅格 sphere: 球体 spheroid: 椭球体 Ideas 因为板块漂移, 会导致地理坐标系的变化 </description>
    </item>
    <item>
      <title>Comp90042</title>
      <link>http://localhost:1313/aquamega/course/comp90042/</link>
      <pubDate>Fri, 26 Apr 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90042/</guid>
      <description>Natural Language Processing preprocessing byte pair encoding classification n-gram part-of-speech tagging hidden markov model recurrent neural network LSTM transformer BERT Vocab vanish: 消失 distill: 提炼 lexical: 词汇的 semetic: 语义的 sentiment: 情感 polysemy: 一词多义 gloss: 词义, from dictionary synonymy: 同义词 antonymy: 反义词 hypernymy: 上义词, is-a meronymy: 下义词, is-part-of spectrum: 范围 conferencing: 会议 </description>
    </item>
    <item>
      <title>Comp90077</title>
      <link>http://localhost:1313/aquamega/course/comp90077/</link>
      <pubDate>Fri, 26 Apr 2024 14:55:58 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90077/</guid>
      <description>Advanced Algorithms and Data Structures Learn some advanced algorithms and data structures.
Treap
Admortized Analysis: Prepaid/Potential
Quake Heap
Splay Tree
Perfect Hashing/Cuckoo Hashing
Range Tree
Min Cut/Max Flow
Karger&amp;rsquo;s algorithm: 找最小割, 找多次取最优, 随机地找两个节点合并, 直到只剩下两个节点
Ford-Fulkerson algorithm: 最早的最大流算法, 重复地找增广路径, 直到找不到
Edmonds-Karp algorithm: 用BFS找增广路径, complexity更低
Hall&amp;rsquo;s theorem: 一个二分图存在完美匹配当且仅当对于每一个子集, 子集的大小大于等于子集的邻居的大小
Vocabs conservation node: 保守节点, a node that has the same flow in and out
residual graph: 残余图, a graph that represents the remaining capacity of each edge</description>
    </item>
    <item>
      <title>Comp90054</title>
      <link>http://localhost:1313/aquamega/course/comp90054/</link>
      <pubDate>Fri, 26 Apr 2024 14:55:43 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90054/</guid>
      <description>AI Planning for Autonomy classical planning (blind/heuristic): https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf PDDL relaxation reinforcement learning: https://gibberblot.github.io/rl-notes/index.html Vocabs acyclic 无环
systematics 系统的 / local 局部
heuristic 启发式
monotonic 单调
priority queue = min heap
Nondeterministic Polynomial: 不确定多项式，没有已知的多项式时间算法可以在所有情况下找到一个解
non-deterministic: countless outcomes mixed strategy equilibria: 混合策略均衡
nash equilibrium: 纳什均衡，每个玩家都在最佳响应下(pure strategy)，没有人会改变策略
安全/目标感知/可接受/一致性 $h$ remaining cost to reach the goal, $*$ optimal
假设$\Pi$是一个计划任务，具有状态空间$\Theta_{\Pi} = (S, L, c, T, I, G)$，并且$h$是$\Pi$的一个启发式。如果启发式满足以下条件，那么它被称为：
安全（Safe）：如果对于所有$h(s) = \infty$的状态$s \in S$，都有$h^*(s) = \infty$，则启发式被称为安全。
目标感知（Goal-aware）：如果对于所有目标状态$s \in S_G$，都有$h(s) = 0$，则启发式被称为目标感知。</description>
    </item>
    <item>
      <title>Welcome</title>
      <link>http://localhost:1313/aquamega/posts/welcome/</link>
      <pubDate>Fri, 26 Apr 2024 14:22:49 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/posts/welcome/</guid>
      <description>Hello World!</description>
    </item>
  </channel>
</rss>

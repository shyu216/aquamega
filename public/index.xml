<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Aqua Mega</title>
    <link>http://localhost:1313/aquamega/</link>
    <description>Recent content on Aqua Mega</description>
    <image>
      <title>Aqua Mega</title>
      <url>https://github.com/shyu216.png</url>
      <link>https://github.com/shyu216.png</link>
    </image>
    <generator>Hugo -- 0.125.4</generator>
    <language>en</language>
    <lastBuildDate>Thu, 30 May 2024 11:13:45 +1000</lastBuildDate>
    <atom:link href="http://localhost:1313/aquamega/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Game Theory</title>
      <link>http://localhost:1313/aquamega/algorithm/game/</link>
      <pubDate>Thu, 30 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/game/</guid>
      <description>pure Strategy: single action
mixed Strategy: probability distribution over actions
weakly dominate: $\leq$
strongly dominate: $&lt;$
a weakly(strictly) dominant strategy: always better than any other strategy
nash equilibrium: 每个agent都选了最优策略，其他agent不会改变策略
indifference: 通过调整我的策略概率，改变对手的收益（payoff）期望，使无论对手如何选择，他的满意度（utility）都一样
Utility Function U_i(a): what can agent i get from action a
Normal Form Game 一轮，不知道对手的策略，只知道对手的utility function
Extensive Form Game 广义形式博弈 轮流决策，所以知道对手的策略
Subgame Perfect Equilibrium 子博弈完美均衡 当前玩家在他的回合的最优策略，对手在他的回合的最优策略。。。
Backward Induction 反向归纳 输入：广义形式博弈 G = (N, Agt, S, s_0, A, T, r) 输出：每个状态 s ∈ S 的子博弈均衡 函数 BackwardInduction(s ∈ S): 如果 A(s) = ∅，则返回 r(s) best_child ← (-∞, .</description>
    </item>
    <item>
      <title>Greedy Relaxed Planning</title>
      <link>http://localhost:1313/aquamega/algorithm/relax/</link>
      <pubDate>Wed, 29 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/relax/</guid>
      <description>h*: the optimal heuristic
h pre&amp;amp;del: the heuristic ignoring preconditions and delete effects, adimissible and consistent, &amp;ldquo;subset sum&amp;rdquo; problem, NP-hard
h goal_count: the number of goals not yet achieved, neither admissible nor consistent
h+/h del: admitssible and consistent, 相当于MST, 每个state只访问一遍，也NP-hard
h add: easily not admissible
h max: easily too small
h ff: use h add and h max
Removing preconditions and delete effects is efficiently constructive but not computable。</description>
    </item>
    <item>
      <title>UCB and Greedy BFS</title>
      <link>http://localhost:1313/aquamega/algorithm/uniform/</link>
      <pubDate>Wed, 29 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/uniform/</guid>
      <description>Uniform Cost Search Priority Queue，最先被探索离起始节点最近（即路径成本最低）的节点。
Breath First Search属于Uniform Cost Search的特例，即每个节点的路径成本都是1。
UCS只看到了路径成本，没有考虑启发式。
def uniformCostSearch(problem): candidate_pq = util.PriorityQueue() init_state = problem.getStartState() init_path = [] candidate_pq.push((init_state,init_path),0) viewed = [] while not candidate_pq.isEmpty(): state,path = candidate_pq.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, child_cost in problem.getSuccessors(state): child_path = path + [child_action] candidate_pq.push((child_state,child_path),problem.getCostOfActions(child_path)) return None Greedy Best First Search BFS只看到了启发式，没有考虑路径成本。
If h=0，BFS是什么根据它的Priority Queue的实现。
def bestFirstSearch(problem, heuristic=nullHeuristic): candidate_pq = util.PriorityQueue() init_state = problem.</description>
    </item>
    <item>
      <title>Part-of-Speech Tagging</title>
      <link>http://localhost:1313/aquamega/nlp/pos/</link>
      <pubDate>Tue, 28 May 2024 03:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/pos/</guid>
      <description>Part-of-Speech(词类) Colourless green ideas sleep furiously.
Open Classes (Content Words) Noun Verb Adjective Adverb Closed Classes (Function Words) (Fixed number of words) Preposition Determiner Pronoun Conjunction Interjection Ambiguity: A word can have multiple meanings. A sentence can have multiple interpretations.
在自然语言处理（NLP）中，POS代表词性标注（Part-of-Speech Tagging）。词性标注是一种将每个词语与其语法范畴（即词性）相关联的过程。它是NLP中的一项基本任务，用于确定句子中每个单词的词类。
常见的词性包括名词、动词、形容词、副词、介词、代词、冠词、连词等。通过进行词性标注，可以帮助计算机理解句子的结构和含义，为其他NLP任务（如句法分析、语义分析和机器翻译等）提供基础。
以下是一个例子，展示了一句英文句子的词性标注示例：
句子：I love eating ice cream.
词性标注：PRON VERB VERB NOUN NOUN.
在这个例子中，&amp;ldquo;I&amp;quot;被标注为代词（PRON），&amp;ldquo;love&amp;quot;和&amp;quot;eating&amp;quot;被标注为动词（VERB），&amp;ldquo;ice&amp;quot;和&amp;quot;cream&amp;quot;被标注为名词（NOUN）。
词性标注可以通过使用基于规则的方法、基于统计的机器学习方法（如隐马尔可夫模型）或基于深度学习的方法（如循环神经网络和转换器模型）来实现。这个任务在NLP中具有广泛的应用，对于许多文本处理任务都是重要的预处理步骤。
NN（noun）名词：cat（猫）、book（书）、table（桌子） VB（verb）动词：run（跑）、eat（吃）、sleep（睡觉） JJ（adjective）形容词：big（大的）、happy（快乐的）、red（红色的） RB（adverb）副词：quickly（快速地）、well（好地）、very（非常） DT（determiner）限定词：the（定冠词）、this（这个）、some（一些） CD（cardinal number）基数词：one（一）、three（三）、ten（十） IN（preposition）介词：on（在&amp;hellip;上）、with（和&amp;hellip;一起）、at（在&amp;hellip;处） PRP（personal pronoun）人称代词：I（我）、you（你）、he（他） MD（modal）情态动词：can（能够）、should（应该）、will（将会） CC（coordinating conjunction）并列连词：and（和）、but（但是）、or（或者） RP（particle）小品词：up（向上）、down（向下）、out（出去） WH（wh-pronoun）疑问代词：who（谁）、what（什么）、which（哪个） TO（to）不定式标记：to（去）、to（为了）、to（到） Penn Treebank 词性标记集 Penn Treebank 的词性标注集（POS tagset）是一种广泛使用的英文词性标注方法。</description>
    </item>
    <item>
      <title>Lexical Semantics and Distributional Semantics</title>
      <link>http://localhost:1313/aquamega/nlp/semantics/</link>
      <pubDate>Tue, 28 May 2024 00:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/semantics/</guid>
      <description>Lexical Semantics 词汇解释
Synonymy 同义词 Antonymy 反义词 Hypernymy 上义词 Meronymy 下义词 WordNet A lexical database of English.
Word Similarity 路径长度：在WordNet中，两个词之间的路径长度可以用来衡量它们的相似性。 深度信息：在WordNet中，词的深度（在词汇树中的位置）也可以用来衡量词的相似性。 信息内容：词的信息内容（在语料库中的频率）也可以用来衡量词的相似性。 Distributional Semantics 词汇和其上下文的共现信息</description>
    </item>
    <item>
      <title>Neual Networks</title>
      <link>http://localhost:1313/aquamega/nlp/nn/</link>
      <pubDate>Tue, 28 May 2024 00:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/nn/</guid>
      <description>神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。
每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。
神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。
神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。
A Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.
Each neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.</description>
    </item>
    <item>
      <title>Hidden Markov Model</title>
      <link>http://localhost:1313/aquamega/nlp/hmm/</link>
      <pubDate>Mon, 27 May 2024 03:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/hmm/</guid>
      <description> w：word t：tag emission probability: $P(w|t)$ 从tag到word的概率 transition probability: $P(t|t&#39;)$ 从上一个tag到当前tag的概率 Viterbi Algorithm 从最开始算起，每个tag的概率是由前一个tag的概率和从前一个tag到当前tag的概率相乘得到的。
$\hat{t} = \arg\max_{t} \prod_{i=1}^{n} P(w_i|t)P(t|t&#39;)$, $t&#39;$是前一个tag, $\hat{t}$是当前的估计值。
Generative vs Discriminative Tagger HMM是generative model，可以用来生成数据，无监督的学习。 discriminative model例如CRF，需要有监督的数据，可以学到更丰富的特征，大多数deep learning的模型都是discriminative model。 </description>
    </item>
    <item>
      <title>Text Classification</title>
      <link>http://localhost:1313/aquamega/nlp/classification/</link>
      <pubDate>Mon, 27 May 2024 02:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/classification/</guid>
      <description>Topic classification Sentiment analysis Native language identification Natual language inference Automatic fact-checking Paraphrase Topic Classification Motivation: library science, information retrieval, etc. Classes: topic categories Features: bag-of-words, n-grams, etc. Corpus: reuters news corpus, pubmed abstracts, tweets with hashtags Sentiment Analysis Motivation: opinion mining, business analytics Classes: positive, negative, neutral Features: n-grams, polarity lexicons, etc. Corpus: movie reviews, SEMEVAL twitter polarity dataset Native Language Identification Motivation: forensic linguistics, educational applications Classes: native languages Features: character n-grams, syntactic features (POS), phonological features Corpus: TOEFL/IELTS essays Natural Language Inference (textual entailment 文本蕴含) Motivation: language understanding Classes: entailment, contradiction, neutral Features: word overlap, length difference, etc.</description>
    </item>
    <item>
      <title>N-gram Language Model</title>
      <link>http://localhost:1313/aquamega/nlp/n-gram/</link>
      <pubDate>Mon, 27 May 2024 01:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/n-gram/</guid>
      <description>Language Model To explain language, based on probability, for generation.
Query completion(搜索引擎) Optical character recognition(OCR) Translation N-gram $$P(w_1, w_2, \cdots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1, w_2) \cdots P(w_n|w_1, w_2, \cdots, w_{n-1})$$ Maximum Likelihood Estimation $$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i)}{C(w_{i-1})}$$ Use special tags to denote the beginning and end of a sentence, i.e. &amp;lt;s&amp;gt;, &amp;lt;/s&amp;gt;.
Smoothing Handle unseen words.
Laplacian (add-one) smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$
Add-k smoothing
$P(w_i|w_{i-1}) = \frac{C(w_{i-1}, w_i) + k}{C(w_{i-1}) + kV}$</description>
    </item>
    <item>
      <title>Preprocessing</title>
      <link>http://localhost:1313/aquamega/nlp/preprocessing/</link>
      <pubDate>Mon, 27 May 2024 00:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/nlp/preprocessing/</guid>
      <description>Preprocessing Word Sentence: words Document: sentences Corpus: documents Word Token: word instance Word Type: verb, noun, etc. Lexicon: word types Steps Remove unwnated formatting, i.e. HTML tags Sentecne segmentation, break text into sentences Tokenization, break sentences into words Normalization, transform words into canonical form, i.e. lower case Stopword removal, delete unwanted words, i.e., &amp;ldquo;the&amp;rdquo;, &amp;ldquo;I&amp;rdquo;, &amp;ldquo;.&amp;rdquo;, etc. Max Match Algorithm Greedily match the longest word in the dictionary. Used in the language withoout space between words, i.</description>
    </item>
    <item>
      <title>A* and Hill Climbing</title>
      <link>http://localhost:1313/aquamega/algorithm/astar/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/astar/</guid>
      <description>A*算法（带有重复检测和重新打开） 设 open 为新的优先级队列，按照 g(state(σ)) + h(state(σ)) 升序排列 open.insert(make-root-node(init())) 设 closed 为空集 设 best-g 为空集 /* 将状态映射到数字 */ 当 open 不为空时： σ := open.pop-min() 如果 state(σ) 不在 closed 中或 g(σ) &amp;lt; best-g(state(σ))： /* 如果 g 更好则重新打开；注意所有具有相同状态但 g 较差的 σ′ 都在 open 中位于 σ 后面，并且在轮到它们时将被跳过 */ closed := closed ∪{state(σ)} best-g(state(σ)) := g(σ) 如果 is-goal(state(σ))：返回 extract-solution(σ) 对于每个 (a,s′) ∈succ(state(σ))： σ′ := make-node(σ,a,s′) 如果 h(state(σ′)) &amp;lt; ∞：open.insert(σ′) 返回 unsolvable def aStarSearch(problem, heuristic=nullHeuristic): &amp;#34;&amp;#34;&amp;#34;Search the node that has the lowest combined cost and heuristic first.</description>
    </item>
    <item>
      <title>BFS and DFS</title>
      <link>http://localhost:1313/aquamega/algorithm/bfs/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/bfs/</guid>
      <description>广度优先搜索（BFS） 队列，先进先出，后进后出。
Optimal when costs are uniform
def breadthFirstSearch(problem): candidate_queue = util.Queue() init_state = problem.getStartState() init_path = [] candidate_queue.push((init_state,init_path)) viewed = [] while not candidate_queue.isEmpty(): state,path = candidate_queue.pop() if problem.isGoalState(state): return path if state not in viewed: viewed.append(state) for child_state, child_action, _ in problem.getSuccessors(state): # ignore cost as we are blind child_path = path + [child_action] candidate_queue.push((child_state,child_path)) return None 深度优先搜索（DFS） 栈，先进后出，后进先出。
搜索空间可能无限大（无限深）。
def depthFirstSearch(problem): candidate_stack = util.Stack() init_state = problem.getStartState() init_path = [] candidate_stack.</description>
    </item>
    <item>
      <title>Dynamic Array</title>
      <link>http://localhost:1313/aquamega/datatype/array/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/datatype/array/</guid>
      <description>Insertion: O(1) expected, O(n) worst case Search: O(n) expected, O(n) worst case Deletion: O(n) expected, O(n) worst case class DynamicArray: def __init__(self): self.n = 0 # Number of elements self.capacity = 1 # Initial capacity self.A = self._make_array(self.capacity) def __len__(self): return self.n def __getitem__(self, k): if not 0 &amp;lt;= k &amp;lt; self.n: raise IndexError(&amp;#39;invalid index&amp;#39;) return self.A[k] def insert(self, element): &amp;#34;&amp;#34;&amp;#34; element = (id,key) &amp;#34;&amp;#34;&amp;#34; if self.n == self.capacity: self.</description>
    </item>
    <item>
      <title>MDPs</title>
      <link>http://localhost:1313/aquamega/algorithm/mdps/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/algorithm/mdps/</guid>
      <description>Markov Decision Processes（MDPs）马尔可夫决策过程 MDP是完全可观察的，概率状态模型：
状态空间 $S$ 初始状态 $s_0 \in S$ 一组目标状态 $G \subseteq S$ 在每个状态 $s \in S$ 中可应用的动作 $A(s) \subseteq A$ 对于 $s \in S$ 和 $a \in A(s)$，有转移概率 $P_a(s&#39;|s)$ 动作成本 $c(a,s) &gt; 0$
其中：
解决方案是将状态映射到动作的函数（策略）
最优解最小化预期的前往目标的成本
Partially Observable MDPs (POMDPs) 部分可观察的马尔可夫决策过程 POMDP是部分可观察的，概率状态模型：
状态 $s \in S$ 在每个状态 $s \in S$ 中可应用的动作 $A(s) \subseteq A$ 对于 $s \in S$ 和 $a \in A(s)$，有转移概率 $P_a(s&#39;|s)$ 初始信念状态 $b_0$ 最终信念状态 $b_f$ 由概率 $P_a(o|s)$，$o \in Obs$ 给出的传感器模型</description>
    </item>
    <item>
      <title>Treap</title>
      <link>http://localhost:1313/aquamega/datatype/treap/</link>
      <pubDate>Thu, 02 May 2024 11:13:45 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/datatype/treap/</guid>
      <description>Tree + Heap = Treap Insertion: O(log n) expected, O(n) worst case Search: O(log n) expected, O(n) worst case Deletion: O(log n) expected, O(n) worst case import random class Node: def __init__(self, id, key, priority): self.id = id self.key = key self.priority = priority self.left = None self.right = None class Treap: def __init__(self): self.root = None def _left_rotate(self, node): right_child = node.right node.right = right_child.left right_child.left = node return right_child def _right_rotate(self, node): left_child = node.</description>
    </item>
    <item>
      <title>Geom90008</title>
      <link>http://localhost:1313/aquamega/course/geom90008/</link>
      <pubDate>Fri, 26 Apr 2024 14:56:11 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/geom90008/</guid>
      <description>Spatial Data Management Course notebook: https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/.
Goal: Learn Postgre(pgAdmin 4) and GIS(QGIS). Help staff with herbarium data.
Vocabs scope creep: 任务蔓延 schema: 数据库的结构 query: 查询 ACID: Atomicity 原子性, Consistency 一致性, Isolation 隔离性, Durability 持久性 waterfall: 瀑布模型, 一次性完成所有工作 agile: 敏捷开发, 分阶段完成工作 ERD/ERM: Entity Relationship Diagram/Model, 实体关系图/模型 entity, attribute, relationship: 实体, 属性, 关系 location: space and time position: reference to a coordinate system 地理學第一定律: 所有事物都與其他事物相關, 但是近處的事物比遠處的事物更相關 longitude: 经度 latitude: 纬度 altitude: 海拔 wgs84: 地球坐标系, 经典的经纬度 polynomial: 多项式 ployline: 折线 polygon: 多边形 vertex: 顶点 vector: 向量 raster: 栅格 sphere: 球体 spheroid: 椭球体 Ideas 因为板块漂移, 会导致地理坐标系的变化 </description>
    </item>
    <item>
      <title>Comp90042</title>
      <link>http://localhost:1313/aquamega/course/comp90042/</link>
      <pubDate>Fri, 26 Apr 2024 14:56:05 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90042/</guid>
      <description>Natural Language Processing Preprocessing Sentence segmentation Tokenization, subword tokenization Word normalization Inflectional vs derivational morphology Lemmatization vs stemming Stopword removal N-gram Language Model Derivation Smoothing techniques Add-k Absolute discounting Katz backoff Kneser-Ney smoothing Interpolation Text Classification Build a classifier Task Topic classification Sentiment analysis Native language identification Algorithms Naive Bayes, logistic regression, SVM kNN, neural networks Bias vs variance：欠拟合under和过拟合over的取舍 Evaluation Precision, recall, F1 Part of Speech Tagging English POS Closed vs open classes Tagsets Penn Treebank tagset Automatic taggers Rule-based Statistical Unigram, classifier-based, HMM Hidden Markov Model Probabilistic formulation: Emission &amp;amp; Transition Training Viterbi algorithm Generative vs discriminative models Feedforward Neural Network Formulation Tasks: Topic classifcation Language models POS tagging Word embeddings Convolutional networks Recurrent Neural Network Formulation RNN: language models LSTM: Functions of gates Variants Tasks: Text classification: sentiment analysis POS tagging Lexical Semantics Definition of word sense, gloss Lexical Relationship: Synonymy, antonymy, hypernymy, meronymy Structure of wordnet Word similarity Path lenght Depth information Information content Word sense unambiguation supervised, unsupervised Distributional Semantics Matrices: VSM, TF-IDF, word-word co-occurrence Association measures: PMI, PPMI Count-based method: SVM Neural method: skip-gram, CBOW Evaluation: Word similarity, analogy Contextual Representation Formulation with RNN ELMo BERT Objective Fine-tuning for downstream tasks Transformers Multi-head attention Second half:</description>
    </item>
    <item>
      <title>Comp90077</title>
      <link>http://localhost:1313/aquamega/course/comp90077/</link>
      <pubDate>Fri, 26 Apr 2024 14:55:58 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90077/</guid>
      <description>Advanced Algorithms and Data Structures Learn some advanced algorithms and data structures.
Treap
Admortized Analysis: Prepaid/Potential
Quake Heap
Splay Tree
Perfect Hashing/Cuckoo Hashing
Range Tree
Min Cut/Max Flow
Karger&amp;rsquo;s algorithm: 找最小割, 找多次取最优, 随机地找两个节点合并, 直到只剩下两个节点
Ford-Fulkerson algorithm: 最早的最大流算法, 重复地找增广路径, 直到找不到
Edmonds-Karp algorithm: 用BFS找增广路径, complexity更低
Hall&amp;rsquo;s theorem: 一个二分图存在完美匹配当且仅当对于每一个子集, 子集的大小大于等于子集的邻居的大小
Vocabs conservation node: 保守节点, a node that has the same flow in and out
residual graph: 残余图, a graph that represents the remaining capacity of each edge</description>
    </item>
    <item>
      <title>Comp90054</title>
      <link>http://localhost:1313/aquamega/course/comp90054/</link>
      <pubDate>Fri, 26 Apr 2024 14:55:43 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/course/comp90054/</guid>
      <description>AI Planning for Autonomy classical planning (blind/heuristic): https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf PDDL relaxation reinforcement learning: https://gibberblot.github.io/rl-notes/index.html Vocabs acyclic 无环
systematics 系统的 / local 局部
heuristic 启发式
monotonic 单调
priority queue = min heap
conformant 符合的
policy 策略，map from state to action, denoted by $\pi$
negation 否定
precondition 前提条件
propositional 命题
predicate 谓词，return true/false
schema 模式，define somthing
misplace 放错
dominate 支配
corollary 推论
point-wise 逐点
pessimistic 悲观
benchmark 基准
novelty 新颖性
prune 修剪
velocity 速度</description>
    </item>
    <item>
      <title>Welcome</title>
      <link>http://localhost:1313/aquamega/posts/welcome/</link>
      <pubDate>Fri, 26 Apr 2024 14:22:49 +1000</pubDate>
      <guid>http://localhost:1313/aquamega/posts/welcome/</guid>
      <description>Hello World!</description>
    </item>
  </channel>
</rss>

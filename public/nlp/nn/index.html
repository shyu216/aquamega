<!doctype html><html lang=en dir=auto><head><script src="/aquamega/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=aquamega/livereload" data-no-instant defer></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neual Networks | Aqua Mega</title>
<meta name=keywords content="comp90042,nlp"><meta name=description content="神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。
每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。
神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。
神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。
A Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.
Each neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc."><meta name=author content="Dale"><link rel=canonical href=http://localhost:1313/aquamega/nlp/nn/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/aquamega/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/aquamega/nlp/nn/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Neual Networks"><meta property="og:description" content="神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。
每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。
神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。
神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。
A Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.
Each neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/aquamega/nlp/nn/"><meta property="og:image" content="https://github.com/shyu216.png"><meta property="article:section" content="nlp"><meta property="article:published_time" content="2024-05-28T00:56:05+10:00"><meta property="article:modified_time" content="2024-05-28T00:56:05+10:00"><meta property="og:site_name" content="Dale的水硕记录"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://github.com/shyu216.png"><meta name=twitter:title content="Neual Networks"><meta name=twitter:description content="神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。
每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。
神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。
神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。
A Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.
Each neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Nlps","item":"http://localhost:1313/aquamega/nlp/"},{"@type":"ListItem","position":2,"name":"Neual Networks","item":"http://localhost:1313/aquamega/nlp/nn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neual Networks","name":"Neual Networks","description":"神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。\n每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。\n神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。\n神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。\nA Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.\nEach neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.","keywords":["comp90042","nlp"],"articleBody":"神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。\n每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。\n神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。\n神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。\nA Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.\nEach neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.) to produce an output, which is then passed on to the neurons in the next layer.\nNeural networks are typically trained using the Backpropagation algorithm and the Gradient Descent algorithm. During training, the neural network minimizes the difference between the predicted values and the actual values by continuously adjusting the connection weights between neurons.\nNeural networks can handle non-linear problems and are suitable for various complex machine learning tasks, such as image recognition, speech recognition, natural language processing, etc. However, the training of neural networks requires a large amount of data and computational resources, and the interpretability of the model is relatively poor, which are some of the challenges.\n前馈神经网络 前馈神经网络是一种基础的神经网络，信息只在一个方向上流动，从输入层到输出层。\n任务：\n主题分类：使用神经网络对文本或其他数据进行主题分类。\n语言模型：使用神经网络来理解和生成语言。\n词性标注：使用神经网络来识别文本中每个词的词性。\n词嵌入：一种表示词汇的技术，将词汇映射到向量空间，使得语义相近的词汇在向量空间中的距离也相近。\n卷积网络：一种特殊的神经网络，特别适合处理网格形式的数据，如图像。\n循环神经网络 循环神经网络是一种可以处理序列数据的神经网络，它可以利用前面的信息来影响后面的输出。\nRNN语言模型：使用循环神经网络来理解和生成语言，特别适合处理文本等序列数据。 LSTM： 门的功能：LSTM是一种特殊的RNN，它有三个门（输入门、遗忘门和输出门）来控制信息的流动。 变体：LSTM有多种变体，如GRU（门控循环单元），它简化了LSTM的结构。 任务： 文本分类：情感分析：使用RNN进行文本分类，如情感分析，判断文本的情感倾向。 词性标注：使用RNN来识别文本中每个词的词性。 非线性激活函数是神经网络中的关键组成部分，它可以帮助神经网络学习和逼近复杂的模式。非线性激活函数的主要目的是将输入信号转换为输出信号，但它不仅仅是复制输入到输出。它会修改或者变换输入的方式，使得可以适应网络的需要。 常见的非线性激活函数包括：\nReLU（Rectified Linear Unit）：这是最常用的激活函数，它将所有负值设为0，正值保持不变。\nSigmoid：这个函数将任何数值都映射到0和1之间，它在早期的神经网络中非常流行，但现在已经较少使用，因为它在输入值很大或很小的时候，梯度几乎为0，这会导致梯度消失问题。\nTanh（Hyperbolic Tangent）：这个函数将任何数值都映射到-1和1之间，它比sigmoid函数的输出范围更广，因此在某些情况下，它的性能比sigmoid函数更好。\nSoftmax：这个函数通常用在神经网络的输出层，特别是在处理多分类问题时。它可以将一组数值转换为概率分布。\nRegularization 正则化 L1 Norm L2 Norm Dropout LSTM Memory cell：记忆单元，用来存储信息 Hidden state：隐藏状态，用来传递信息 Forget gate：遗忘门，用来控制记忆单元中的信息是否被遗忘 Input gate：输入门，用来控制新的信息是否被存储到记忆单元中 Output gate：输出门，用来控制隐藏状态中的信息是否被传递到下一个时间步 ","wordCount":"231","inLanguage":"en","image":"https://github.com/shyu216.png","datePublished":"2024-05-28T00:56:05+10:00","dateModified":"2024-05-28T00:56:05+10:00","author":{"@type":"Person","name":"Dale"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/aquamega/nlp/nn/"},"publisher":{"@type":"Organization","name":"Aqua Mega","logo":{"@type":"ImageObject","url":"http://localhost:1313/aquamega/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/aquamega/ accesskey=h title="Dale的水硕记录 (Alt + H)"><img src=https://github.com/shyu216.png alt aria-label=logo height=35>Dale的水硕记录</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:1313/aquamega/posts/ title=posts><span>posts</span></a></li><li><a href=http://localhost:1313/aquamega/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/aquamega/tags/ title=tags><span>tags</span></a></li><li><a href=https://github.com/shyu216 title=github><span>github</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/aquamega/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/aquamega/nlp/>Nlps</a></div><h1 class="post-title entry-hint-parent">Neual Networks</h1><div class=post-meta><span title='2024-05-28 00:56:05 +1000 AEST'>May 28, 2024</span>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;231 words&nbsp;·&nbsp;Dale&nbsp;|&nbsp;<a href=https://github.com/shyu216/aquamega/content/nlp/nn.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><p>神经网络（Neural Network）是一种模拟人脑神经元工作方式的机器学习模型。它由大量的神经元（也称为节点或单元）组成，这些神经元按照一定的层次结构排列，包括输入层、隐藏层和输出层。</p><p>每个神经元接收来自上一层神经元的输入，然后通过一个激活函数（如 Sigmoid、ReLU 等）处理这些输入，得到一个输出，这个输出再传递给下一层的神经元。</p><p>神经网络的训练通常使用反向传播（Backpropagation）算法和梯度下降（Gradient Descent）算法。在训练过程中，神经网络通过不断调整神经元之间的连接权重，来最小化预测值和真实值之间的差异。</p><p>神经网络可以处理非线性问题，适用于各种复杂的机器学习任务，如图像识别、语音识别、自然语言处理等。但是，神经网络的训练需要大量的数据和计算资源，而且模型的解释性较差，这是它的一些挑战。</p><p>A Neural Network is a machine learning model that simulates the way neurons in the human brain work. It consists of a large number of neurons (also known as nodes or units) arranged in a certain hierarchical structure, including input layers, hidden layers, and output layers.</p><p>Each neuron receives input from the neurons in the previous layer, then processes these inputs through an activation function (such as Sigmoid, ReLU, etc.) to produce an output, which is then passed on to the neurons in the next layer.</p><p>Neural networks are typically trained using the Backpropagation algorithm and the Gradient Descent algorithm. During training, the neural network minimizes the difference between the predicted values and the actual values by continuously adjusting the connection weights between neurons.</p><p>Neural networks can handle non-linear problems and are suitable for various complex machine learning tasks, such as image recognition, speech recognition, natural language processing, etc. However, the training of neural networks requires a large amount of data and computational resources, and the interpretability of the model is relatively poor, which are some of the challenges.</p><h1 id=前馈神经网络>前馈神经网络<a hidden class=anchor aria-hidden=true href=#前馈神经网络>#</a></h1><p>前馈神经网络是一种基础的神经网络，信息只在一个方向上流动，从输入层到输出层。</p><ol><li><p>任务：</p></li><li><p>主题分类：使用神经网络对文本或其他数据进行主题分类。</p></li><li><p>语言模型：使用神经网络来理解和生成语言。</p></li><li><p>词性标注：使用神经网络来识别文本中每个词的词性。</p></li><li><p>词嵌入：一种表示词汇的技术，将词汇映射到向量空间，使得语义相近的词汇在向量空间中的距离也相近。</p></li><li><p>卷积网络：一种特殊的神经网络，特别适合处理网格形式的数据，如图像。</p></li></ol><h1 id=循环神经网络>循环神经网络<a hidden class=anchor aria-hidden=true href=#循环神经网络>#</a></h1><p>循环神经网络是一种可以处理序列数据的神经网络，它可以利用前面的信息来影响后面的输出。</p><ol><li>RNN语言模型：使用循环神经网络来理解和生成语言，特别适合处理文本等序列数据。</li><li>LSTM：<ol><li>门的功能：LSTM是一种特殊的RNN，它有三个门（输入门、遗忘门和输出门）来控制信息的流动。</li><li>变体：LSTM有多种变体，如GRU（门控循环单元），它简化了LSTM的结构。</li></ol></li><li>任务：<ol><li>文本分类：情感分析：使用RNN进行文本分类，如情感分析，判断文本的情感倾向。</li><li>词性标注：使用RNN来识别文本中每个词的词性。</li></ol></li></ol><h1 id=非线性激活函数是神经网络中的关键组成部分它可以帮助神经网络学习和逼近复杂的模式非线性激活函数的主要目的是将输入信号转换为输出信号但它不仅仅是复制输入到输出它会修改或者变换输入的方式使得可以适应网络的需要>非线性激活函数是神经网络中的关键组成部分，它可以帮助神经网络学习和逼近复杂的模式。非线性激活函数的主要目的是将输入信号转换为输出信号，但它不仅仅是复制输入到输出。它会修改或者变换输入的方式，使得可以适应网络的需要。<a hidden class=anchor aria-hidden=true href=#非线性激活函数是神经网络中的关键组成部分它可以帮助神经网络学习和逼近复杂的模式非线性激活函数的主要目的是将输入信号转换为输出信号但它不仅仅是复制输入到输出它会修改或者变换输入的方式使得可以适应网络的需要>#</a></h1><p>常见的非线性激活函数包括：</p><ul><li><p>ReLU（Rectified Linear Unit）：这是最常用的激活函数，它将所有负值设为0，正值保持不变。</p></li><li><p>Sigmoid：这个函数将任何数值都映射到0和1之间，它在早期的神经网络中非常流行，但现在已经较少使用，因为它在输入值很大或很小的时候，梯度几乎为0，这会导致梯度消失问题。</p></li><li><p>Tanh（Hyperbolic Tangent）：这个函数将任何数值都映射到-1和1之间，它比sigmoid函数的输出范围更广，因此在某些情况下，它的性能比sigmoid函数更好。</p></li><li><p>Softmax：这个函数通常用在神经网络的输出层，特别是在处理多分类问题时。它可以将一组数值转换为概率分布。</p></li></ul><h1 id=regularization-正则化>Regularization 正则化<a hidden class=anchor aria-hidden=true href=#regularization-正则化>#</a></h1><ul><li>L1 Norm</li><li>L2 Norm</li><li>Dropout</li></ul><h1 id=lstm>LSTM<a hidden class=anchor aria-hidden=true href=#lstm>#</a></h1><ul><li>Memory cell：记忆单元，用来存储信息</li><li>Hidden state：隐藏状态，用来传递信息</li><li>Forget gate：遗忘门，用来控制记忆单元中的信息是否被遗忘</li><li>Input gate：输入门，用来控制新的信息是否被存储到记忆单元中</li><li>Output gate：输出门，用来控制隐藏状态中的信息是否被传递到下一个时间步</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/aquamega/tags/comp90042/>Comp90042</a></li><li><a href=http://localhost:1313/aquamega/tags/nlp/>Nlp</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/aquamega/nlp/semantics/><span class=title>« Prev</span><br><span>Lexical Semantics and Distributional Semantics</span>
</a><a class=next href=http://localhost:1313/aquamega/nlp/hmm/><span class=title>Next »</span><br><span>Hidden Markov Model</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/aquamega/>Aqua Mega</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>
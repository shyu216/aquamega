<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>24s1 on Dale的水硕日记</title><link>https://shyu216.github.io/aquamega/tags/24s1/</link><description>Recent content in 24s1 on Dale的水硕日记</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 09 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://shyu216.github.io/aquamega/tags/24s1/index.xml" rel="self" type="application/rss+xml"/><item><title>COMP90042</title><link>https://shyu216.github.io/aquamega/p/comp90042/</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><guid>https://shyu216.github.io/aquamega/p/comp90042/</guid><description>&lt;ol>
&lt;li>Preprocessing
&lt;ol>
&lt;li>Sentence segmentation&lt;/li>
&lt;li>Tokenization, subword tokenization&lt;/li>
&lt;li>Word normalization
&lt;ol>
&lt;li>Inflectional vs derivational morphology&lt;/li>
&lt;li>Lemmatization vs stemming&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Stopword removal&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>N-gram Language Model
&lt;ol>
&lt;li>Derivation&lt;/li>
&lt;li>Smoothing techniques
&lt;ol>
&lt;li>Add-k&lt;/li>
&lt;li>Absolute discounting&lt;/li>
&lt;li>Katz backoff&lt;/li>
&lt;li>Kneser-Ney smoothing&lt;/li>
&lt;li>Interpolation&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Text Classification
&lt;ol>
&lt;li>Build a classifier&lt;/li>
&lt;li>Task
&lt;ol>
&lt;li>Topic classification&lt;/li>
&lt;li>Sentiment analysis&lt;/li>
&lt;li>Native language identification&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Algorithms
&lt;ol>
&lt;li>Naive Bayes, logistic regression, SVM&lt;/li>
&lt;li>kNN, neural networks&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Bias vs variance：欠拟合under和过拟合over的取舍&lt;/li>
&lt;li>Evaluation
&lt;ol>
&lt;li>Precision, recall, F1&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Part of Speech Tagging
&lt;ol>
&lt;li>English POS
&lt;ol>
&lt;li>Closed vs open classes&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Tagsets
&lt;ol>
&lt;li>Penn Treebank tagset&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Automatic taggers
&lt;ol>
&lt;li>Rule-based&lt;/li>
&lt;li>Statistical
&lt;ol>
&lt;li>Unigram, classifier-based, HMM&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Hidden Markov Model
&lt;ol>
&lt;li>Probabilistic formulation:
&lt;ol>
&lt;li>Emission &amp;amp; Transition&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Training&lt;/li>
&lt;li>Viterbi algorithm&lt;/li>
&lt;li>Generative vs discriminative models&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Feedforward Neural Network
&lt;ol>
&lt;li>Formulation&lt;/li>
&lt;li>Tasks:
&lt;ol>
&lt;li>Topic classifcation&lt;/li>
&lt;li>Language models&lt;/li>
&lt;li>POS tagging&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Word embeddings&lt;/li>
&lt;li>Convolutional networks&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Recurrent Neural Network
&lt;ol>
&lt;li>Formulation&lt;/li>
&lt;li>RNN: language models&lt;/li>
&lt;li>LSTM:
&lt;ol>
&lt;li>Functions of gates&lt;/li>
&lt;li>Variants&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Tasks:
&lt;ol>
&lt;li>Text classification: sentiment analysis&lt;/li>
&lt;li>POS tagging&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Lexical Semantics
&lt;ol>
&lt;li>Definition of word sense, gloss&lt;/li>
&lt;li>Lexical Relationship:
&lt;ol>
&lt;li>Synonymy, antonymy, hypernymy, meronymy&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Structure of wordnet&lt;/li>
&lt;li>Word similarity
&lt;ol>
&lt;li>Path lenght&lt;/li>
&lt;li>Depth information&lt;/li>
&lt;li>Information content&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Word sense unambiguation
&lt;ol>
&lt;li>supervised, unsupervised&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Distributional Semantics
&lt;ol>
&lt;li>Matrices:
&lt;ol>
&lt;li>VSM, TF-IDF, word-word co-occurrence&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Association measures: PMI, PPMI&lt;/li>
&lt;li>Count-based method: SVM&lt;/li>
&lt;li>Neural method: skip-gram, CBOW&lt;/li>
&lt;li>Evaluation:
&lt;ol>
&lt;li>Word similarity, analogy&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Contextual Representation
&lt;ol>
&lt;li>Formulation with RNN&lt;/li>
&lt;li>ELMo&lt;/li>
&lt;li>BERT
&lt;ol>
&lt;li>Objective&lt;/li>
&lt;li>Fine-tuning for downstream tasks&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Transformers
&lt;ol>
&lt;li>Multi-head attention&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;p>Second half:&lt;/p>
&lt;ol>
&lt;li>Attention
&lt;ol>
&lt;li>Sequential model with attention&lt;/li>
&lt;li>Attention variants:
&lt;ol>
&lt;li>Concat&lt;/li>
&lt;li>Dot product&lt;/li>
&lt;li>Scaled dot product&lt;/li>
&lt;li>Location-based&lt;/li>
&lt;li>Cosine similarity&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Global vs local attention&lt;/li>
&lt;li>Self-attention&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Machine Translation
&lt;ol>
&lt;li>Statistical MT&lt;/li>
&lt;li>Neural MT with teacher forcing&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Transformer
&lt;ol>
&lt;li>Mutli-head self-attention&lt;/li>
&lt;li>Position encoding&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Pretrained Language Models
&lt;ol>
&lt;li>Encoder architecture
&lt;ol>
&lt;li>BERT&lt;/li>
&lt;li>bidirectional context&lt;/li>
&lt;li>good for classification&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Encoder-decoder architecture&lt;/li>
&lt;li>Decoder architecture
&lt;ol>
&lt;li>GPT&lt;/li>
&lt;li>unidirectional context&lt;/li>
&lt;li>good for generation&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Large Language Models
&lt;ol>
&lt;li>In-context learning&lt;/li>
&lt;li>Step-by-step reasoning&lt;/li>
&lt;li>Human-feedback reinforcement&lt;/li>
&lt;li>Instruction following&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Named Entity Recognition
&lt;ol>
&lt;li>Predict entities in a text&lt;/li>
&lt;li>Traditional ML methods&lt;/li>
&lt;li>Bi-LSTM with another RNN/CRF&lt;/li>
&lt;li>Multi-aspect NER&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Coreference Resolution 共指消解
&lt;ol>
&lt;li>Coreference, anaphora&lt;/li>
&lt;li>B-Cubed metric&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Question Answering and Reading Comprehension
&lt;ol>
&lt;li>Knowledge-based QA&lt;/li>
&lt;li>Visual QA&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Spoken Language Understanding
&lt;ol>
&lt;li>Attention RNNs&lt;/li>
&lt;li>Joint BERT&lt;/li>
&lt;li>Tri-level model&lt;/li>
&lt;li>Explainable NLU&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Vision Language Pretrained Model
&lt;ol>
&lt;li>V-L Interaction Model
&lt;ol>
&lt;li>Self-attention, co-attention, VSE&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Constrastive Language-Image Pretraining&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Ethics
&lt;ol>
&lt;li>Sotial bias 刻板印象&lt;/li>
&lt;li>Incivility 仇恨言论&lt;/li>
&lt;li>Privacy Violation 歧视&lt;/li>
&lt;li>Misinformation 谣言&lt;/li>
&lt;li>Technological divide 发展不平衡&lt;/li>
&lt;/ol>
&lt;/li>
&lt;/ol>
&lt;h2 id="vocab">Vocab
&lt;/h2>&lt;ul>
&lt;li>stemming:词干提取，去掉后缀，通常不是个词&lt;/li>
&lt;li>lemmatisation:词形还原&lt;/li>
&lt;li>morphology &lt;strong>詞法學&lt;/strong>&lt;/li>
&lt;li>lexicon 字典&lt;/li>
&lt;li>corpus 语料&lt;/li>
&lt;li>tokenization 分词&lt;/li>
&lt;li>entailment vs. contradiction 蕴涵与矛盾&lt;/li>
&lt;li>neutral 中性&lt;/li>
&lt;li>vanish 消失&lt;/li>
&lt;li>distill 提炼&lt;/li>
&lt;li>lexical 词汇的，与词汇或词语有关&lt;/li>
&lt;li>semantic 语义的，与词语的含义或解释有关&lt;/li>
&lt;li>syntactic 句法的，与句子结构有关&lt;/li>
&lt;li>sentiment 情感，对某事物的态度&lt;/li>
&lt;li>polysemy 一词多义&lt;/li>
&lt;li>gloss 词义, from dictionary，词典中对词语含义的解释&lt;/li>
&lt;li>synonymy 同义词&lt;/li>
&lt;li>antonymy 反义词&lt;/li>
&lt;li>hypernymy 上义词, is-a，是一个更一般的词&lt;/li>
&lt;li>hyponymy 一个更具体的词&lt;/li>
&lt;li>meronymy 下义词, is-part-of&lt;/li>
&lt;li>spectrum 范围&lt;/li>
&lt;li>conference 共指，在文本中指代同一实体&lt;/li>
&lt;li>anaphora 指代，一种语言现象，代替先前提到的词&lt;/li>
&lt;li>utterence 话语&lt;/li>
&lt;li>utter 说，发出声音&lt;/li>
&lt;li>tedious 冗长的工作，乏味，枯燥&lt;/li>
&lt;li>versality 可以在多种不同的情况和环境中使用，具有很高的适应性和灵活性&lt;/li>
&lt;li>continuation 相连&lt;/li>
&lt;li>patch-based 基于块的&lt;/li>
&lt;li>interpretability 可解释性，白盒&lt;/li>
&lt;li>explainability 可解释性，黑盒&lt;/li>
&lt;li>monolingual 只使用一种语言的情况或者系统&lt;/li>
&lt;li>sinusoidal 正弦的&lt;/li>
&lt;li>proximal 接近的&lt;/li>
&lt;li>miscellaneous 混杂的&lt;/li>
&lt;li>antecedent 先行词, Trump&lt;/li>
&lt;li>anaphor 指代词, he&lt;/li>
&lt;li>pronominal 代词的&lt;/li>
&lt;li>adjective 形容词&lt;/li>
&lt;li>factoid 事实&lt;/li>
&lt;li>intent 意图&lt;/li>
&lt;li>clause 子句&lt;/li>
&lt;li>toxicity 有毒&lt;/li>
&lt;li>insult 侮辱&lt;/li>
&lt;li>humiliation 羞辱&lt;/li>
&lt;li>altered 改变&lt;/li>
&lt;li>conjunction 连接词&lt;/li>
&lt;li>conform 符合&lt;/li>
&lt;li>genre 风格&lt;/li>
&lt;li>spurious 伪造的&lt;/li>
&lt;li>occurrence 出现&lt;/li>
&lt;li>maginalisation 边缘化&lt;/li>
&lt;li>nuance 细微差别&lt;/li>
&lt;/ul>
&lt;h2 id="information-bottleneck">Information Bottleneck
&lt;/h2>&lt;p>信息瓶颈问题是指在机器学习模型中，模型在尝试压缩输入数据的同时，还要尽可能保留与目标输出相关的信息。这是一个权衡问题，因为模型需要找到一个平衡点，既能有效地压缩数据，又能保留足够的信息来进行准确的预测。&lt;/p></description></item><item><title>COMP90054</title><link>https://shyu216.github.io/aquamega/p/comp90054/</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><guid>https://shyu216.github.io/aquamega/p/comp90054/</guid><description>&lt;ul>
&lt;li>classical planning (blind/heuristic): &lt;a class="link" href="https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf" target="_blank" rel="noopener"
>https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf&lt;/a>&lt;/li>
&lt;li>PDDL&lt;/li>
&lt;li>relaxation&lt;/li>
&lt;li>reinforcement learning: &lt;a class="link" href="https://gibberblot.github.io/rl-notes/index.html" target="_blank" rel="noopener"
>https://gibberblot.github.io/rl-notes/index.html&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="vocabs">Vocabs
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>acyclic 无环&lt;/p>
&lt;/li>
&lt;li>
&lt;p>systematics 系统的 / local 局部&lt;/p>
&lt;/li>
&lt;li>
&lt;p>heuristic 启发式&lt;/p>
&lt;/li>
&lt;li>
&lt;p>monotonic 单调&lt;/p>
&lt;/li>
&lt;li>
&lt;p>priority queue = min heap&lt;/p>
&lt;/li>
&lt;li>
&lt;p>conformant 符合的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>policy 策略，map from state to action, denoted by $\pi$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>negation 否定&lt;/p>
&lt;/li>
&lt;li>
&lt;p>precondition 前提条件&lt;/p>
&lt;/li>
&lt;li>
&lt;p>propositional 命题&lt;/p>
&lt;/li>
&lt;li>
&lt;p>predicate 谓词，return true/false&lt;/p>
&lt;/li>
&lt;li>
&lt;p>schema 模式，define somthing&lt;/p>
&lt;/li>
&lt;li>
&lt;p>misplace 放错&lt;/p>
&lt;/li>
&lt;li>
&lt;p>dominate 支配&lt;/p>
&lt;/li>
&lt;li>
&lt;p>corollary 推论&lt;/p>
&lt;/li>
&lt;li>
&lt;p>point-wise 逐点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>pessimistic 悲观&lt;/p>
&lt;/li>
&lt;li>
&lt;p>benchmark 基准&lt;/p>
&lt;/li>
&lt;li>
&lt;p>novelty 新颖性&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prune 修剪&lt;/p>
&lt;/li>
&lt;li>
&lt;p>velocity 速度&lt;/p>
&lt;/li>
&lt;li>
&lt;p>stochastic 随机&lt;/p>
&lt;/li>
&lt;li>
&lt;p>tabular 表格&lt;/p>
&lt;/li>
&lt;li>
&lt;p>dilemma 困境&lt;/p>
&lt;/li>
&lt;li>
&lt;p>equilibria 平衡&lt;/p>
&lt;/li>
&lt;li>
&lt;p>, set minus operation&lt;/p>
&lt;/li>
&lt;li>
&lt;p>payoff 收益&lt;/p>
&lt;/li>
&lt;li>
&lt;p>utility 效用&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Nondeterministic Polynomial: 不确定多项式，没有已知的多项式时间算法可以在所有情况下找到一个解&lt;/p>
&lt;ul>
&lt;li>non-deterministic: countless outcomes&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>mixed strategy equilibria: 混合策略均衡&lt;/p>
&lt;/li>
&lt;li>
&lt;p>nash equilibrium: 纳什均衡，每个玩家都在最佳响应下(pure strategy)，没有人会改变策略&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="安全目标感知可接受一致性">安全/目标感知/可接受/一致性
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>$h$ remaining cost to reach the goal, $*$ optimal&lt;/p>
&lt;/li>
&lt;li>
&lt;p>假设$\Pi$是一个计划任务，具有状态空间$\Theta_{\Pi} = (S, L, c, T, I, G)$，并且$h$是$\Pi$的一个启发式。如果启发式满足以下条件，那么它被称为：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>安全（Safe）：如果对于所有$h(s) = \infty$的状态$s \in S$，都有$h^*(s) = \infty$，则启发式被称为安全。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>目标感知（Goal-aware）：如果对于所有目标状态$s \in S_G$，都有$h(s) = 0$，则启发式被称为目标感知。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>可接受（Admissible）：如果对于所有状态$s \in S$，都有$h(s) \leq h^*(s)$，则启发式被称为可接受。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一致（Consistent）：如果对于所有$s \xrightarrow{a} s&amp;rsquo;$的转移，都有$h(s) \leq h(s&amp;rsquo;) + c(a)$，则启发式被称为一致。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>命题：假设$\Pi$是一个计划任务，具有状态空间$\Theta_{\Pi} = (S, L, c, T, I, S_G)$，并且$h$是$\Pi$的一个启发式。&lt;/p>
&lt;ul>
&lt;li>如果$h$是一致的和目标感知的，则$h$是可接受的。&lt;/li>
&lt;li>如果$h$是可接受的，则$h$是目标感知的。&lt;/li>
&lt;li>如果$h$是可接受的，则$h$是安全的。&lt;/li>
&lt;li>没有其他这种形式的蕴含关系成立。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>不可接受：最优的节点如果被高估，就会优先扩展其他节点，而错过最优。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一致性：保证最优路径依次访问。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="strips-问题是一个四元组p--langle-foig-rangle">STRIPS: 问题是一个四元组$P = \langle F,O,I,G \rangle$：
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>$F$ fact, 原子, 变量&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$O$ 或 $A$ operator, action, 操作符, 动作&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$I \subseteq F$代表初始情况&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$G \subseteq F$代表目标情况&lt;/p>
&lt;/li>
&lt;li>
&lt;p>操作符$o \in O$由以下表示：&lt;/p>
&lt;ul>
&lt;li>添加列表$Add(o) \subseteq F$&lt;/li>
&lt;li>删除列表$Del(o) \subseteq F$&lt;/li>
&lt;li>前提条件列表$Pre(o) \subseteq F$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="relaxiation">Relaxiation:
&lt;/h2>&lt;p>Goal：Helps compute heuristic function。&lt;/p>
&lt;p>设$h^* : P \rightarrow R^+_0 \cup {\infty}$是一个函数。$h^&lt;em>$的松弛是一个三元组$R= (P&amp;rsquo;,r,h&amp;rsquo;^&lt;/em>)$，其中$P&amp;rsquo;$是任意集合，$r : P \rightarrow P&amp;rsquo;$和$h&amp;rsquo;^* : P&amp;rsquo; \rightarrow R^+_0 \cup {\infty}$是函数，对于所有的$\Pi \in P$，松弛启发式$h_R(\Pi) := h&amp;rsquo;^&lt;em>(r(\Pi))$满足$h_R(\Pi) \leq h^&lt;/em>(\Pi)$。松弛是：&lt;/p>
&lt;ul>
&lt;li>问题$P$：寻路。&lt;/li>
&lt;li>更简单的问题$P&amp;rsquo;$：鸟类的寻路。&lt;/li>
&lt;li>$P&amp;rsquo;$的完美启发式$h&amp;rsquo;^*$：直线距离。&lt;/li>
&lt;li>转换$r$：假装你是一只鸟。&lt;/li>
&lt;li>原生的，如果$P&amp;rsquo; \subseteq P$且$h&amp;rsquo;^* = h^*$；&lt;/li>
&lt;li>可有效构造的，如果存在一个多项式时间算法，给定$\Pi \in P$，可以计算$r(\Pi)$；&lt;/li>
&lt;li>可有效计算的，如果存在一个多项式时间算法，给定$\Pi&amp;rsquo; \in P&amp;rsquo;$，可以计算$h&amp;rsquo;^*(\Pi&amp;rsquo;)$。&lt;/li>
&lt;/ul>
&lt;p>提醒：你有一个问题$P$，你希望估计其完美启发式$h^&lt;em>$。你定义了一个更简单的问题$P&amp;rsquo;$，其完美启发式$h&amp;rsquo;^&lt;/em>$可以用来（可接受地！）估计$h^&lt;em>$。你定义了一个从$P$到$P&amp;rsquo;$的转换$r$。给定$\Pi \in P$，你通过$h&amp;rsquo;^&lt;/em>(r(\Pi))$来估计$h^*(\Pi)$。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>notation in this course&lt;/strong>:&lt;/p>
&lt;p>$\Pi_s$：将初始状态替换为$s$的$\Pi$，即，将$\Pi = (F,A,c,I,G)$更改为$(F,A,c,s,G)$。&lt;/p>
&lt;ul>
&lt;li>c: clause, preconditions+effects&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="有关quality-value的一些概念">有关Quality Value的一些概念：
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>Value：在强化学习中，value通常指的是一个状态的价值，也就是从这个状态开始，遵循某个策略能够获得的预期回报。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Q-value：Q-value是对于状态-动作对(state-action pair)的价值的一种评估。也就是说，如果在某个状态下执行某个动作，然后遵循某个策略，能够获得的预期回报。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Q-value table：Q-value table是一种数据结构，用于存储每个状态-动作对的Q-value。在表格型强化学习算法（如Q-learning）中，Q-value table是主要的数据结构。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Q-value function：Q-value function是一个函数，它接受一个状态和一个动作作为输入，返回这个状态-动作对的Q-value。在函数逼近方法（如深度Q网络）中，Q-value function通常由神经网络来表示。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Q-table：Q-table和Q-value table是同一个概念，只是名称不同。&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>COMP90077</title><link>https://shyu216.github.io/aquamega/p/comp90077/</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><guid>https://shyu216.github.io/aquamega/p/comp90077/</guid><description>&lt;p>Learn some advanced algorithms and data structures.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Treap&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Admortized Analysis: Prepaid/Potential&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Quake Heap&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Splay Tree&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Perfect Hashing/Cuckoo Hashing&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Range Tree&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Min Cut/Max Flow&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Karger&amp;rsquo;s algorithm: 找最小割, 找多次取最优, 随机地找两个节点合并, 直到只剩下两个节点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Ford-Fulkerson algorithm: 最早的最大流算法, 重复地找增广路径, 直到找不到&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Edmonds-Karp algorithm: 用BFS找增广路径, complexity更低&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Hall&amp;rsquo;s theorem: 一个二分图存在完美匹配当且仅当对于每一个子集, 子集的大小大于等于子集的邻居的大小&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="readings">Readings
&lt;/h2>&lt;ul>
&lt;li>Karger&amp;rsquo;s Randomised Contraction algorithm: Chapter 13.2 of [KT]&lt;/li>
&lt;li>Flow networks, max flow, min cut and basic Ford-Fulkerson: Chapter 7.1 - 7.2 of [KT] and also Chapter 10.1 - 10.4 of [JE]&lt;/li>
&lt;li>Flow network applications (Bipartite Matching and Disjoint Paths): Chapter 7.5-7.6 of [KT]&lt;/li>
&lt;li>Capacity-Scaling: Chapter 7.3 of [KT]Edmonds-Karps algorithms: Chapter 10.6 of [JE]&lt;/li>
&lt;li>Circulation with demands: Chapter 7.7 of [KT]&lt;/li>
&lt;li>Linear Programming: &lt;a class="link" href="https://jeffe.cs.illinois.edu/teaching/algorithms/notes/H-lp.pdf" target="_blank" rel="noopener"
>https://jeffe.cs.illinois.edu/teaching/algorithms/notes/H-lp.pdf&lt;/a>&lt;/li>
&lt;li>Approximation Algorithms: Chapter 1 of &lt;a class="link" href="https://www.designofapproxalgs.com/book.pdf" target="_blank" rel="noopener"
>https://www.designofapproxalgs.com/book.pdf&lt;/a> and see also Approximation Algorithms by Vazirani&lt;/li>
&lt;li>[JE] = &lt;a class="link" href="https://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf" target="_blank" rel="noopener"
>https://jeffe.cs.illinois.edu/teaching/algorithms/book/Algorithms-JeffE.pdf&lt;/a>&lt;/li>
&lt;li>[KT] = Algorithm Design by Kleinberg and Tardos&lt;/li>
&lt;/ul>
&lt;h2 id="vocabs">Vocabs
&lt;/h2>&lt;ul>
&lt;li>
&lt;p>asymptotic notation: 渐进符号&lt;/p>
&lt;/li>
&lt;li>
&lt;p>subtle: 微妙的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>conjecture: 猜想&lt;/p>
&lt;/li>
&lt;li>
&lt;p>concave: 凹的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>invalidate: 使无效&lt;/p>
&lt;/li>
&lt;li>
&lt;p>conceptual: 概念上的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>dominate: 支配&lt;/p>
&lt;/li>
&lt;li>
&lt;p>trial: 尝试&lt;/p>
&lt;/li>
&lt;li>
&lt;p>prime: 素数&lt;/p>
&lt;/li>
&lt;li>
&lt;p>evict: 驱逐&lt;/p>
&lt;/li>
&lt;li>
&lt;p>disjoint: 不相交的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>fraction: 分数&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cascadinng: 级联&lt;/p>
&lt;/li>
&lt;li>
&lt;p>auxilinary: 辅助的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sink: 汇点&lt;/p>
&lt;/li>
&lt;li>
&lt;p>discrepency: 差异&lt;/p>
&lt;/li>
&lt;li>
&lt;p>incoporate: 合并&lt;/p>
&lt;/li>
&lt;li>
&lt;p>converse: 相反的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>sought: 寻找&lt;/p>
&lt;/li>
&lt;li>
&lt;p>polytope: 多面体&lt;/p>
&lt;/li>
&lt;li>
&lt;p>incident: 相邻的&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;Incident&amp;rdquo;：当我们说一条边和一个顶点是&amp;quot;incident&amp;quot;（相邻），意味着这条边的一个端点就是这个顶点。&lt;/li>
&lt;li>&amp;ldquo;Adjacent&amp;rdquo;：当我们说两个顶点是&amp;quot;adjacent&amp;quot;（邻接），意味着存在一条边连接这两个顶点。同样，当我们说两条边是&amp;quot;adjacent&amp;quot;（邻接），意味着这两条边共享一个公共顶点。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>feasible: 可行的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>polynomial: 多项式&lt;/p>
&lt;/li>
&lt;li>
&lt;p>comprise: 包括&lt;/p>
&lt;/li>
&lt;li>
&lt;p>logarithmic: 对数的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>rounding: 四舍五入&lt;/p>
&lt;/li>
&lt;li>
&lt;p>infeasible: 不可行的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>unbounded: 无界的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>reciprocal: 倒数&lt;/p>
&lt;/li>
&lt;li>
&lt;p>incremental: 增量的&lt;/p>
&lt;/li>
&lt;li>
&lt;p>conservation: 保守&lt;/p>
&lt;/li>
&lt;li>
&lt;p>slackness: 松弛&lt;/p>
&lt;/li>
&lt;li>
&lt;p>conservation node: 保守节点, a node that has the same flow in and out&lt;/p>
&lt;/li>
&lt;li>
&lt;p>residual graph: 残余图, a graph that represents the remaining capacity of each edge&lt;/p>
&lt;/li>
&lt;li>
&lt;p>augmenting path: 增广路径, a path from source to sink in the residual graph&lt;/p>
&lt;/li>
&lt;li>
&lt;p>feasible flow: 可行流, a flow that satisfies the capacity constraints and conservation constraints&lt;/p>
&lt;/li>
&lt;li>
&lt;p>perfect matching: 完美匹配, a matching that covers all the nodes&lt;/p>
&lt;/li>
&lt;li>
&lt;p>bipartite graph: 二分图, a graph that can be divided into two sets such that all edges are between the two sets&lt;/p>
&lt;/li>
&lt;li>
&lt;p>disjoint paths: 不相交路径, paths that do not share any nodes&lt;/p>
&lt;/li>
&lt;li>
&lt;p>vertex cover: 顶点覆盖, a set of vertices that covers all the edges&lt;/p>
&lt;/li>
&lt;li>
&lt;p>maximal matching: 最大匹配, a matching that cannot be extended by adding another edge&lt;/p>
&lt;/li>
&lt;li>
&lt;p>cardinality 基数，集合中元素的数量&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>GEOM90008</title><link>https://shyu216.github.io/aquamega/p/geom90008/</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><guid>https://shyu216.github.io/aquamega/p/geom90008/</guid><description>&lt;p>Course notebook: &lt;a class="link" href="https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/" target="_blank" rel="noopener"
>https://tomkom.pages.gitlab.unimelb.edu.au/spatialdatamanagement/&lt;/a>.&lt;/p>
&lt;ul>
&lt;li>postgreSQL&lt;/li>
&lt;li>postGIS&lt;/li>
&lt;li>QGIS&lt;/li>
&lt;/ul>
&lt;h2 id="vocabs">Vocabs
&lt;/h2>&lt;ul>
&lt;li>scope creep: 任务蔓延&lt;/li>
&lt;li>schema: 数据库的结构&lt;/li>
&lt;li>query: 查询&lt;/li>
&lt;li>ACID: Atomicity 原子性, Consistency 一致性, Isolation 隔离性, Durability 持久性&lt;/li>
&lt;li>waterfall: 瀑布模型, 一次性完成所有工作&lt;/li>
&lt;li>agile: 敏捷开发, 分阶段完成工作&lt;/li>
&lt;li>ERD/ERM: Entity Relationship Diagram/Model, 实体关系图/模型&lt;/li>
&lt;li>entity, attribute, relationship: 实体, 属性, 关系&lt;/li>
&lt;li>location: space and time&lt;/li>
&lt;li>position: reference to a coordinate system&lt;/li>
&lt;li>地理學第一定律: 所有事物都與其他事物相關, 但是近處的事物比遠處的事物更相關&lt;/li>
&lt;li>longitude: 经度&lt;/li>
&lt;li>latitude: 纬度&lt;/li>
&lt;li>altitude: 海拔&lt;/li>
&lt;li>wgs84: 地球坐标系, 经典的经纬度&lt;/li>
&lt;li>polynomial: 多项式&lt;/li>
&lt;li>ployline: 折线&lt;/li>
&lt;li>polygon: 多边形&lt;/li>
&lt;li>vertex: 顶点&lt;/li>
&lt;li>vector: 向量&lt;/li>
&lt;li>raster: 栅格&lt;/li>
&lt;li>sphere: 球体&lt;/li>
&lt;li>spheroid: 椭球体&lt;/li>
&lt;/ul>
&lt;h2 id="coordinate-system">Coordinate system
&lt;/h2>&lt;p>因为板块漂移, 会导致地理坐标系的变化, 所以经纬度总是在更新&lt;/p></description></item></channel></rss>
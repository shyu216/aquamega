---
title: COMP90054
description: AI Planning for Autonomy
date: "2024-06-09T00:00:00+10:00"

tags:
   - 24s1
   - AI
   - algo
categories:
   - COMP90054
---

# AI Planning for Autonomy

- classical planning (blind/heuristic): [https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf](https://fai.cs.uni-saarland.de/hoffmann/papers/ki11.pdf)
- PDDL
- relaxation
- reinforcement learning: [https://gibberblot.github.io/rl-notes/index.html](https://gibberblot.github.io/rl-notes/index.html)

## Vocabs

- acyclic 无环
- systematics 系统的 / local 局部
- heuristic 启发式
- monotonic 单调
- priority queue = min heap
- conformant 符合的
- policy 策略，map from state to action, denoted by $\pi$
- negation 否定
- precondition 前提条件
- propositional 命题
- predicate 谓词，return true/false
- schema 模式，define somthing
- misplace 放错
- dominate 支配
- corollary 推论
- point-wise 逐点
- pessimistic 悲观
- benchmark 基准
- novelty 新颖性
- prune 修剪
- velocity 速度
- stochastic 随机
- tabular 表格
- dilemma 困境
- equilibria 平衡
- \, set minus operation
- payoff 收益
- utility 效用


- Nondeterministic Polynomial: 不确定多项式，没有已知的多项式时间算法可以在所有情况下找到一个解
  - non-deterministic: countless outcomes

- mixed strategy equilibria: 混合策略均衡
- nash equilibrium: 纳什均衡，每个玩家都在最佳响应下(pure strategy)，没有人会改变策略


## 安全/目标感知/可接受/一致性


- $h$ remaining cost to reach the goal, $*$ optimal
  
- 假设$\Pi$是一个计划任务，具有状态空间$\Theta_{\Pi} = (S, L, c, T, I, G)$，并且$h$是$\Pi$的一个启发式。如果启发式满足以下条件，那么它被称为：

  - 安全（Safe）：如果对于所有$h(s) = \infty$的状态$s \in S$，都有$h^*(s) = \infty$，则启发式被称为安全。

  - 目标感知（Goal-aware）：如果对于所有目标状态$s \in S_G$，都有$h(s) = 0$，则启发式被称为目标感知。

  - 可接受（Admissible）：如果对于所有状态$s \in S$，都有$h(s) \leq h^*(s)$，则启发式被称为可接受。

  - 一致（Consistent）：如果对于所有$s \xrightarrow{a} s'$的转移，都有$h(s) \leq h(s') + c(a)$，则启发式被称为一致。

- 命题：假设$\Pi$是一个计划任务，具有状态空间$\Theta_{\Pi} = (S, L, c, T, I, S_G)$，并且$h$是$\Pi$的一个启发式。
  - 如果$h$是一致的和目标感知的，则$h$是可接受的。
  - 如果$h$是可接受的，则$h$是目标感知的。
  - 如果$h$是可接受的，则$h$是安全的。
  - 没有其他这种形式的蕴含关系成立。


- 不可接受：最优的节点如果被高估，就会优先扩展其他节点，而错过最优。
- 一致性：保证最优路径依次访问。

## STRIPS: 问题是一个四元组$P = \langle F,O,I,G \rangle$：

- $F$ fact, 原子, 变量
- $O$ 或 $A$ operator, action, 操作符, 动作
- $I \subseteq F$代表初始情况
- $G \subseteq F$代表目标情况

- 操作符$o \in O$由以下表示：
    - 添加列表$Add(o) \subseteq F$
    - 删除列表$Del(o) \subseteq F$
    - 前提条件列表$Pre(o) \subseteq F$
  
## Relaxiation: 

Goal：Helps compute heuristic function。

设$h^* : P \rightarrow R^+_0 \cup \{\infty\}$是一个函数。$h^*$的松弛是一个三元组$R= (P',r,h'^*)$，其中$P'$是任意集合，$r : P \rightarrow P'$和$h'^* : P' \rightarrow R^+_0 \cup \{\infty\}$是函数，对于所有的$\Pi \in P$，松弛启发式$h_R(\Pi) := h'^*(r(\Pi))$满足$h_R(\Pi) \leq h^*(\Pi)$。松弛是：
  - 问题$P$：寻路。
  - 更简单的问题$P'$：鸟类的寻路。
  - $P'$的完美启发式$h'^*$：直线距离。
  - 转换$r$：假装你是一只鸟。
  - 原生的，如果$P' \subseteq P$且$h'^* = h^*$；
  - 可有效构造的，如果存在一个多项式时间算法，给定$\Pi \in P$，可以计算$r(\Pi)$；
  - 可有效计算的，如果存在一个多项式时间算法，给定$\Pi' \in P'$，可以计算$h'^*(\Pi')$。

  提醒：你有一个问题$P$，你希望估计其完美启发式$h^*$。你定义了一个更简单的问题$P'$，其完美启发式$h'^*$可以用来（可接受地！）估计$h^*$。你定义了一个从$P$到$P'$的转换$r$。给定$\Pi \in P$，你通过$h'^*(r(\Pi))$来估计$h^*(\Pi)$。


- **notation in this course**:
  
   $\Pi_s$：将初始状态替换为$s$的$\Pi$，即，将$\Pi = (F,A,c,I,G)$更改为$(F,A,c,s,G)$。
   - c: clause, preconditions+effects


## 有关Quality Value的一些概念：

  - Value：在强化学习中，value通常指的是一个状态的价值，也就是从这个状态开始，遵循某个策略能够获得的预期回报。

  - Q-value：Q-value是对于状态-动作对(state-action pair)的价值的一种评估。也就是说，如果在某个状态下执行某个动作，然后遵循某个策略，能够获得的预期回报。

  - Q-value table：Q-value table是一种数据结构，用于存储每个状态-动作对的Q-value。在表格型强化学习算法（如Q-learning）中，Q-value table是主要的数据结构。

  - Q-value function：Q-value function是一个函数，它接受一个状态和一个动作作为输入，返回这个状态-动作对的Q-value。在函数逼近方法（如深度Q网络）中，Q-value function通常由神经网络来表示。

  - Q-table：Q-table和Q-value table是同一个概念，只是名称不同。
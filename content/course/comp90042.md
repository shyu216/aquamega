---
date: "2024-04-26T14:56:05+10:00"
draft: false
title: "Comp90042"
tags: ["comp90042", "nlp", "24s1"]
categories: ["course"]
---

# Natural Language Processing

1. Preprocessing
   1. Sentence segmentation 
   2. Tokenization, subword tokenization
   3. Word normalization
      1. Inflectional vs derivational morphology
      2. Lemmatization vs stemming
   4. Stopword removal
2. N-gram Language Model
   1. Derivation
   2. Smoothing techniques
      1. Add-k
      2. Absolute discounting
      3. Katz backoff
      4. Kneser-Ney smoothing
      5. Interpolation
3. Text Classification
   1. Build a classifier
   2. Task
      1. Topic classification
      2. Sentiment analysis
      3. Native language identification
   3. Algorithms
      1. Naive Bayes, logistic regression, SVM
      2. kNN, neural networks
   4. Bias vs variance：欠拟合under和过拟合over的取舍
   5. Evaluation
      1. Precision, recall, F1
4. Part of Speech Tagging
   1. English POS
      1. Closed vs open classes
   2. Tagsets
      1. Penn Treebank tagset
   3. Automatic taggers
      1. Rule-based
      2. Statistical
         1. Unigram, classifier-based, HMM
5. Hidden Markov Model
   1. Probabilistic formulation:
      1. Emission & Transition
   2. Training
   3. Viterbi algorithm
   4. Generative vs discriminative models
6. Feedforward Neural Network
   1. Formulation
   2. Tasks:
      1. Topic classifcation
      2. Language models
      3. POS tagging
   3. Word embeddings
   4. Convolutional networks
7. Recurrent Neural Network
   1. Formulation
   2. RNN: language models
   3. LSTM:
      1. Functions of gates
      2. Variants
   4. Tasks:
      1. Text classification: sentiment analysis
      2. POS tagging
8. Lexical Semantics
   1. Definition of word sense, gloss
   2. Lexical Relationship:
      1. Synonymy, antonymy, hypernymy, meronymy
   3. Structure of wordnet
   4. Word similarity
      1. Path lenght
      2. Depth information
      3. Information content
   5. Word sense unambiguation
      1. supervised, unsupervised
9. Distributional Semantics
   1.  Matrices:
       1.  VSM, TF-IDF, word-word co-occurrence
   2.  Association measures: PMI, PPMI
   3.  Count-based method: SVM
   4.  Neural method: skip-gram, CBOW
   5.  Evaluation:
       1.  Word similarity, analogy
10. Contextual Representation
    1.  Formulation with RNN
    2.  ELMo
    3.  BERT
        1.  Objective
        2.  Fine-tuning for downstream tasks
    4.  Transformers
        1. Multi-head attention

Second half:

1. Attention
   1. Sequential model with attention
   2. Attention variants:
      1. Concat
      2. Dot product
      3. Scaled dot product
      4. Location-based
      5. Cosine similarity
   3. Global vs local attention
   4. Self-attention
2. Machine Translation
   1. Statistical MT
   2. Neural MT with teacher forcing
3. Transformer
   1. Mutli-head self-attention
   2. Position encoding
4. Pretrained Language Models
   1. Encoder architecture
      1. BERT
      2. bidirectional context
      3. good for classification
   2. Encoder-decoder architecture
   3. Decoder architecture
      1. GPT
      2. unidirectional context
      3. good for generation
5. Large Language Models
   1. In-context learning
   2. Step-by-step reasoning
   3. Human-feedback reinforcement
   4. Instruction following
6. Named Entity Recognition 
   1. Predict entities in a text
   2. Traditional ML methods
   3. Bi-LSTM with another RNN/CRF
   4. Multi-aspect NER
7. Coreference Resolution 共指消解
   1. Coreference, anaphora
   2. B-Cubed metric
8. Question Answering and Reading Comprehension
   1. Knowledge-based QA
   2. Visual QA
9. Spoken Language Understanding
   1. Attention RNNs
   2. Joint BERT
   3. Tri-level model
   4. Explainable NLU
10. Vision Language Pretrained Model
    1.  V-L Interaction Model
        1.  Self-attention, co-attention, VSE
    2.  Constrastive Language-Image Pretraining 
11. Ethics
    1.  Sotial bias 刻板印象
    2.  Incivility 仇恨言论
    3.  Privacy Violation 歧视
    4.  Misinformation 谣言
    5.  Technological divide 发展不平衡





## Vocab

- stemming:词干提取，去掉后缀，通常不是个词
- lemmatisation:词形还原
- morphology **詞法學**
- lexicon 字典
- corpus 语料
- tokenization 分词
- entailment vs. contradiction 蕴涵与矛盾
- neutral 中性
- vanish 消失
- distill 提炼
- lexical 词汇的，与词汇或词语有关
- semantic 语义的，与词语的含义或解释有关
- syntactic 句法的，与句子结构有关
- sentiment 情感，对某事物的态度
- polysemy 一词多义
- gloss 词义, from dictionary，词典中对词语含义的解释
- synonymy 同义词
- antonymy 反义词
- hypernymy 上义词, is-a，是一个更一般的词
- hyponymy 一个更具体的词
- meronymy 下义词, is-part-of
- spectrum 范围
- conference 共指，在文本中指代同一实体
- anaphora 指代，一种语言现象，代替先前提到的词
- utterence 话语
- utter 说，发出声音
- tedious 冗长的工作，乏味，枯燥
- versality 可以在多种不同的情况和环境中使用，具有很高的适应性和灵活性
- continuation 相连
- patch-based 基于块的
- interpretability 可解释性，白盒
- explainability 可解释性，黑盒
- monolingual 只使用一种语言的情况或者系统
- sinusoidal 正弦的
- proximal 接近的
- miscellaneous 混杂的
- antecedent 先行词, Trump
- anaphor 指代词, he
- pronominal 代词的
- adjective 形容词
- factoid 事实
- 

# Information Bottleneck
信息瓶颈问题是指在机器学习模型中，模型在尝试压缩输入数据的同时，还要尽可能保留与目标输出相关的信息。这是一个权衡问题，因为模型需要找到一个平衡点，既能有效地压缩数据，又能保留足够的信息来进行准确的预测。